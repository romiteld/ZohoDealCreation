"""
FastAPI main application using PostgreSQL with pgvector
Production-ready with Azure Container Apps deployment
"""

# LangGraph implementation - PostgreSQL with vector support
import os
import logging
import asyncio
import traceback
from datetime import datetime
from typing import Optional, List, Dict, Any
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, Depends, status, Header, Request, WebSocket, Query, File, UploadFile
from fastapi.responses import FileResponse, JSONResponse, HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from starlette.middleware.base import BaseHTTPMiddleware
from dotenv import load_dotenv

# Load environment variables
load_dotenv('.env.local')
load_dotenv()

# Initialize logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import models and services
from app.models import EmailPayload as EmailRequest, ProcessingResult as ZohoResponse, ExtractedData, WeeklyDigestFilters
from app.business_rules import BusinessRulesEngine
from app.integrations import ZohoApiClient as ZohoIntegration, AzureBlobStorageClient as AzureBlobStorage, PostgreSQLClient
from app.microsoft_graph_client import MicrosoftGraphClient
from app.azure_ad_auth import get_auth_service
from app.realtime_queue_manager import get_queue_manager, websocket_endpoint, EmailStatus
from app.manifest_cache_service import get_manifest_cache_service
from app.duplicate_checker import DuplicateChecker

# Import Vault Agent routes
from app.api.vault_agent.routes import router as vault_agent_router

# Import Admin routes
from app.admin import policies_router
from app.admin.import_exports_v2 import router as import_v2_router

# Import Apollo enrichment
from app.apollo_enricher import enrich_contact_with_apollo

# API Key Authentication with secure comparison and rate limiting
import hmac
from collections import defaultdict
from datetime import datetime, timedelta
import hashlib

API_KEY = os.getenv("API_KEY")
if not API_KEY or API_KEY == "your-secure-api-key-here":
    logger.warning("API_KEY not properly configured - using development key")
    API_KEY = os.getenv("API_KEY", "dev-key-only-for-testing")

# Rate limiting for API key verification
api_key_attempts = defaultdict(list)
MAX_ATTEMPTS = 5
LOCKOUT_DURATION = timedelta(minutes=15)

# Import auth utility instead of defining here
from app.auth import verify_api_key as _verify_api_key

async def verify_api_key(request: Request, x_api_key: str = Header(...)):
    """Verify API key from header with timing-safe comparison and rate limiting"""
    if not x_api_key:
        logger.warning("Missing API key in request")
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="API Key required"
        )
    
    # Get client IP for rate limiting
    client_ip = request.client.host
    key_hash = hashlib.sha256(x_api_key.encode()).hexdigest()[:16]
    rate_limit_key = f"{client_ip}:{key_hash}"
    
    # Check rate limiting
    current_time = datetime.utcnow()
    attempts = api_key_attempts[rate_limit_key]
    
    # Clean old attempts
    api_key_attempts[rate_limit_key] = [
        attempt for attempt in attempts 
        if current_time - attempt < LOCKOUT_DURATION
    ]
    
    # Check if locked out
    if len(api_key_attempts[rate_limit_key]) >= MAX_ATTEMPTS:
        logger.warning(f"Rate limit exceeded for {client_ip} with key {key_hash}")
        raise HTTPException(
            status_code=status.HTTP_429_TOO_MANY_REQUESTS,
            detail=f"Too many failed attempts. Try again in {LOCKOUT_DURATION.total_seconds() / 60} minutes"
        )
    
    # Use timing-safe comparison to prevent timing attacks
    if not hmac.compare_digest(x_api_key, API_KEY):
        # Record failed attempt
        api_key_attempts[rate_limit_key].append(current_time)
        logger.warning(f"Invalid API key attempt from {client_ip} with key {key_hash}")
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Invalid API Key"
        )
    
    # Clear successful attempts
    if rate_limit_key in api_key_attempts:
        del api_key_attempts[rate_limit_key]
    
    return x_api_key

async def verify_user_auth(authorization: str = Header(None)):
    """Verify user authentication via Microsoft token or API key"""
    if not authorization:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Authorization header required"
        )
    
    # Check if it's a Bearer token (Microsoft auth) or API key
    if authorization.startswith('Bearer '):
        # Microsoft token validation
        token = authorization.split(' ')[1]
        
        # For development/testing, allow test tokens
        if token.startswith('test-token-'):
            logger.info("Test token accepted for development")
            return {"type": "test", "user_id": "test-user"}
        
        try:
            # Validate Microsoft token
            auth_service = get_auth_service()
            user_info = auth_service.decode_token(token)
            
            if not user_info or 'oid' not in user_info:
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Invalid Microsoft token"
                )
            
            return {
                "type": "microsoft",
                "user_id": user_info['oid'],
                "email": user_info.get('preferred_username'),
                "name": user_info.get('name')
            }
            
        except Exception as e:
            logger.error(f"Microsoft token validation failed: {e}")
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid or expired token"
            )
    
    elif authorization == API_KEY:
        # API key authentication
        return {"type": "api_key", "user_id": "api_client"}
    
    else:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authorization format"
        )

# Combined dependency: allow either Azure AD Bearer token OR API key
async def verify_auth_or_api_key(request: Request, authorization: str = Header(None), x_api_key: str = Header(None)):
    """Validate Authorization: Bearer <token> (Azure AD) or X-API-Key.
    Prefer a valid API key if present; otherwise try Bearer token.
    This avoids failing when clients include unrelated Bearer tokens (e.g., Graph) alongside a valid API key.
    """
    # First, accept a valid API key if provided
    if x_api_key:
        api_env = os.getenv("API_KEY", "")
        if hmac.compare_digest(x_api_key, api_env):
            return {"type": "api_key", "user_id": "api_client"}
        # If API key is present but invalid, return 403 immediately
        raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail="Invalid API Key")

    # Next, try Microsoft Bearer token if provided
    if authorization and authorization.startswith('Bearer '):
        return await verify_user_auth(authorization)

    # If neither header present, unauthorized
    raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Authorization or API key required")

# Lifespan context manager for startup/shutdown
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage application lifecycle"""
    # Startup
    logger.info("Starting Well Intake API with enhanced database connection management")
    
    # Initialize Database Connection Manager (Agent #4)
    database_url = os.getenv("DATABASE_URL")
    if database_url:
        try:
            from app.database_connection_manager import get_connection_manager, ensure_learning_services_ready
            
            # Get the centralized connection manager
            connection_manager = await get_connection_manager()
            app.state.connection_manager = connection_manager
            
            # Ensure learning services are ready (critical for Agent #3)
            learning_ready = await ensure_learning_services_ready()
            if learning_ready:
                logger.info("Learning services database access verified and ready")
            else:
                logger.warning("Learning services database access issues - some features may be limited")
            
            # Maintain backward compatibility with existing PostgreSQL client
            postgres_client = PostgreSQLClient(database_url)
            await asyncio.wait_for(postgres_client.init_pool(), timeout=30.0)
            await postgres_client.ensure_tables()
            app.state.postgres_client = postgres_client
            
            # Store enhanced client reference for advanced features
            enhanced_client = connection_manager.get_enhanced_client()
            if enhanced_client:
                app.state.enhanced_postgres_client = enhanced_client
                logger.info("Enhanced PostgreSQL client available with 400K context support")
            
            logger.info("Database connection manager initialized successfully")
            logger.info(f"Connection health: {connection_manager.get_health_status().to_dict()}")
            
        except asyncio.TimeoutError:
            logger.warning("Database connection initialization timed out (will continue with limited functionality)")
            app.state.postgres_client = None
            app.state.connection_manager = None
            app.state.enhanced_postgres_client = None
        except Exception as e:
            logger.warning(f"Database connection initialization failed (will continue with limited functionality): {e}")
            logger.error(f"Database error traceback: {traceback.format_exc()}")
            app.state.postgres_client = None
            app.state.connection_manager = None
            app.state.enhanced_postgres_client = None
    else:
        logger.warning("DATABASE_URL not configured, database features disabled")
        app.state.postgres_client = None
        app.state.connection_manager = None
        app.state.enhanced_postgres_client = None
    
    # Initialize vector store if available
    try:
        from app.vector_store import get_vector_store
        vector_store = await get_vector_store()
        app.state.vector_store = vector_store
        logger.info("Cosmos DB vector store initialized")
    except Exception as e:
        logger.warning(f"Vector store initialization failed (will continue without): {e}")
        app.state.vector_store = None
    
    # Initialize Zoho integration with PostgreSQL client
    app.state.zoho_integration = ZohoIntegration(
        oauth_service_url=os.getenv("ZOHO_OAUTH_SERVICE_URL", "https://well-zoho-oauth.azurewebsites.net")
    )
    # Pass PostgreSQL client to Zoho client for caching
    if hasattr(app.state, 'postgres_client') and app.state.postgres_client:
        app.state.zoho_integration.pg_client = app.state.postgres_client
        logger.info("Zoho integration initialized with PostgreSQL caching")

    # Initialize duplicate checker with PostgreSQL client
    app.state.duplicate_checker = DuplicateChecker()
    if hasattr(app.state, 'postgres_client') and app.state.postgres_client:
        app.state.duplicate_checker.postgres_client = app.state.postgres_client
        logger.info("Duplicate checker initialized with PostgreSQL support")

    # Initialize Service Bus manager if connection string is available
    service_bus_conn = os.getenv("SERVICE_BUS_CONNECTION_STRING") or os.getenv("AZURE_SERVICE_BUS_CONNECTION_STRING")
    if service_bus_conn:
        try:
            from app.service_bus_manager import ServiceBusManager
            app.state.service_bus_manager = ServiceBusManager()
            await app.state.service_bus_manager.connect()
            logger.info("Azure Service Bus manager initialized for batch processing")
        except Exception as e:
            logger.warning(f"Service Bus initialization failed (batch processing disabled): {e}")
            app.state.service_bus_manager = None
    else:
        logger.info("Service Bus not configured, batch processing will use direct mode")
        app.state.service_bus_manager = None
    
    # Initialize Microsoft Graph client for real email integration
    try:
        app.state.graph_client = MicrosoftGraphClient()
        is_connected = await app.state.graph_client.test_connection()
        if is_connected:
            logger.info("Microsoft Graph client initialized successfully")
        else:
            logger.warning("Microsoft Graph client initialization failed - using fallback mode")
            app.state.graph_client = None
    except Exception as e:
        logger.warning(f"Microsoft Graph client initialization error: {e}")
        app.state.graph_client = None
    
    # Initialize manifest cache warmup after all services are ready
    try:
        from app.startup_warmup import perform_manifest_startup_warmup, schedule_background_manifest_warmup
        
        # Perform initial cache warmup (fast startup)
        warmup_stats = await perform_manifest_startup_warmup()
        
        if 'error' not in warmup_stats:
            logger.info(f"Manifest cache warmed during startup: {warmup_stats.get('success_rate', '0%')} success rate")
            
            # Schedule comprehensive warmup in background
            await schedule_background_manifest_warmup()
        else:
            logger.warning(f"Manifest warmup failed: {warmup_stats.get('error', 'Unknown error')}")
            
        app.state.manifest_warmup_stats = warmup_stats
        
    except Exception as e:
        logger.warning(f"Manifest warmup initialization failed: {e}")
        app.state.manifest_warmup_stats = {"error": str(e)}
    
    # Initialize Redis monitoring service
    try:
        from app.redis_monitoring import get_monitoring_service
        from app.redis_cache_manager import get_cache_manager
        
        monitoring_service = get_monitoring_service()
        cache_manager = await get_cache_manager()
        
        # Start background monitoring
        await monitoring_service.start_monitoring(cache_manager, interval_minutes=5)
        
        app.state.redis_monitoring = monitoring_service
        logger.info("Redis monitoring service initialized and started")
        
    except Exception as e:
        logger.warning(f"Redis monitoring initialization failed: {e}")
        app.state.redis_monitoring = None
    
    # Initialize Learning Services with Enhanced Database Connection Manager (Agent #3 + #4)
    # Always available for all email processing - critical for AI learning and improvement
    try:
        from app.correction_learning import CorrectionLearningService
        from app.learning_analytics import LearningAnalytics
        
        # Initialize correction learning service with enhanced database connection
        if hasattr(app.state, 'connection_manager') and app.state.connection_manager:
            # Use connection manager for reliable database access
            app.state.correction_service = CorrectionLearningService(
                app.state.connection_manager,  # Enhanced connection manager
                use_azure_search=True
            )
            
            # Initialize learning analytics with the correction service's search manager
            app.state.learning_analytics = LearningAnalytics(
                search_manager=app.state.correction_service.search_manager,
                enable_ab_testing=True
            )
            
            logger.info("Learning services initialized successfully (CorrectionLearningService + LearningAnalytics)")
        else:
            logger.warning("PostgreSQL not available - learning services disabled")
            app.state.correction_service = None
            app.state.learning_analytics = None
            
    except Exception as e:
        logger.warning(f"Learning services initialization failed: {e}")
        app.state.correction_service = None
        app.state.learning_analytics = None
    
    yield
    
    # Shutdown
    logger.info("Shutting down Well Intake API")
    
    # Clean up vector store
    if hasattr(app.state, 'vector_store') and app.state.vector_store:
        try:
            from app.vector_store import cleanup_vector_store
            await cleanup_vector_store()
        except Exception as e:
            logger.error(f"Error cleaning up vector store: {e}")
    
    # Clean up database connection manager first (Agent #4)
    if hasattr(app.state, 'connection_manager') and app.state.connection_manager:
        try:
            await app.state.connection_manager.cleanup()
            logger.info("Database connection manager cleaned up successfully")
        except Exception as e:
            logger.error(f"Error cleaning up connection manager: {e}")
    
    # Clean up traditional postgres client (maintained for compatibility)
    if hasattr(app.state, 'postgres_client') and app.state.postgres_client:
        try:
            await app.state.postgres_client.close()
        except Exception as e:
            logger.error(f"Error closing postgres client: {e}")
    
    # Clean up service bus manager
    if hasattr(app.state, 'service_bus_manager') and app.state.service_bus_manager:
        try:
            await app.state.service_bus_manager.close()
        except Exception as e:
            logger.error(f"Error closing service bus manager: {e}")
    
    # Clean up learning services (Agent #3)
    if hasattr(app.state, 'correction_service'):
        app.state.correction_service = None
    if hasattr(app.state, 'learning_analytics'):
        app.state.learning_analytics = None
    
    # Clean up Redis monitoring
    if hasattr(app.state, 'redis_monitoring') and app.state.redis_monitoring:
        try:
            await app.state.redis_monitoring.stop_monitoring()
        except Exception as e:
            logger.error(f"Error stopping Redis monitoring: {e}")
    
    logger.info("Well Intake API shutdown completed")

# Create FastAPI app
app = FastAPI(
    title="Well Intake API",
    description="Intelligent email processing with Cosmos DB",
    version="2.0.0",
    lifespan=lifespan
)

# Configure CORS with strict domain allowlist
ALLOWED_ORIGINS = [
    # Azure Front Door domain
    "https://well-intake-api-dnajdub4azhjcgc3.z03.azurefd.net",
    
    # Custom domain for Outlook Add-in (when DNS is configured)
    "https://addin.emailthewell.com",
    
    # Azure Static Web App
    "https://proud-ocean-087af290f.2.azurestaticapps.net",
    # Temporary Azure Storage Static Website (UI)
    "https://wellintakewebui78196327.z13.web.core.windows.net",
    
    # Azure Container Apps domains
    "https://well-intake-api.wittyocean-dfae0f9b.eastus.azurecontainerapps.io",
    
    # Microsoft Office domains
    "https://outlook.office.com",
    "https://outlook.office365.com",
    "https://outlook.live.com",
    "https://outlook.office365-ppe.com",
    "https://outlook-sdf.office.com",
    "https://outlook.office365.us",
    "https://outlook.office365.cn",
    
    # Development environments (remove in production)
    "http://localhost:3000",
    "http://localhost:8000",
    "http://127.0.0.1:3000",
    "http://127.0.0.1:8000"
]

# Remove localhost origins in production
if os.getenv("ENVIRONMENT", "production") == "production":
    ALLOWED_ORIGINS = [origin for origin in ALLOWED_ORIGINS if not origin.startswith("http://localhost") and not origin.startswith("http://127.0.0.1")]

# Security Headers Middleware
class SecurityHeadersMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        response = await call_next(request)
        response.headers["X-Content-Type-Options"] = "nosniff"
        
        # Allow framing for Outlook Add-in endpoints
        add_in_paths = ["/manifest.xml", "/commands.html", "/commands.js", "/taskpane.html",
                       "/taskpane.js", "/placeholder.html", "/loader.html", "/config.js",
                       "/icon-16.png", "/icon-32.png", "/icon-64.png", "/icon-80.png", "/icon-128.png",
                       "/icons/",  # versioned icon path prefix
                       "/addin/manifest.xml", "/addin/commands.html", "/addin/commands.js",
                       "/addin/taskpane.html", "/addin/taskpane.js", "/addin/config.js",
                       "/addin/icon-16.png", "/addin/icon-32.png", "/addin/icon-64.png", "/addin/icon-80.png", "/addin/icon-128.png",
                       "/apollo-styles.css"]  # Apollo integration files
        if not any(request.url.path.startswith(path) for path in add_in_paths):
            # Only set X-Frame-Options for non-add-in endpoints
            response.headers["X-Frame-Options"] = "SAMEORIGIN"
        else:
            # For add-in endpoints, allow framing from Office domains
            if "X-Frame-Options" in response.headers:
                del response.headers["X-Frame-Options"]
            
        response.headers["X-XSS-Protection"] = "1; mode=block"
        response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"
        response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
        response.headers["Permissions-Policy"] = "geolocation=(), microphone=(), camera=()"
        
        # Adjust CSP for add-in endpoints
        if any(request.url.path.startswith(path) for path in add_in_paths):
            # More permissive CSP for add-in pages
            response.headers["Content-Security-Policy"] = (
                "default-src 'self' https://*.office.com https://*.office365.com https://*.microsoft.com; "
                "script-src 'self' 'unsafe-inline' 'unsafe-eval' https://appsforoffice.microsoft.com https://*.office.com https://ajax.aspnetcdn.com https://*.azurefd.net; "
                "style-src 'self' 'unsafe-inline' https://*.office.com; "
                "img-src 'self' data: https://*.office.com https://*.microsoft.com https://*.azurecontainerapps.io; "
                "connect-src 'self' https://*.azurecontainerapps.io https://*.office.com https://*.azurefd.net; "
                "frame-src 'self' https://*.office.com https://*.office365.com https://*.microsoft.com https://telemetryservice.firstpartyapps.oaspapps.com; "
                "frame-ancestors https://outlook.office.com https://outlook.office365.com https://*.outlook.com https://outlook.officeppe.com https://*.microsoft.com https://*.office.com;"
            )
        else:
            # Standard CSP for API endpoints
            response.headers["Content-Security-Policy"] = "default-src 'self'; script-src 'self' 'unsafe-inline' https://appsforoffice.microsoft.com https://*.azurefd.net; style-src 'self' 'unsafe-inline';"
        
        return response

# Add security headers middleware
app.add_middleware(SecurityHeadersMiddleware)

# Add Trusted Host middleware to prevent host header attacks
ALLOWED_HOSTS = [
    "*.azurecontainerapps.io",
    "localhost",
    "127.0.0.1",
    "testserver"  # For FastAPI TestClient
]

if os.getenv("ENVIRONMENT", "production") == "production":
    ALLOWED_HOSTS = ["*.azurecontainerapps.io"]

# Add TrustedHostMiddleware - disabled in test environment
if os.getenv("ENVIRONMENT", "production") != "test":
    app.add_middleware(
        TrustedHostMiddleware,
        allowed_hosts=ALLOWED_HOSTS
    )

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    # Allow Chrome extension and any Azure Static Web App domains
    allow_origin_regex=r"chrome-extension:\/\/.*|https:\/\/.*\\.azurestaticapps\\.net",
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
    expose_headers=["X-RateLimit-Limit", "X-RateLimit-Remaining", "X-RateLimit-Reset"]
)

# Include streaming endpoints
from app.streaming_endpoints import router as streaming_router
app.include_router(streaming_router)

# Include manifest template endpoints
from app.manifest_endpoints import router as manifest_router
app.include_router(manifest_router)

# Include manifest monitoring dashboard
from app.manifest_monitoring import router as manifest_monitoring_router
app.include_router(manifest_monitoring_router)

# Include CDN management endpoints
try:
    from app.cdn_endpoints import router as cdn_router
    app.include_router(cdn_router)
    logger.info("CDN router loaded successfully")
except ImportError as e:
    logger.error(f"Failed to load CDN router: {e}")
except Exception as e:
    logger.error(f"Error loading CDN router: {e}")

# Initialize services (zoho_integration will be initialized in lifespan)
business_rules = BusinessRulesEngine()
blob_storage = AzureBlobStorage(
    connection_string=os.getenv("AZURE_STORAGE_CONNECTION_STRING", ""),
    container_name=os.getenv("AZURE_CONTAINER_NAME", "email-attachments")
)

# Removed: SerperDev API (deprecated)

async def run_curator_and_send(filters: WeeklyDigestFilters, req: Request) -> Dict[str, Any]:
    """Helper function to run curator and optionally send digest emails"""
    from datetime import datetime, timedelta
    
    # Parse dates from filters
    from_date = None
    to_date = None
    if filters.from_:
        from_date = datetime.strptime(filters.from_, "%Y-%m-%d")
    if filters.to_date:
        to_date = datetime.strptime(filters.to_date, "%Y-%m-%d")
    
    # Initialize curator
    from app.jobs.talentwell_curator import TalentWellCurator
    curator = TalentWellCurator()
    await curator.initialize()
    
    # Run curator with initial filters
    digest_result = await curator.run_weekly_digest(
        audience=filters.audience,
        from_date=from_date,
        to_date=to_date,
        owner=filters.owner,
        dry_run=filters.dry_run,  # Use the actual dry_run setting from request
        ignore_cooldown=filters.ignore_cooldown  # Pass ignore_cooldown parameter
    )
    
    cards_count = digest_result['manifest']['cards_count']
    
    # Check for fallback if no candidates found
    if cards_count == 0 and filters.fallback_if_empty:
        logger.info("No candidates found, applying fallback strategy")
        
        # Fallback: widen date window to 30 days and remove owner filter
        fallback_to_date = to_date or datetime.now()
        fallback_from_date = fallback_to_date - timedelta(days=30)
        
        logger.info(f"Fallback: expanding date range to {fallback_from_date} - {fallback_to_date}, owner=None")
        
        digest_result = await curator.run_weekly_digest(
            audience=filters.audience,
            from_date=fallback_from_date,
            to_date=fallback_to_date,
            owner=None,  # Remove owner filter
            dry_run=filters.dry_run,  # Use the actual dry_run setting from request
            ignore_cooldown=filters.ignore_cooldown  # Maintain ignore_cooldown setting
        )
        cards_count = digest_result['manifest']['cards_count']
    
    # Determine recipients
    recipients = []
    if filters.recipients:
        recipients = filters.recipients
    else:
        # Use default internal recipients
        default_recipients = os.getenv('TALENTWELL_RECIPIENTS_INTERNAL', '')
        if default_recipients:
            recipients = [email.strip() for email in default_recipients.split(',') if email.strip()]
    
    # Validate recipients if not dry run
    if not filters.dry_run and not recipients:
        raise HTTPException(
            status_code=422, 
            detail="No recipients provided. Include 'to' field or set TALENTWELL_RECIPIENTS_INTERNAL environment variable"
        )
    
    errors = []
    sent = False
    
    # Check if we have no candidates after fallback
    if cards_count == 0:
        errors.append("No candidates")
    
    # Send email if not dry run and has recipients (even with 0 candidates, for testing)
    if not filters.dry_run and recipients:
        if cards_count > 0:
            try:
                from app.mail.send import mailer
                
                # Send to each recipient
                for recipient in recipients:
                    delivery_result = await mailer.send_test_digest(
                        subject=digest_result['subject'],
                        html_content=digest_result['email_html'],
                        test_recipient=recipient
                    )
                    
                    if not delivery_result.get('success', False):
                        errors.append(f"Failed to send to {recipient}: {delivery_result.get('error', 'Unknown error')}")
                
                sent = len(errors) == 1 and errors[0] == "No candidates"  # Only "No candidates" error means email was skipped
                
            except Exception as e:
                errors.append(f"Email sending failed: {str(e)}")
        else:
            # No candidates, so we don't send the email
            sent = False
    
    # Build response
    response = {
        "sent": sent,
        "subject": digest_result['subject'],
        "cards_count": cards_count,
        "manifest": digest_result['manifest']
    }
    
    # Include HTML for dry runs or errors
    if filters.dry_run or not sent:
        response["email_html"] = digest_result['email_html']
    
    # Include errors if any
    if errors:
        response["errors"] = errors
    
    return response


@app.get("/")
async def root():
    """Root endpoint with service information"""
    return {
        "service": "Well Intake API",
        "version": "2.1.0",
        "status": "operational",
        "storage": "Cosmos DB for PostgreSQL",
        "vector_support": "pgvector",
        "batch_processing": "Azure Service Bus with GPT-5-mini",
        "max_batch_size": 50,
        "context_window": "400K tokens",
        "no_sqlite": True,
        "endpoints": [
            "/health",
            "/health/database",
            "/intake/email",
            "/batch/submit",
            "/batch/process",
            "/batch/status/{batch_id}",
            "/batch/queue/status",
            "/batch/queue/process",
            "/batch/deadletter/process",
            "/test/kevin-sullivan",
            "/cache/status",
            "/cache/invalidate",
            "/cache/warmup",
            "/manifest.xml",
            "/commands.js",
            "/auth/login",
            "/auth/callback",
            "/api/user/info",
            "/api/user/logout",
            "/api/auth/users",
            "/api/emails/queue",
            "/api/emails/{email_id}",
            "/api/voice/process",
            "/api/webhook/github",
            "/api/webhook/github/status",
            "/api/webhook/github/test"
        ]
    }

@app.get("/learning/analytics/{field_name}", dependencies=[Depends(verify_api_key)])
async def get_field_analytics(field_name: str, days_back: int = 30, domain: Optional[str] = None, request: Request = None):
    """Get learning analytics for a specific extraction field"""
    try:
        # Input validation
        if not field_name or len(field_name) > 50:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid field name"
            )
        
        if days_back < 1 or days_back > 365:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="days_back must be between 1 and 365"
            )
        
        if domain and not domain.replace('.', '').replace('-', '').isalnum():
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid domain format"
            )
        
        # Use globally initialized learning analytics service
        learning_analytics = getattr(request.app.state, 'learning_analytics', None)
        
        if not learning_analytics:
            return {
                "field_name": field_name,
                "analytics_disabled": True,
                "message": "Learning analytics service not available"
            }
        
        # Get field analytics
        # Temporarily disable analytics
        result = {
            "field_name": field_name,
            "analytics_disabled": True,
            "message": "Analytics temporarily disabled"
        }
        # result = await learning_analytics.get_field_analytics(
        #     field_name=field_name,
        #     days_back=days_back,
        #     email_domain=domain
        # )
        
        return result
    except Exception as e:
        logger.error(f"Failed to get field analytics: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/learning/variants", dependencies=[Depends(verify_api_key)])
async def get_prompt_variants(request: Request):
    """Get A/B testing report for prompt variants"""
    try:
        # Use globally initialized learning analytics service
        learning_analytics = getattr(request.app.state, 'learning_analytics', None)
        
        if not learning_analytics:
            return {
                "variants_disabled": True,
                "message": "Learning analytics service not available"
            }
        
        report = await learning_analytics.get_variant_report()
        
        return report
    except Exception as e:
        logger.error(f"Failed to get variant report: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/learning/insights", dependencies=[Depends(verify_api_key)])
async def get_learning_insights(domain: Optional[str] = None, days_back: int = 30, request: Request = None):
    """Get overall learning insights from Azure AI Search with pattern matching metrics"""
    try:
        # Use the globally initialized search manager if available
        search_manager = None
        if request and hasattr(request.app.state, 'correction_service') and request.app.state.correction_service:
            search_manager = request.app.state.correction_service.search_manager
        
        if not search_manager:
            # Fallback to creating a new instance
            try:
                from app.azure_ai_search_manager import AzureAISearchManager
                search_manager = AzureAISearchManager()
            except Exception as e:
                logger.warning(f"Could not initialize Azure AI Search: {e}")
                # Return default insights when search is not available
                return {
                    "total_patterns": 0,
                    "confidence_improvement": 0,
                    "most_common_corrections": [],
                    "learning_rate": 0,
                    "company_templates": 0,
                    "patterns_used_for_enhancement": 0,
                    "successful_pattern_matches": 0,
                    "average_pattern_confidence": 0,
                    "most_effective_domains": [],
                    "message": "Learning insights temporarily unavailable"
                }

        insights = await search_manager.get_learning_insights(
            email_domain=domain,
            days_back=days_back
        )

        # Enhance with pattern matching effectiveness metrics
        if search_manager:
            try:
                # Get recent pattern usage statistics
                pattern_stats = {
                    "patterns_used_for_enhancement": 0,
                    "successful_pattern_matches": 0,
                    "average_pattern_confidence": 0,
                    "most_effective_domains": []
                }
                
                # This would be based on actual usage logs in a production system
                # For now, provide basic insights based on indexed patterns
                if insights.get("total_patterns", 0) > 0:
                    pattern_stats["patterns_used_for_enhancement"] = insights.get("total_patterns", 0)
                    pattern_stats["average_pattern_confidence"] = insights.get("average_confidence", 0)
                    pattern_stats["successful_pattern_matches"] = int(insights.get("total_patterns", 0) * insights.get("average_success_rate", 0.7))
                
                insights["pattern_matching"] = pattern_stats
                insights["ai_search_active"] = True
                
            except Exception as e:
                logger.warning(f"Could not enhance insights with pattern metrics: {e}")
                insights["ai_search_active"] = False
        
        return insights
    except Exception as e:
        logger.error(f"Failed to get learning insights: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/ai-search/patterns/search", dependencies=[Depends(verify_api_key)])
async def search_patterns(
    email_content: str, 
    email_domain: Optional[str] = None, 
    top_k: int = 5,
    min_confidence: float = 0.6,
    request: Request = None
):
    """Search for similar email patterns using AI Search"""
    try:
        if not request or not hasattr(request.app.state, 'correction_service'):
            raise HTTPException(status_code=503, detail="AI Search service not available")
            
        search_manager = request.app.state.correction_service.search_manager
        if not search_manager:
            raise HTTPException(status_code=503, detail="Azure AI Search not configured")
        
        patterns = await search_manager.search_similar_patterns(
            email_content=email_content,
            email_domain=email_domain,
            top_k=top_k,
            min_confidence=min_confidence
        )
        
        return {
            "query_domain": email_domain,
            "patterns_found": len(patterns),
            "min_confidence": min_confidence,
            "patterns": patterns,
            "search_metadata": {
                "timestamp": datetime.utcnow().isoformat(),
                "content_length": len(email_content)
            }
        }
        
    except Exception as e:
        logger.error(f"Pattern search failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/ai-search/templates/{company_domain}", dependencies=[Depends(verify_api_key)])
async def get_company_template(company_domain: str, request: Request):
    """Get company-specific extraction template"""
    try:
        if not hasattr(request.app.state, 'correction_service'):
            raise HTTPException(status_code=503, detail="AI Search service not available")
            
        search_manager = request.app.state.correction_service.search_manager
        if not search_manager:
            raise HTTPException(status_code=503, detail="Azure AI Search not configured")
        
        template = await search_manager.get_company_template(company_domain)
        
        if not template:
            raise HTTPException(status_code=404, detail=f"No template found for domain: {company_domain}")
        
        return {
            "company_domain": company_domain,
            "template": template,
            "retrieved_at": datetime.utcnow().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Template retrieval failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/ai-search/patterns/index", dependencies=[Depends(verify_api_key)])
async def index_email_pattern(
    email_domain: str,
    email_content: str,
    extraction_result: Dict[str, Any],
    corrections: Optional[Dict[str, Any]] = None,
    confidence_score: float = 0.5,
    request: Request = None
):
    """Manually index an email pattern for learning"""
    try:
        if not hasattr(request.app.state, 'correction_service'):
            raise HTTPException(status_code=503, detail="AI Search service not available")
            
        search_manager = request.app.state.correction_service.search_manager
        if not search_manager:
            raise HTTPException(status_code=503, detail="Azure AI Search not configured")
        
        success = await search_manager.index_email_pattern(
            email_domain=email_domain,
            email_content=email_content,
            extraction_result=extraction_result,
            corrections=corrections,
            confidence_score=confidence_score
        )
        
        if success:
            # Also update company template
            await search_manager.update_company_template(
                company_domain=email_domain,
                extraction_data=corrections or extraction_result,
                corrections=corrections
            )
            
            return {
                "status": "success",
                "message": "Pattern indexed successfully",
                "email_domain": email_domain,
                "confidence_score": confidence_score,
                "has_corrections": corrections is not None,
                "indexed_at": datetime.utcnow().isoformat()
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to index pattern")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Pattern indexing failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/ai-search/status", dependencies=[Depends(verify_api_key)])
async def get_ai_search_status(request: Request):
    """Get Azure AI Search service status and metrics"""
    try:
        status = {
            "service_available": False,
            "search_configured": False,
            "patterns_indexed": 0,
            "templates_stored": 0,
            "last_activity": None,
            "configuration": {
                "endpoint": bool(os.getenv("AZURE_SEARCH_ENDPOINT")),
                "api_key": bool(os.getenv("AZURE_SEARCH_KEY")),
                "openai_embeddings": bool(os.getenv("OPENAI_API_KEY"))
            }
        }
        
        if hasattr(request.app.state, 'correction_service') and request.app.state.correction_service:
            status["service_available"] = True
            search_manager = request.app.state.correction_service.search_manager
            
            if search_manager and search_manager.search_client:
                status["search_configured"] = True
                
                # Get basic insights for metrics
                try:
                    insights = await search_manager.get_learning_insights(days_back=7)
                    status["patterns_indexed"] = insights.get("total_patterns", 0)
                    status["average_confidence"] = insights.get("average_confidence", 0)
                    status["learning_insights"] = insights.get("insights", [])
                    
                    if insights.get("total_patterns", 0) > 0:
                        status["last_activity"] = "recent"
                        
                except Exception as e:
                    logger.warning(f"Could not fetch AI Search metrics: {e}")
        
        return {
            "timestamp": datetime.utcnow().isoformat(),
            "ai_search": status
        }
        
    except Exception as e:
        logger.error(f"AI Search status check failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/health")
async def health_check():
    """Health check endpoint with service status"""
    health_status = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "services": {
            "api": "operational",
            "cosmos_db": "unknown",
            "zoho": "unknown",
            "blob_storage": "unknown",
            "openai": "configured" if os.getenv("OPENAI_API_KEY") else "missing",
            "vector_store": "cosmos_db_pgvector",
            "redis_cache": "unknown",
            "ai_search": "unknown"
        },
        "environment": os.getenv("ENVIRONMENT", "production"),
        "no_sqlite": True
    }
    
    # Check Database Connection Manager (Agent #4)
    try:
        if hasattr(app.state, 'connection_manager') and app.state.connection_manager:
            conn_health = app.state.connection_manager.get_health_status()
            if conn_health.is_healthy:
                health_status["services"]["cosmos_db"] = "operational"
                health_status["database_connection_manager"] = {
                    "status": "healthy",
                    "connections": conn_health.connection_count,
                    "active_connections": conn_health.active_connections,
                    "avg_response_time_ms": round(conn_health.avg_response_time_ms, 2),
                    "total_queries": conn_health.total_queries,
                    "enhanced_client_available": hasattr(app.state, 'enhanced_postgres_client') and app.state.enhanced_postgres_client is not None,
                    "learning_services_ready": True  # Will be verified below
                }
            else:
                health_status["services"]["cosmos_db"] = "unhealthy"
                health_status["database_connection_manager"] = {
                    "status": "unhealthy",
                    "last_error": conn_health.last_error,
                    "failed_attempts": conn_health.failed_attempts
                }
        elif hasattr(app.state, 'postgres_client') and app.state.postgres_client:
            # Fallback to legacy client check
            await app.state.postgres_client.test_connection()
            health_status["services"]["cosmos_db"] = "operational"
            health_status["database_connection_manager"] = {
                "status": "legacy_mode",
                "note": "Using legacy PostgreSQL client instead of connection manager"
            }
        else:
            health_status["services"]["cosmos_db"] = "not_configured"
            health_status["database_connection_manager"] = {
                "status": "not_configured"
            }
    except Exception as e:
        logger.warning(f"Database health check failed: {e}")
        health_status["services"]["cosmos_db"] = "error"
        health_status["database_connection_manager"] = {
            "status": "error",
            "error": str(e)
        }
    
    # Check Zoho
    try:
        if hasattr(app.state, 'zoho_integration') and app.state.zoho_integration:
            app.state.zoho_integration.get_access_token()
            health_status["services"]["zoho"] = "operational"
        else:
            health_status["services"]["zoho"] = "not_initialized"
    except Exception as e:
        logger.warning(f"Zoho health check failed: {e}")
        health_status["services"]["zoho"] = "error"
    
    # Check Blob Storage
    try:
        if blob_storage.test_connection():
            health_status["services"]["blob_storage"] = "operational"
        else:
            health_status["services"]["blob_storage"] = "container_not_found"
    except Exception as e:
        logger.warning(f"Blob Storage health check failed: {e}")
        health_status["services"]["blob_storage"] = "error"
    
    # Check Redis Cache with enhanced fallback status
    try:
        from app.redis_cache_manager import get_redis_health_status
        redis_health = await get_redis_health_status()
        
        # Map Redis health status to service status
        status_mapping = {
            "healthy": "operational",
            "unhealthy": "degraded", 
            "not_configured": "not_configured",
            "circuit_breaker_open": "circuit_breaker_open",
            "fallback_mode": "fallback_mode",
            "error": "error"
        }
        
        health_status["services"]["redis_cache"] = status_mapping.get(
            redis_health.get("status"), "unknown"
        )
        
        # Include detailed Redis metrics and fallback information
        health_status["redis_details"] = {
            "connection_status": redis_health.get("connection_status"),
            "circuit_breaker_status": redis_health.get("circuit_breaker_status"),
            "fallback_mode": redis_health.get("fallback_mode"),
            "response_time_ms": redis_health.get("response_time_ms"),
            "error": redis_health.get("error")
        }
        
        # Get performance metrics if available
        try:
            from app.redis_cache_manager import get_cache_manager
            cache_manager = await get_cache_manager()
            metrics = await cache_manager.get_metrics()
            health_status["cache_metrics"] = {
                "hit_rate": metrics.get("hit_rate", "0%"),
                "total_requests": metrics.get("total_requests", 0),
                "estimated_savings": f"${metrics.get('savings', 0):.2f}",
                "uptime_percentage": metrics.get("uptime_percentage", "0%"),
                "fallback_activations": metrics.get("fallback_activations", 0)
            }
        except:
            health_status["cache_metrics"] = {
                "hit_rate": "0%",
                "total_requests": 0,
                "estimated_savings": "$0.00",
                "uptime_percentage": "0%",
                "fallback_activations": 0
            }
            
    except Exception as e:
        logger.warning(f"Redis cache health check failed: {e}")
        health_status["services"]["redis_cache"] = "error"
        health_status["redis_details"] = {
            "error": f"Health check failed: {str(e)}"
        }
    
    # Check Azure AI Search
    try:
        if hasattr(app.state, 'correction_service') and app.state.correction_service:
            search_manager = app.state.correction_service.search_manager
            if search_manager and search_manager.search_client:
                # Quick health check by attempting to get insights
                try:
                    insights = await search_manager.get_learning_insights(days_back=1)
                    health_status["services"]["ai_search"] = "operational"
                    health_status["ai_search_details"] = {
                        "patterns_indexed": insights.get("total_patterns", 0),
                        "average_confidence": insights.get("average_confidence", 0),
                        "status": "active_learning"
                    }
                except Exception:
                    # Search service is configured but having issues
                    health_status["services"]["ai_search"] = "degraded"
                    health_status["ai_search_details"] = {
                        "status": "configured_but_unresponsive",
                        "fallback": "pattern_learning_disabled"
                    }
            else:
                health_status["services"]["ai_search"] = "not_configured"
                health_status["ai_search_details"] = {
                    "endpoint": bool(os.getenv("AZURE_SEARCH_ENDPOINT")),
                    "api_key": bool(os.getenv("AZURE_SEARCH_KEY")),
                    "status": "configuration_missing"
                }
        else:
            health_status["services"]["ai_search"] = "not_initialized"
            health_status["ai_search_details"] = {
                "status": "learning_services_not_available"
            }
    except Exception as e:
        logger.warning(f"AI Search health check failed: {e}")
        health_status["services"]["ai_search"] = "error"
        health_status["ai_search_details"] = {
            "error": str(e),
            "status": "health_check_failed"
        }
    
    return health_status


@app.get("/health/database", dependencies=[Depends(verify_api_key)])
async def database_health_check():
    """Detailed database connection health check for Agent #4"""
    try:
        if hasattr(app.state, 'connection_manager') and app.state.connection_manager:
            # Get comprehensive health report from connection manager
            health_report = app.state.connection_manager.get_health_report()
            
            # Test learning services readiness
            from app.database_connection_manager import ensure_learning_services_ready
            learning_ready = await ensure_learning_services_ready()
            
            health_report['learning_services'] = {
                'ready': learning_ready,
                'correction_service_available': hasattr(app.state, 'correction_service') and app.state.correction_service is not None,
                'learning_analytics_available': hasattr(app.state, 'learning_analytics') and app.state.learning_analytics is not None
            }
            
            # Add table verification
            try:
                async with app.state.connection_manager.get_connection() as conn:
                    tables = await conn.fetch("""
                        SELECT table_name 
                        FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        AND table_name IN ('ai_corrections', 'learning_patterns', 'extraction_analytics', 'company_templates')
                        ORDER BY table_name
                    """)
                    
                    health_report['learning_tables'] = {
                        'available': [row['table_name'] for row in tables],
                        'expected': ['ai_corrections', 'learning_patterns', 'extraction_analytics', 'company_templates'],
                        'all_present': len(tables) == 4
                    }
            except Exception as e:
                health_report['learning_tables'] = {
                    'error': str(e),
                    'all_present': False
                }
            
            return {
                'status': 'healthy' if health_report['status']['is_healthy'] else 'unhealthy',
                'timestamp': datetime.utcnow().isoformat(),
                **health_report
            }
        else:
            return {
                'status': 'not_configured',
                'timestamp': datetime.utcnow().isoformat(),
                'error': 'Database connection manager not initialized',
                'fallback_available': hasattr(app.state, 'postgres_client') and app.state.postgres_client is not None
            }
    except Exception as e:
        logger.error(f"Database health check failed: {e}")
        return {
            'status': 'error',
            'timestamp': datetime.utcnow().isoformat(),
            'error': str(e)
        }


@app.post("/intake/email", response_model=ZohoResponse)
async def process_email(request: EmailRequest, req: Request, _auth=Depends(verify_auth_or_api_key)):
    """Process email and create Zoho CRM records with bulletproof persistence and idempotency"""
    
    # Generate correlation ID for tracking this request
    import uuid
    import hashlib
    import json
    import asyncio
    import os
    from datetime import datetime
    
    correlation_id = str(uuid.uuid4())
    
    try:
        # Input validation and sanitization
        import re
        
        # CRITICAL: Remove null bytes and control characters from all input fields
        # This prevents 500 errors from corrupted .msg file parsing
        if request.sender_email:
            request.sender_email = request.sender_email.replace('\x00', '').strip()
            request.sender_email = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F]', '', request.sender_email)
        
        if request.subject:
            request.subject = request.subject.replace('\x00', '').strip()
            request.subject = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F]', '', request.subject)
        
        if request.body:
            request.body = request.body.replace('\x00', '')
            request.body = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F]', '', request.body)
        
        # Validate required fields
        if not request.subject:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Subject is required for deal_name. Correlation ID: {correlation_id}"
            )
        
        # Validate email format after cleaning
        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        if not re.match(email_pattern, request.sender_email):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Invalid sender email format. Correlation ID: {correlation_id}"
            )
        
        # Normalize email addresses (lowercase)
        request.sender_email = request.sender_email.lower()
        
        # Validate body length (prevent excessive processing)
        if len(request.body) > 100000:  # 100KB limit
            raise HTTPException(
                status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
                detail=f"Email body too large (max 100KB). Correlation ID: {correlation_id}"
            )
        
        # Sanitize inputs to prevent injection attacks
        if request.sender_name and len(request.sender_name) > 200:
            request.sender_name = request.sender_name[:200]
        
        if request.subject and len(request.subject) > 500:
            request.subject = request.subject[:500]
        
        # IDEMPOTENCY: Generate or use message_id
        message_id = getattr(request, 'internet_message_id', None) or getattr(request, 'message_id', None)
        if not message_id:
            # Generate hash from subject+sender+timestamp for idempotency
            timestamp = datetime.utcnow().strftime("%Y-%m-%d")
            idempotency_string = f"{request.subject}:{request.sender_email}:{timestamp}"
            message_id = hashlib.sha256(idempotency_string.encode()).hexdigest()
        
        logger.info(f"Processing email from {request.sender_email} with correlation_id: {correlation_id}, message_id: {message_id}")
        
        # IDEMPOTENCY CHECK: Check intake_audit table first
        if hasattr(req.app.state, 'postgres_client') and req.app.state.postgres_client:
            await req.app.state.postgres_client.init_pool()
            
            # Check if message was already processed
            check_query = """
            SELECT correlation_id, deal_id, response_payload, outcome
            FROM intake_audit
            WHERE message_id = $1
              AND outcome = 'success'
            ORDER BY created_at DESC
            LIMIT 1
            """
            
            async with req.app.state.postgres_client.pool.acquire() as conn:
                existing_record = await conn.fetchrow(check_query, message_id)
                
                if existing_record:
                    # Return existing result
                    logger.info(f"Found existing successful processing for message_id: {message_id}")
                    response_data = json.loads(existing_record['response_payload']) if existing_record['response_payload'] else {}
                    
                    return ZohoResponse(
                        status="success",
                        deal_id=response_data.get("deal_id"),
                        account_id=response_data.get("account_id"),
                        contact_id=response_data.get("contact_id"),
                        deal_name=response_data.get("deal_name"),
                        primary_email=response_data.get("primary_email"),
                        message="Email already processed (idempotent response)",
                        extracted=response_data.get("extracted")
                    )
        
        # If Graph context provided, enrich body and attachments from Microsoft Graph
        if getattr(request, 'graph_access_token', None) and getattr(request, 'graph_message_id', None):
            try:
                token = request.graph_access_token
                msg_id = request.graph_message_id
                import aiohttp
                async with aiohttp.ClientSession(headers={"Authorization": f"Bearer {token}"}) as session:
                    # Expand attachments (common file attachments inline)
                    url = f"https://graph.microsoft.com/v1.0/me/messages/{msg_id}?$select=subject,from,body,bodyPreview,conversationId,hasAttachments&$expand=attachments($select=id,name,contentType,size,isInline,@odata.type,contentBytes)"
                    async with session.get(url) as r:
                        if r.status == 200:
                            data = await r.json()
                            # Prefer full HTML or text from Graph
                            if data.get('body', {}).get('content'):
                                request.body = data['body']['content'][:100000]
                            if data.get('subject') and not request.subject:
                                request.subject = data['subject']
                            # Map attachments
                            atts = []
                            for a in data.get('attachments', []) or []:
                                otype = a.get('@odata.type', '')
                                if otype.endswith('fileAttachment') and a.get('contentBytes'):
                                    atts.append({
                                        'filename': a.get('name') or 'attachment',
                                        'content_base64': a.get('contentBytes') or '',
                                        'content_type': a.get('contentType') or 'application/octet-stream'
                                    })
                                elif otype.endswith('itemAttachment'):
                                    # Fetch nested item and include its body text inline to aid extraction
                                    att_id = a.get('id')
                                    if att_id:
                                        nested_url = f"https://graph.microsoft.com/v1.0/me/messages/{msg_id}/attachments/{att_id}?$expand=item($select=subject,from,body,hasAttachments)"
                                        async with session.get(nested_url) as nr:
                                            if nr.status == 200:
                                                ndata = await nr.json()
                                                item = ndata.get('item') or {}
                                                nbody = (item.get('body') or {}).get('content') or ''
                                                if nbody:
                                                    # Append nested email content to the main body for AI context
                                                    request.body += "\n\n--- Forwarded/Attached Email ---\n" + nbody[:50000]
                        else:
                            logger.warning(f"Graph message fetch failed: {r.status}")
            except Exception as e:
                logger.warning(f"Graph enrichment failed: {e}")

        # Get globally initialized learning services
        correction_service = getattr(req.app.state, 'correction_service', None)
        learning_analytics = getattr(req.app.state, 'learning_analytics', None)
        prompt_variant = None
        
        # Process user corrections if provided (using globally available services)
        if request.user_corrections and request.ai_extraction and correction_service and learning_analytics:
            try:
                from app.correction_learning import FeedbackLoop
                import hashlib
                from datetime import datetime
                
                # Use the globally initialized correction service
                feedback_loop = FeedbackLoop(correction_service)
                
                # Process and store the user feedback
                feedback_result = await feedback_loop.process_user_feedback(
                    email_data={
                        'sender_email': request.sender_email,
                        'body': request.body
                    },
                    ai_extraction=request.ai_extraction,
                    user_edits=request.user_corrections
                )
                
                # Track correction in analytics
                if feedback_result.get('fields_corrected', 0) > 0:
                    extraction_id = hashlib.md5(
                        f"{request.sender_email}:{datetime.utcnow().isoformat()}".encode()
                    ).hexdigest()
                    
                    field_corrections = {}
                    for field, changes in feedback_result.get('corrections', {}).items():
                        field_corrections[field] = (
                            changes['original'], 
                            changes['corrected']
                        )
                    
                    await learning_analytics.track_correction(
                        extraction_id=extraction_id,
                        field_corrections=field_corrections,
                        prompt_variant_id=prompt_variant.variant_id if prompt_variant else None
                    )
                    
                    # Index correction pattern in Azure AI Search for future pattern learning
                    if correction_service and correction_service.search_manager:
                        try:
                            # Verify AI Search is available before indexing corrections
                            if correction_service.search_manager.search_client:
                                correction_indexed = await correction_service.search_manager.index_email_pattern(
                                    email_domain=sender_domain,
                                    email_content=request.body,
                                    extraction_result=request.ai_extraction,
                                    corrections=request.user_corrections,
                                    confidence_score=0.9  # High confidence for user-corrected data
                                )
                                
                                if correction_indexed:
                                    logger.info("Indexed user correction pattern for future learning")
                                    
                                    # Update company template with corrections
                                    template_updated = await correction_service.search_manager.update_company_template(
                                        company_domain=sender_domain,
                                        extraction_data=request.user_corrections,
                                        corrections=request.user_corrections
                                    )
                                    
                                    if template_updated:
                                        logger.info(f"Updated company template with user corrections for {sender_domain}")
                                    else:
                                        logger.warning(f"Failed to update template with corrections for {sender_domain}")
                                else:
                                    logger.warning("Failed to index correction pattern - future learning may be impacted")
                            else:
                                logger.info("AI Search not available - correction stored in PostgreSQL only")
                                
                        except Exception as e:
                            logger.warning(f"Failed to index correction pattern (learning degraded): {e}")
                            # Don't fail the correction processing - it's still stored in PostgreSQL
                
                logger.info(f"Stored user corrections: {feedback_result['fields_corrected']} fields corrected")
            except Exception as e:
                logger.warning(f"Could not store user corrections: {e}")
        elif request.user_corrections and request.ai_extraction and not correction_service:
            logger.warning("User corrections provided but learning services not available")
        
        # Process attachments
        attachment_urls = []
        if request.attachments:
            for attachment in request.attachments:
                url = await blob_storage.upload_attachment(
                    attachment.filename,
                    attachment.content_base64,
                    attachment.content_type
                )
                if url:
                    attachment_urls.append(url)
        
        # Extract sender domain
        sender_domain = request.sender_email.split('@')[1] if '@' in request.sender_email else 'unknown.com'
        
        # Check if user provided corrections (skip AI if so)
        if request.user_corrections:
            # User has already reviewed and corrected the data
            logger.info("Using user-provided corrections directly")
            from app.models import ExtractedData, CompanyRecord, ContactRecord, DealRecord
            
            # Properly handle structured user_corrections with nested records
            company_record = CompanyRecord(**request.user_corrections.get("company_record", {}))
            contact_record = ContactRecord(**request.user_corrections.get("contact_record", {}))
            deal_record = DealRecord(**request.user_corrections.get("deal_record", {}))
            
            extracted_data = ExtractedData(
                company_record=company_record,
                contact_record=contact_record,
                deal_record=deal_record
            )
        else:
            # PHASE 1: Check Azure AI Search for similar patterns before processing
            similar_patterns = []
            company_template = None
            search_manager = None
            ai_search_available = False
            
            if hasattr(req.app.state, 'correction_service') and req.app.state.correction_service:
                search_manager = req.app.state.correction_service.search_manager
                
                if search_manager and search_manager.search_client:
                    try:
                        # Search for similar email patterns
                        similar_patterns = await search_manager.search_similar_patterns(
                            email_content=request.body,
                            email_domain=sender_domain,
                            top_k=3,
                            min_confidence=0.6
                        )
                        
                        # Get company-specific template
                        company_template = await search_manager.get_company_template(sender_domain)
                        
                        ai_search_available = True
                        if similar_patterns:
                            logger.info(f"Found {len(similar_patterns)} similar email patterns for optimization")
                        if company_template:
                            logger.info(f"Using company template for {sender_domain} (accuracy: {company_template.get('accuracy_score', 0):.2%})")
                        else:
                            logger.info(f"No company template found for {sender_domain}, will learn from this email")
                            
                    except Exception as e:
                        logger.warning(f"AI Search pattern lookup failed, falling back to standard processing: {e}")
                        ai_search_available = False
                        similar_patterns = []
                        company_template = None
                else:
                    logger.info("AI Search not configured - proceeding with standard extraction")
            else:
                logger.info("Learning services not available - proceeding with basic extraction")
            
            # Use LangGraph implementation with pattern enhancement
            try:
                logger.info("Using LangGraph for email processing")
                from app.langgraph_manager import EmailProcessingWorkflow
                
                # Initialize with learning if available
                workflow = EmailProcessingWorkflow()
                
                # Enhance prompts with learned patterns
                learning_hints = ""
                if correction_service:
                    # Get domain patterns for enhanced extraction
                    patterns = await correction_service.get_common_patterns(
                        email_domain=sender_domain,
                        min_frequency=1
                    )
                    if patterns:
                        logger.info(f"Applying {len(patterns)} learned correction patterns")
                        
                # Build learning hints from AI Search patterns
                if similar_patterns or company_template:
                    hints = []
                    
                    # Add insights from similar patterns
                    for pattern in similar_patterns[:2]:  # Top 2 patterns
                        if pattern.get('improvement_suggestions'):
                            hints.extend(pattern['improvement_suggestions'])
                    
                    # Add company template guidance
                    if company_template:
                        common_fields = company_template.get('common_fields', {})
                        for field, values in common_fields.items():
                            if values and len(values) > 0:
                                hints.append(f"For {field}, common values for this company: {', '.join(values[:3])}")
                    
                    if hints:
                        learning_hints = "Pattern guidance: " + "; ".join(hints[:5])  # Top 5 hints
                        logger.info(f"Enhanced extraction with {len(hints)} pattern insights")
                
                extracted_data = await workflow.process_email(
                    request.body,
                    sender_domain,
                    learning_hints=learning_hints
                )
                logger.info(f"Extracted data with LangGraph: {extracted_data}")

                # CLIENT EXTRACTION: If user context is provided, use AI to extract true client details
                if request.user_context:
                    try:
                        from app.client_extractor import extract_client_details_with_ai

                        full_email_content = f"Subject: {request.subject}\n\nBody: {request.body}"
                        client_details = await extract_client_details_with_ai(
                            full_email_content,
                            request.user_context
                        )

                        if client_details:
                            logger.info(f"AI client extraction successful: {client_details}")

                            # Override sender fields if client email/name were extracted
                            if client_details.get('client_email'):
                                request.sender_email = client_details['client_email']
                                logger.info(f"Overrode sender_email with client email: {client_details['client_email']}")

                            if client_details.get('client_name'):
                                request.sender_name = client_details['client_name']
                                logger.info(f"Overrode sender_name with client name: {client_details['client_name']}")

                            # Store additional AI extraction details for downstream use
                            if not request.ai_extraction:
                                request.ai_extraction = {}
                            request.ai_extraction.update(client_details)

                        else:
                            logger.info("AI client extraction found no distinct client information")

                    except Exception as e:
                        logger.error(f"Client extraction error: {str(e)}")
                        # Don't fail the entire processing - this is an enhancement feature

                # COMPREHENSIVE ENRICHMENT: Use Apollo + Firecrawl v2 with FIRE-1 agent for maximum data extraction
                try:
                    from app.firecrawl_enricher import comprehensive_enrichment
                    from app.apollo_enricher import extract_linkedin_urls

                    # Extract company name from existing data for better enrichment
                    company_hint = None
                    if hasattr(extracted_data, 'company_name') and extracted_data.company_name:
                        company_hint = extracted_data.company_name
                    elif hasattr(extracted_data, 'model_dump'):
                        extracted_dict = extracted_data.model_dump()
                        company_hint = extracted_dict.get('company_name')

                    # First, specifically extract LinkedIn URLs for maximum discovery
                    linkedin_data = await extract_linkedin_urls(
                        email=request.sender_email,
                        name=request.sender_name,
                        company=company_hint,
                        save_to_db=True
                    )

                    # Log LinkedIn extraction results
                    if linkedin_data and linkedin_data.get("linkedin_url"):
                        logger.info(f"LinkedIn URL found: {linkedin_data['linkedin_url']}")

                        # Prepare social profiles information
                        social_profiles = []
                        if linkedin_data.get("linkedin_url"):
                            social_profiles.append(f"LinkedIn: {linkedin_data['linkedin_url']}")
                        if linkedin_data.get("company_linkedin_url"):
                            social_profiles.append(f"Company LinkedIn: {linkedin_data['company_linkedin_url']}")
                        if linkedin_data.get("twitter_url"):
                            social_profiles.append(f"Twitter: {linkedin_data['twitter_url']}")
                        if linkedin_data.get("facebook_url"):
                            social_profiles.append(f"Facebook: {linkedin_data['facebook_url']}")
                        if linkedin_data.get("github_url"):
                            social_profiles.append(f"GitHub: {linkedin_data['github_url']}")

                        # Add social profiles to notes
                        if social_profiles:
                            social_note = "\n\nSOCIAL PROFILES:\n" + "\n".join(social_profiles)
                            if hasattr(extracted_data, 'notes'):
                                extracted_data.notes = (extracted_data.notes or "") + social_note
                            elif extracted_dict:
                                extracted_dict['notes'] = extracted_dict.get('notes', '') + social_note

                    # Perform comprehensive enrichment with Apollo + Firecrawl v2 FIRE-1 agent
                    apollo_result = await comprehensive_enrichment(
                        email=request.sender_email,
                        name=request.sender_name,
                        company=company_hint,
                        domain=None  # Will be extracted from email if needed
                    )

                    # Extract comprehensive location and website data
                    location_result = None
                    try:
                        from app.apollo_location_extractor import extract_company_location_data, extract_person_location_data

                        # Try to extract company location data if we have company information
                        if company_hint or (apollo_result and apollo_result.get('company', {}).get('domain')):
                            company_domain = None
                            if apollo_result and apollo_result.get('company', {}).get('domain'):
                                company_domain = apollo_result['company']['domain']

                            location_result = await extract_company_location_data(
                                company_name=company_hint,
                                company_domain=company_domain,
                                include_geocoding=False  # Skip geocoding for speed
                            )
                            logger.info(f"Extracted location data: {len(location_result.get('locations', []))} locations found")

                        # If no company data, try person location extraction
                        elif request.sender_email or request.sender_name:
                            location_result = await extract_person_location_data(
                                email=request.sender_email,
                                name=request.sender_name,
                                include_company_locations=True
                            )
                            logger.info(f"Extracted person location data with {len(location_result.get('company_locations', []))} company locations")

                    except Exception as loc_e:
                        logger.warning(f"Location extraction failed: {str(loc_e)}")
                        location_result = None

                    if apollo_result and (apollo_result.get('person') or apollo_result.get('company')):
                        logger.info(f"Apollo deep enrichment successful - Completeness: {apollo_result.get('data_completeness', 0):.0f}%")

                        # Initialize enrichment metrics for monitoring
                        enrichment_metrics = {
                            'data_points_extracted': 0,
                            'linkedin_found': False,
                            'phone_numbers_found': 0,
                            'company_enriched': False,
                            'employees_found': 0,
                            'decision_makers_found': 0
                        }

                        # Map Apollo response fields to internal schema if no user corrections exist
                        if not request.user_corrections:
                            apollo_mapped = {}

                            # Extract person data if available
                            person_data = apollo_result.get('person', {})
                            if person_data:
                                # Core contact information
                                if person_data.get('client_name'):
                                    apollo_mapped['candidate_name'] = person_data['client_name']
                                    enrichment_metrics['data_points_extracted'] += 1

                                if person_data.get('job_title'):
                                    apollo_mapped['job_title'] = person_data['job_title']
                                    enrichment_metrics['data_points_extracted'] += 1

                                if person_data.get('location'):
                                    apollo_mapped['location'] = person_data['location']
                                    enrichment_metrics['data_points_extracted'] += 1

                                # Phone numbers - prioritize mobile, then work, then any
                                phone_added = False
                                if person_data.get('mobile_phone'):
                                    apollo_mapped['mobile_phone'] = person_data['mobile_phone']
                                    phone_added = True
                                    enrichment_metrics['phone_numbers_found'] += 1
                                    enrichment_metrics['data_points_extracted'] += 1

                                if person_data.get('work_phone'):
                                    apollo_mapped['work_phone'] = person_data['work_phone']
                                    if not phone_added:
                                        apollo_mapped['phone_number'] = person_data['work_phone']
                                    enrichment_metrics['phone_numbers_found'] += 1
                                    enrichment_metrics['data_points_extracted'] += 1
                                elif person_data.get('phone') and not phone_added:
                                    apollo_mapped['phone_number'] = person_data['phone']
                                    enrichment_metrics['phone_numbers_found'] += 1
                                    enrichment_metrics['data_points_extracted'] += 1

                                # LinkedIn and social profiles
                                if person_data.get('linkedin_url'):
                                    apollo_mapped['linkedin_url'] = person_data['linkedin_url']
                                    enrichment_metrics['linkedin_found'] = True
                                    enrichment_metrics['data_points_extracted'] += 1

                                # Company information from person data
                                if person_data.get('firm_company'):
                                    apollo_mapped['company_name'] = person_data['firm_company']
                                    enrichment_metrics['data_points_extracted'] += 1

                                if person_data.get('company_website'):
                                    apollo_mapped['company_website'] = person_data['company_website']
                                    enrichment_metrics['data_points_extracted'] += 1

                            # Extract company data if available
                            company_data = apollo_result.get('company', {})
                            if company_data:
                                enrichment_metrics['company_enriched'] = True

                                # Override with more detailed company info if available
                                if company_data.get('company_name') and not apollo_mapped.get('company_name'):
                                    apollo_mapped['company_name'] = company_data['company_name']
                                    enrichment_metrics['data_points_extracted'] += 1

                                if company_data.get('website') and not apollo_mapped.get('company_website'):
                                    apollo_mapped['company_website'] = company_data['website']
                                    enrichment_metrics['data_points_extracted'] += 1

                                # Add comprehensive location data if extracted
                                if location_result:
                                    # Primary company location with full address details
                                    if location_result.get('locations'):
                                        primary_location = None
                                        all_locations = []

                                        for loc in location_result['locations']:
                                            if loc.get('is_primary'):
                                                primary_location = loc
                                            location_str = loc.get('formatted_address', loc.get('full_address', ''))
                                            if location_str:
                                                location_type = loc.get('location_type', 'Office').capitalize()
                                                all_locations.append(f"{location_type}: {location_str}")

                                        # Update primary location with full address
                                        if primary_location:
                                            # Full address with all components
                                            if primary_location.get('full_address'):
                                                apollo_mapped['location'] = primary_location['full_address']
                                                enrichment_metrics['data_points_extracted'] += 1

                                            # Store individual address components
                                            if primary_location.get('street_address'):
                                                apollo_mapped['company_street'] = primary_location['street_address']
                                            if primary_location.get('city'):
                                                apollo_mapped['company_city'] = primary_location['city']
                                            if primary_location.get('state'):
                                                apollo_mapped['company_state'] = primary_location['state']
                                            if primary_location.get('postal_code'):
                                                apollo_mapped['company_postal_code'] = primary_location['postal_code']
                                            if primary_location.get('country'):
                                                apollo_mapped['company_country'] = primary_location['country']

                                            # Add timezone if available
                                            if primary_location.get('timezone'):
                                                apollo_mapped['timezone'] = primary_location['timezone']
                                                enrichment_metrics['data_points_extracted'] += 1

                                        # Store all locations if multiple offices found
                                        if len(all_locations) > 1:
                                            apollo_mapped['all_office_locations'] = '\n'.join(all_locations)
                                            enrichment_metrics['data_points_extracted'] += 1

                                    # Add comprehensive website data
                                    if location_result.get('websites'):
                                        websites_data = location_result['websites']

                                        # Update company website if more comprehensive
                                        if websites_data.get('primary_website') and not apollo_mapped.get('company_website'):
                                            apollo_mapped['company_website'] = websites_data['primary_website']
                                            enrichment_metrics['data_points_extracted'] += 1

                                        # Blog URL
                                        if websites_data.get('blog_url'):
                                            apollo_mapped['blog_url'] = websites_data['blog_url']
                                            enrichment_metrics['data_points_extracted'] += 1

                                        # Careers page
                                        if websites_data.get('careers_page'):
                                            apollo_mapped['careers_page'] = websites_data['careers_page']
                                            enrichment_metrics['data_points_extracted'] += 1

                                        # Social profiles
                                        social_profiles = websites_data.get('social_profiles', {})
                                        if social_profiles:
                                            if social_profiles.get('linkedin') and not apollo_mapped.get('company_linkedin'):
                                                apollo_mapped['company_linkedin'] = social_profiles['linkedin']
                                            if social_profiles.get('twitter'):
                                                apollo_mapped['company_twitter'] = social_profiles['twitter']
                                            if social_profiles.get('facebook'):
                                                apollo_mapped['company_facebook'] = social_profiles['facebook']

                                # Company phone if no personal phone found
                                if company_data.get('phone') and not apollo_mapped.get('phone_number'):
                                    apollo_mapped['company_phone'] = company_data['phone']
                                    enrichment_metrics['data_points_extracted'] += 1

                                # Company details for context
                                company_details = []
                                if company_data.get('employee_count'):
                                    company_details.append(f"Employees: {company_data['employee_count']}")
                                if company_data.get('industry'):
                                    company_details.append(f"Industry: {company_data['industry']}")
                                if company_data.get('founded_year'):
                                    company_details.append(f"Founded: {company_data['founded_year']}")

                                # Key employees and decision makers
                                if company_data.get('key_employees'):
                                    enrichment_metrics['employees_found'] = len(company_data['key_employees'])

                                    # Format key employees for notes
                                    employee_info = []
                                    for emp in company_data['key_employees'][:5]:  # Top 5
                                        emp_str = f" {emp.get('name', 'Unknown')} - {emp.get('title', 'No title')}"
                                        if emp.get('email'):
                                            emp_str += f" ({emp['email']})"
                                        if emp.get('linkedin'):
                                            emp_str += f" [LinkedIn]"
                                        employee_info.append(emp_str)

                                    if employee_info:
                                        apollo_mapped['key_employees'] = '\n'.join(employee_info)
                                        enrichment_metrics['data_points_extracted'] += 1

                                if company_data.get('decision_makers'):
                                    enrichment_metrics['decision_makers_found'] = len(company_data['decision_makers'])

                                if company_data.get('recruiters'):
                                    # Store recruiter contacts for future reference
                                    recruiter_info = []
                                    for rec in company_data['recruiters'][:3]:  # Top 3
                                        rec_str = f" {rec.get('name', 'Unknown')} - {rec.get('title', 'Recruiter')}"
                                        if rec.get('email'):
                                            rec_str += f" ({rec['email']})"
                                        if rec.get('phone'):
                                            rec_str += f" Ph: {rec['phone']}"
                                        recruiter_info.append(rec_str)

                                    if recruiter_info:
                                        apollo_mapped['recruiters'] = '\n'.join(recruiter_info)
                                        enrichment_metrics['data_points_extracted'] += 1

                            # Handle alternative matches for validation
                            if person_data and person_data.get('alternative_matches'):
                                alt_matches_str = []
                                for alt in person_data['alternative_matches'][:2]:  # Top 2 alternatives
                                    alt_str = f" {alt.get('name', 'Unknown')} at {alt.get('company', 'Unknown Company')}"
                                    if alt.get('email'):
                                        alt_str += f" ({alt['email']})"
                                    alt_matches_str.append(alt_str)

                                if alt_matches_str:
                                    apollo_mapped['alternative_contacts'] = '\n'.join(alt_matches_str)

                            # Build comprehensive notes with all enriched data
                            if apollo_mapped:
                                notes_sections = []

                                # Preserve existing notes
                                if hasattr(extracted_data, 'notes') and extracted_data.notes:
                                    notes_sections.append(extracted_data.notes)
                                elif hasattr(extracted_data, 'model_dump'):
                                    extracted_dict = extracted_data.model_dump()
                                    if extracted_dict.get('notes'):
                                        notes_sections.append(extracted_dict['notes'])

                                # Add Apollo enrichment section
                                notes_sections.append("\n=== APOLLO.IO ENRICHMENT ===")

                                if apollo_mapped.get('linkedin_url'):
                                    notes_sections.append(f"LinkedIn: {apollo_mapped['linkedin_url']}")

                                if apollo_mapped.get('mobile_phone') or apollo_mapped.get('work_phone'):
                                    phone_section = "Phone Numbers:"
                                    if apollo_mapped.get('mobile_phone'):
                                        phone_section += f"\n   Mobile: {apollo_mapped['mobile_phone']}"
                                    if apollo_mapped.get('work_phone'):
                                        phone_section += f"\n   Work: {apollo_mapped['work_phone']}"
                                    notes_sections.append(phone_section)

                                if apollo_mapped.get('key_employees'):
                                    notes_sections.append(f"\nKey Employees:\n{apollo_mapped['key_employees']}")

                                if apollo_mapped.get('recruiters'):
                                    notes_sections.append(f"\nRecruiters:\n{apollo_mapped['recruiters']}")

                                if apollo_mapped.get('alternative_contacts'):
                                    notes_sections.append(f"\nAlternative Matches:\n{apollo_mapped['alternative_contacts']}")

                                # Add location and website information
                                if apollo_mapped.get('all_office_locations'):
                                    notes_sections.append(f"\nOffice Locations:\n{apollo_mapped['all_office_locations']}")
                                elif apollo_mapped.get('location'):
                                    notes_sections.append(f"\nLocation: {apollo_mapped['location']}")

                                if apollo_mapped.get('timezone'):
                                    notes_sections.append(f"Timezone: {apollo_mapped['timezone']}")

                                # Add website information
                                website_info = []
                                if apollo_mapped.get('blog_url'):
                                    website_info.append(f"Blog: {apollo_mapped['blog_url']}")
                                if apollo_mapped.get('careers_page'):
                                    website_info.append(f"Careers: {apollo_mapped['careers_page']}")
                                if apollo_mapped.get('company_twitter'):
                                    website_info.append(f"Twitter: {apollo_mapped['company_twitter']}")
                                if apollo_mapped.get('company_facebook'):
                                    website_info.append(f"Facebook: {apollo_mapped['company_facebook']}")

                                if website_info:
                                    notes_sections.append("\nWeb Presence:")
                                    notes_sections.extend(website_info)

                                # Add data completeness and metrics
                                notes_sections.append(f"\nEnrichment Completeness: {apollo_result.get('data_completeness', 0):.0f}%")
                                notes_sections.append(f"Data Points Extracted: {enrichment_metrics['data_points_extracted']}")

                                apollo_mapped['notes'] = '\n'.join(notes_sections)

                                # Store Apollo enrichment as user corrections for consistent processing
                                request.user_corrections = apollo_mapped
                                logger.info(f"Applied Apollo deep enrichment: {list(apollo_mapped.keys())}")

                                # Log enrichment metrics for monitoring
                                logger.info(f"Apollo Enrichment Metrics: {enrichment_metrics}")

                                # Update sender name and email from enriched data if available
                                if person_data.get('client_name') and not request.sender_name:
                                    request.sender_name = person_data['client_name']
                                if person_data.get('email') and person_data['email'] != request.sender_email:
                                    # Keep original sender_email but log the enriched one
                                    logger.info(f"Apollo provided alternative email: {person_data['email']}")
                            else:
                                logger.info("Apollo data received but no mappable fields found")
                        else:
                            logger.info("User corrections already exist, skipping Apollo mapping")
                    else:
                        # Fallback to simple enrichment if deep enrichment fails
                        logger.info(f"Apollo deep enrichment failed, trying simple enrichment for {request.sender_email}")
                        simple_apollo_data = await enrich_contact_with_apollo(request.sender_email)

                        if simple_apollo_data and not request.user_corrections:
                            apollo_mapped = {}

                            # Map basic fields
                            if simple_apollo_data.get('client_name'):
                                apollo_mapped['candidate_name'] = simple_apollo_data['client_name']
                            if simple_apollo_data.get('firm_company'):
                                apollo_mapped['company_name'] = simple_apollo_data['firm_company']
                            if simple_apollo_data.get('job_title'):
                                apollo_mapped['job_title'] = simple_apollo_data['job_title']
                            if simple_apollo_data.get('phone'):
                                apollo_mapped['phone_number'] = simple_apollo_data['phone']
                            if simple_apollo_data.get('website'):
                                apollo_mapped['company_website'] = simple_apollo_data['website']
                            if simple_apollo_data.get('location'):
                                apollo_mapped['location'] = simple_apollo_data['location']

                            if apollo_mapped:
                                request.user_corrections = apollo_mapped
                                logger.info(f"Applied Apollo simple enrichment: {list(apollo_mapped.keys())}")
                        else:
                            logger.info(f"No Apollo enrichment data found for {request.sender_email}")
                except Exception as e:
                    logger.warning(f"Apollo enrichment failed for {request.sender_email}: {str(e)}")
                    # Continue processing without Apollo enrichment

                # Apply historical corrections to the extracted data
                if hasattr(req.app.state, 'correction_service') and req.app.state.correction_service:
                    try:
                        # Convert ExtractedData to dict if needed
                        extracted_dict = extracted_data.model_dump() if hasattr(extracted_data, 'model_dump') else extracted_data.__dict__

                        # Apply learned corrections
                        corrected_data, corrections_applied = await req.app.state.correction_service.apply_historical_corrections(
                            extracted_dict,
                            sender_domain,
                            request.body
                        )

                        if corrections_applied:
                            logger.info(f"Applied {len(corrections_applied)} historical corrections: {list(corrections_applied.keys())}")
                            # Update the extracted_data with corrections
                            from app.models import ExtractedData
                            extracted_data = ExtractedData(**corrected_data)
                    except Exception as e:
                        logger.warning(f"Could not apply historical corrections: {e}")

                # PHASE 2: Track pattern matching metrics in learning analytics
                if hasattr(req.app.state, 'learning_analytics') and req.app.state.learning_analytics:
                    try:
                        import time
                        processing_time_ms = 2000  # Approximate LangGraph processing time
                        
                        # Track extraction with pattern metrics
                        extraction_metric = await req.app.state.learning_analytics.track_extraction(
                            email_domain=sender_domain,
                            extraction_result=extracted_data.model_dump() if hasattr(extracted_data, 'model_dump') else extracted_data.__dict__,
                            processing_time_ms=processing_time_ms,
                            used_template=company_template is not None,
                            pattern_matches=len(similar_patterns)
                        )
                        
                        logger.info(f"Tracked extraction metrics: {len(similar_patterns)} pattern matches, template: {company_template is not None}")
                        
                    except Exception as e:
                        logger.warning(f"Failed to track pattern matching metrics: {e}")
                
                # PHASE 3: Index this extraction for future pattern learning
                if search_manager and ai_search_available:
                    try:
                        indexing_success = await search_manager.index_email_pattern(
                            email_domain=sender_domain,
                            email_content=request.body,
                            extraction_result=extracted_data.model_dump() if hasattr(extracted_data, 'model_dump') else extracted_data.__dict__,
                            confidence_score=0.8  # Default confidence for successful extractions
                        )
                        
                        if indexing_success:
                            logger.info("Successfully indexed email pattern for future learning")
                        else:
                            logger.warning("Failed to index email pattern - pattern learning may be degraded")
                            
                        # Update company template
                        template_updated = await search_manager.update_company_template(
                            company_domain=sender_domain,
                            extraction_data=extracted_data.model_dump() if hasattr(extracted_data, 'model_dump') else extracted_data.__dict__
                        )
                        
                        if template_updated:
                            logger.info(f"Updated company template for {sender_domain}")
                        else:
                            logger.warning(f"Failed to update company template for {sender_domain}")
                            
                    except Exception as e:
                        logger.warning(f"Pattern indexing failed, learning capability reduced: {e}")
                        # Don't fail the entire processing - this is just for learning
                elif not ai_search_available:
                    logger.info("AI Search not available - skipping pattern learning (extraction still successful)")
                
            except Exception as e:
                logger.warning(f"LangGraph processing failed: {e}, using fallback extractor")
                from app.langgraph_manager import SimplifiedEmailExtractor
                extracted_data = SimplifiedEmailExtractor.extract(request.body, request.sender_email)

                # COMPREHENSIVE ENRICHMENT: Apply Apollo + Firecrawl v2 for fallback extraction
                try:
                    from app.firecrawl_enricher import comprehensive_enrichment

                    # Extract company name from fallback data
                    company_hint = None
                    if hasattr(extracted_data, 'company_name') and extracted_data.company_name:
                        company_hint = extracted_data.company_name
                    elif hasattr(extracted_data, '__dict__'):
                        company_hint = extracted_data.__dict__.get('company_name')

                    # Try comprehensive enrichment with all services
                    apollo_result = await comprehensive_enrichment(
                        email=request.sender_email,
                        name=request.sender_name,
                        company=company_hint,
                        domain=None  # Will be extracted from email
                    )

                    if apollo_result and (apollo_result.get('person') or apollo_result.get('company')):
                        logger.info(f"Apollo deep enrichment successful (fallback) - Completeness: {apollo_result.get('data_completeness', 0):.0f}%")

                        # Map Apollo response fields to internal schema if no user corrections exist
                        if not request.user_corrections:
                            apollo_mapped = {}
                            person_data = apollo_result.get('person', {})
                            company_data = apollo_result.get('company', {})

                            # Extract essential fields with deep data
                            if person_data:
                                if person_data.get('client_name'):
                                    apollo_mapped['candidate_name'] = person_data['client_name']
                                if person_data.get('job_title'):
                                    apollo_mapped['job_title'] = person_data['job_title']
                                if person_data.get('location'):
                                    apollo_mapped['location'] = person_data['location']
                                if person_data.get('linkedin_url'):
                                    apollo_mapped['linkedin_url'] = person_data['linkedin_url']
                                if person_data.get('mobile_phone'):
                                    apollo_mapped['mobile_phone'] = person_data['mobile_phone']
                                elif person_data.get('phone'):
                                    apollo_mapped['phone_number'] = person_data['phone']

                            # Add company enrichment
                            if company_data:
                                if company_data.get('company_name'):
                                    apollo_mapped['company_name'] = company_data['company_name']
                                if company_data.get('website'):
                                    apollo_mapped['company_website'] = company_data['website']

                            # Store enriched data in user_corrections if we have mapped data
                            if apollo_mapped:
                                # Preserve any existing AI extraction notes
                                if hasattr(extracted_data, 'notes') and extracted_data.notes:
                                    apollo_mapped['notes'] = extracted_data.notes + "\n\n=== Apollo Enrichment (Fallback) ==="
                                else:
                                    apollo_mapped['notes'] = "=== Apollo Enrichment (Fallback) ==="

                                # Add enrichment completeness
                                apollo_mapped['notes'] += f"\nCompleteness: {apollo_result.get('data_completeness', 0):.0f}%"

                                # Store Apollo enrichment as user corrections for consistent processing
                                request.user_corrections = apollo_mapped
                                logger.info(f"Applied Apollo deep enrichment (fallback): {list(apollo_mapped.keys())}")

                                # Update sender name from enriched data if available
                                if person_data.get('client_name') and not request.sender_name:
                                    request.sender_name = person_data['client_name']
                            else:
                                logger.info("Apollo data received but no mappable fields found (fallback)")
                        else:
                            logger.info("User corrections already exist, skipping Apollo mapping (fallback)")
                    else:
                        # Fall back to simple enrichment
                        logger.info(f"Trying simple Apollo enrichment for {request.sender_email} (fallback case)")
                        apollo_data = await enrich_contact_with_apollo(request.sender_email)

                        if apollo_data and not request.user_corrections:
                            apollo_mapped = {}
                            if apollo_data.get('client_name'):
                                apollo_mapped['candidate_name'] = apollo_data['client_name']
                            if apollo_data.get('firm_company'):
                                apollo_mapped['company_name'] = apollo_data['firm_company']
                            if apollo_data.get('job_title'):
                                apollo_mapped['job_title'] = apollo_data['job_title']
                            if apollo_data.get('phone'):
                                apollo_mapped['phone_number'] = apollo_data['phone']
                            if apollo_data.get('website'):
                                apollo_mapped['company_website'] = apollo_data['website']
                            if apollo_data.get('location'):
                                apollo_mapped['location'] = apollo_data['location']

                            if apollo_mapped:
                                request.user_corrections = apollo_mapped
                                logger.info(f"Applied Apollo simple enrichment (fallback): {list(apollo_mapped.keys())}")
                        else:
                            logger.info(f"No Apollo enrichment data found for {request.sender_email} (fallback case)")
                except Exception as apollo_e:
                    logger.warning(f"Apollo enrichment failed for {request.sender_email} (fallback case): {str(apollo_e)}")
                    # Continue processing without Apollo enrichment

        # Apply business rules
        processed_data = business_rules.process_data(
            extracted_data.model_dump() if hasattr(extracted_data, 'model_dump') else extracted_data,
            request.body,
            request.sender_email,
            request.subject  # Pass subject for client consultation detection
        )
        
        # Check if user input is required for missing fields
        if processed_data.get('requires_user_input'):
            missing_fields = processed_data.get('missing_fields', [])
            logger.info(f"Missing critical fields requiring user input: {missing_fields}")
            
            # Return response indicating user input is needed
            return ZohoResponse(
                status="requires_input",
                message=f"Missing information required: {', '.join(missing_fields)}",
                extracted=extracted_data,
                missing_fields=missing_fields
            )
        
        # Convert back to ExtractedData model
        from app.models import ExtractedData
        enhanced_data = ExtractedData(**processed_data)
        
        # Parse deal information
        deal_id = str(uuid.uuid4())  # Generate new deal ID
        deal_name = request.subject or "Unknown Deal"
        account_name = enhanced_data.company_name or "Unknown Company"
        owner_email = os.environ.get('ZOHO_DEFAULT_OWNER_EMAIL', 'daniel.romitelli@emailthewell.com')
        
        # If dry_run, return preview without creating Zoho records
        if getattr(request, 'dry_run', False):
            return ZohoResponse(
                status="preview",
                message="Preview only - no Zoho records created",
                extracted=enhanced_data,
                correlation_id=correlation_id
            )

        # Prepare raw JSON for storage
        raw_json = {
            "message_id": message_id,
            "correlation_id": correlation_id,
            "sender_email": request.sender_email,
            "sender_name": request.sender_name,
            "subject": request.subject,
            "body": request.body[:10000],  # Truncate for storage
            "extracted_data": enhanced_data.model_dump() if hasattr(enhanced_data, 'model_dump') else enhanced_data.__dict__,
            "attachments": [{"filename": a.filename} for a in (request.attachments or [])]
        }
        
        # TRANSACTION: Begin database transaction with retry logic
        saved_to_db = False
        saved_to_zoho = False
        zoho_id = None
        zoho_result = {}
        
        if hasattr(req.app.state, 'postgres_client') and req.app.state.postgres_client:
            await req.app.state.postgres_client.init_pool()

            # Check for duplicates BEFORE starting transaction
            if hasattr(req.app.state, 'duplicate_checker') and req.app.state.duplicate_checker:
                duplicate_data = {
                    'candidate_name': enhanced_data.candidate_name,
                    'company_name': enhanced_data.company_name or account_name,
                    'email': enhanced_data.email,
                    'job_title': enhanced_data.job_title
                }

                existing_record = await req.app.state.duplicate_checker.check_database_duplicate(duplicate_data)

                # Also check Zoho CRM for duplicates
                zoho_duplicate = None
                if hasattr(req.app.state, 'zoho_integration') and req.app.state.zoho_integration:
                    zoho_duplicate = await req.app.state.duplicate_checker.check_zoho_duplicate_flexible(
                        req.app.state.zoho_integration,
                        duplicate_data
                    )
                    if zoho_duplicate:
                        logger.warning(f"Found duplicate in Zoho CRM: {zoho_duplicate}")
                        # Create existing_record format for consistency
                        existing_record = {
                            'zoho_contact_id': zoho_duplicate.get('contact_id'),
                            'zoho_account_id': zoho_duplicate.get('account_id'),
                            'time_since_creation': 0,  # Zoho duplicate is immediate block
                            'match_type': zoho_duplicate.get('match_type', 'zoho_match')
                        }

                if existing_record and (req.app.state.duplicate_checker.should_block_duplicate(existing_record) or zoho_duplicate):
                    # Duplicate found within time window - block creation
                    time_since = existing_record.get('time_since_creation', 0)
                    logger.warning(
                        f"Blocking duplicate: {enhanced_data.candidate_name} at {account_name} "
                        f"(created {time_since:.0f} seconds ago)"
                    )

                    # Return duplicate response
                    return ZohoResponse(
                        status="duplicate_blocked",
                        deal_id=existing_record.get('zoho_deal_id'),
                        account_id=existing_record.get('zoho_account_id'),
                        contact_id=existing_record.get('zoho_contact_id'),
                        deal_name=deal_name,
                        primary_email=enhanced_data.email or request.sender_email,
                        message=f"Duplicate record detected - {enhanced_data.candidate_name} at {account_name} was already processed {time_since:.0f} seconds ago",
                        extracted=enhanced_data,
                        saved_to_db=False,
                        saved_to_zoho=False,
                        correlation_id=correlation_id,
                        duplicate_info={
                            "is_duplicate": True,
                            "time_since_creation": time_since,
                            "existing_deal_id": existing_record.get('deal_id'),
                            "existing_zoho_deal_id": existing_record.get('zoho_deal_id')
                        }
                    )

            # Start transaction
            async with req.app.state.postgres_client.pool.acquire() as conn:
                async with conn.transaction():
                    try:
                        # Step 1: Upsert into deals table with candidate details
                        upsert_query = """
                        INSERT INTO deals (
                            id, deal_id, deal_name, owner_email, owner_name, stage,
                            contact_name, contact_email, account_name,
                            source, source_detail, created_at, metadata,
                            candidate_name, company_name, email, job_title
                        ) VALUES ($1::VARCHAR, $1::VARCHAR, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16)
                        ON CONFLICT (id) DO UPDATE SET
                            deal_id = EXCLUDED.deal_id,
                            deal_name = EXCLUDED.deal_name,
                            modified_at = CURRENT_TIMESTAMP,
                            metadata = EXCLUDED.metadata
                        RETURNING deal_id;
                        """

                        await conn.fetchrow(
                            upsert_query,
                            deal_id,
                            deal_name,
                            owner_email,
                            enhanced_data.referrer_name or "System",
                            "New",  # Initial stage
                            enhanced_data.candidate_name,
                            request.sender_email,
                            account_name,
                            enhanced_data.source or "Email Inbound",
                            enhanced_data.source_detail,
                            datetime.utcnow(),
                            json.dumps(raw_json),
                            enhanced_data.candidate_name,  # For duplicate detection
                            enhanced_data.company_name or account_name,  # For duplicate detection
                            enhanced_data.email,  # For duplicate detection
                            enhanced_data.job_title  # For duplicate detection
                        )

                        saved_to_db = True
                        logger.info(f"Saved deal to database: {deal_id}")

                        # Step 2: Call Zoho CRM API with retry logic
                        max_retries = 3
                        retry_delays = [1, 2, 4]  # Exponential backoff

                        for attempt in range(max_retries):
                            try:
                                zoho_result = await req.app.state.zoho_integration.create_or_update_records(
                                    enhanced_data,
                                    request.sender_email,
                                    attachment_urls,
                                    False  # is_duplicate
                                )
                                
                                saved_to_zoho = True
                                zoho_id = zoho_result.get("deal_id")

                                # Update deals table with Zoho IDs
                                update_query = """
                                UPDATE deals
                                SET zoho_deal_id = $2,
                                    zoho_contact_id = $3,
                                    zoho_account_id = $4
                                WHERE deal_id = $1
                                """
                                await conn.execute(
                                    update_query,
                                    deal_id,
                                    zoho_result.get("deal_id"),
                                    zoho_result.get("contact_id"),
                                    zoho_result.get("account_id")
                                )

                                # Check if this was a duplicate
                                if zoho_result.get('was_duplicate'):
                                    logger.info(f"Duplicate records found in Zoho for {request.sender_email}")

                                    # Prepare detailed duplicate message
                                    duplicate_details = []
                                    if zoho_result.get('existing_contact_id'):
                                        duplicate_details.append(f"Contact (ID: {zoho_result.get('existing_contact_id')})")
                                    if zoho_result.get('existing_account_id'):
                                        duplicate_details.append(f"Company (ID: {zoho_result.get('existing_account_id')})")
                                    if zoho_result.get('deal_id') and zoho_result.get('was_duplicate'):
                                        duplicate_details.append(f"Deal '{deal_name}'")

                                    duplicate_message = f"Record already exists in Zoho: {', '.join(duplicate_details) if duplicate_details else 'Duplicate found'}"

                                    # Return duplicate response with specific status
                                    return ZohoResponse(
                                        status="duplicate",
                                        deal_id=zoho_id,
                                        account_id=zoho_result.get("account_id"),
                                        contact_id=zoho_result.get("contact_id"),
                                        deal_name=zoho_result.get("deal_name", deal_name),
                                        primary_email=zoho_result.get("primary_email", request.sender_email),
                                        message=duplicate_message,
                                        extracted=enhanced_data,
                                        saved_to_db=saved_to_db,
                                        saved_to_zoho=True,
                                        correlation_id=correlation_id,
                                        duplicate_info={
                                            "is_duplicate": True,
                                            "duplicate_types": duplicate_details,
                                            "existing_contact": zoho_result.get('existing_contact_id'),
                                            "existing_account": zoho_result.get('existing_account_id'),
                                            "existing_deal": zoho_id if zoho_result.get('was_duplicate') else None
                                        }
                                    )

                                logger.info(f"Created Zoho records on attempt {attempt + 1}: {zoho_id}")
                                break
                                
                            except Exception as zoho_error:
                                error_str = str(zoho_error).lower()
                                
                                # Check if it's a retryable error
                                if attempt < max_retries - 1:
                                    if "rate limit" in error_str or "429" in error_str or "500" in error_str or "502" in error_str or "503" in error_str:
                                        await asyncio.sleep(retry_delays[attempt])
                                        logger.warning(f"Zoho API error on attempt {attempt + 1}, retrying: {zoho_error}")
                                        continue
                                    elif "token" in error_str or "unauthorized" in error_str:
                                        # Try to refresh token
                                        try:
                                            # Token refresh is handled internally by ZohoIntegration
                                            await asyncio.sleep(1)
                                            continue
                                        except:
                                            pass
                                
                                # Final attempt failed
                                raise zoho_error
                        
                        # Step 3: Log to intake_audit
                        audit_query = """
                        INSERT INTO intake_audit (
                            correlation_id, message_id, operation_type, deal_id,
                            request_payload, response_payload, outcome, 
                            processing_time_ms, created_at
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                        """
                        
                        response_payload = {
                            "deal_id": deal_id,
                            "zoho_id": zoho_id,
                            "account_id": zoho_result.get("account_id"),
                            "contact_id": zoho_result.get("contact_id"),
                            "deal_name": zoho_result.get("deal_name", deal_name),
                            "primary_email": zoho_result.get("primary_email", request.sender_email),
                            "extracted": enhanced_data.model_dump() if hasattr(enhanced_data, 'model_dump') else enhanced_data.__dict__
                        }
                        
                        await conn.execute(
                            audit_query,
                            uuid.UUID(correlation_id),
                            message_id,
                            "intake_email",
                            deal_id,
                            json.dumps(raw_json),
                            json.dumps(response_payload),
                            "success",
                            2000,  # Approximate processing time
                            datetime.utcnow()
                        )
                        
                        logger.info(f"Transaction completed successfully for correlation_id: {correlation_id}")
                        
                    except Exception as tx_error:
                        # Transaction will automatically rollback
                        logger.error(f"Transaction failed, rolling back: {tx_error}")
                        
                        # Log failure to intake_audit (outside transaction)
                        try:
                            async with req.app.state.postgres_client.pool.acquire() as audit_conn:
                                await audit_conn.execute(
                                    """
                                    INSERT INTO intake_audit (
                                        correlation_id, message_id, operation_type, deal_id,
                                        request_payload, outcome, error_message, created_at
                                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                                    """,
                                    uuid.UUID(correlation_id),
                                    message_id,
                                    "intake_email",
                                    deal_id if saved_to_db else None,
                                    json.dumps(raw_json),
                                    "db_fail" if not saved_to_db else "zoho_fail",
                                    str(tx_error)[:1000],
                                    datetime.utcnow()
                                )
                        except:
                            pass
                        
                        raise HTTPException(
                            status_code=500,
                            detail=f"Transaction failed: {str(tx_error)}. Correlation ID: {correlation_id}"
                        )
        
        # Return success response
        return ZohoResponse(
            status="success",
            deal_id=zoho_result.get("deal_id", deal_id),
            account_id=zoho_result.get("account_id"),
            contact_id=zoho_result.get("contact_id"),
            deal_name=zoho_result.get("deal_name", deal_name),
            primary_email=zoho_result.get("primary_email", request.sender_email),
            message="Email processed successfully",
            extracted=enhanced_data,
            saved_to_db=saved_to_db,
            saved_to_zoho=saved_to_zoho,
            correlation_id=correlation_id
        )
        
    except Exception as e:
        logger.error(f"Error processing email: {str(e)}")
        
        # Log failure to intake_audit for comprehensive tracking
        if hasattr(req.app.state, 'postgres_client') and req.app.state.postgres_client:
            try:
                await req.app.state.postgres_client.init_pool()
                
                # Get variables that may not exist if error occurred early
                message_id = locals().get('message_id')
                if not message_id:
                    # Generate fallback message_id if error occurred before idempotency check
                    timestamp = datetime.utcnow().strftime("%Y-%m-%d")
                    subject = getattr(request, 'subject', '') or 'unknown'
                    sender_email = getattr(request, 'sender_email', '') or 'unknown'
                    idempotency_string = f"{subject}:{sender_email}:{timestamp}"
                    message_id = hashlib.sha256(idempotency_string.encode()).hexdigest()
                
                correlation_id = locals().get('correlation_id', str(uuid.uuid4()))
                
                # Prepare request payload for audit
                request_payload = {
                    "sender_email": getattr(request, 'sender_email', None),
                    "sender_name": getattr(request, 'sender_name', None),
                    "subject": getattr(request, 'subject', None),
                    "body": getattr(request, 'body', '')[:1000] if getattr(request, 'body', None) else None,
                }
                
                # Log audit entry for failure
                async with req.app.state.postgres_client.pool.acquire() as conn:
                    await conn.execute(
                        """
                        INSERT INTO intake_audit (
                            correlation_id, message_id, operation_type,
                            request_payload, outcome, error_message, created_at
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7)
                        """,
                        uuid.UUID(correlation_id),
                        message_id,
                        "intake_email",
                        json.dumps(request_payload),
                        "failure",
                        str(e)[:1000],
                        datetime.utcnow()
                    )
                
                logger.info(f"Logged failure to intake_audit with correlation_id: {correlation_id}")
                
            except Exception as storage_error:
                logger.warning(f"Failed to store error audit log: {storage_error}")
        
        # Return error with correlation_id if available
        correlation_id = locals().get('correlation_id')
        if correlation_id:
            raise HTTPException(
                status_code=500, 
                detail=f"Transaction failed: {str(e)}. Correlation ID: {correlation_id}"
            )
        else:
            raise HTTPException(status_code=500, detail=str(e))

@app.get("/intake/email/status/{extraction_id}", dependencies=[Depends(verify_api_key)])
async def get_extraction_status(extraction_id: str, req: Request):
    """Check the status of an email extraction/enrichment."""
    try:
        if not hasattr(req.app.state, 'cache_manager'):
            raise HTTPException(status_code=503, detail="Cache not available")
        
        cache_key = f"extraction:{extraction_id}"
        cached_data = await req.app.state.cache_manager.get_cache(cache_key)
        
        if not cached_data:
            raise HTTPException(status_code=404, detail="Extraction not found")
        
        return cached_data
        
    except Exception as e:
        logger.error(f"Error getting extraction status: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/test/kevin-sullivan", dependencies=[Depends(verify_api_key)])
async def test_kevin_sullivan(req: Request):
    """Test endpoint with Kevin Sullivan sample email"""
    test_email = EmailRequest(
        sender_email="referrer@wellpartners.com",
        sender_name="John Referrer",
        subject="Introduction - Kevin Sullivan for Senior Financial Advisor Role",
        body="""
        Hi Team,
        
        I wanted to introduce you to Kevin Sullivan who would be perfect for the 
        Senior Financial Advisor position in the Fort Wayne area.
        
        Kevin has over 10 years of experience in wealth management and has consistently 
        exceeded his targets. He's currently looking for new opportunities and would be 
        a great addition to your team.
        
        Please let me know if you'd like to schedule a call to discuss further.
        
        Best regards,
        John Referrer
        Well Partners Recruiting
        """,
        received_date=datetime.utcnow().isoformat(),
        attachments=[]
    )
    
    return await process_email(test_email, req)

# Batch processing endpoints
@app.post("/batch/submit", dependencies=[Depends(verify_api_key)])
async def submit_batch(emails: List[EmailRequest], priority: int = 0):
    """
    Submit a batch of emails for processing using GPT-5-mini
    
    Args:
        emails: List of email payloads to process
        priority: Processing priority (0-9, higher = more priority)
    
    Returns:
        Batch submission confirmation with batch ID
    """
    try:
        # Validate batch size
        if len(emails) > 50:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Batch size exceeds maximum of 50 emails"
            )
        
        if len(emails) == 0:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Batch must contain at least one email"
            )
        
        # Validate priority
        if priority < 0 or priority > 9:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Priority must be between 0 and 9"
            )
        
        logger.info(f"Received batch submission with {len(emails)} emails")
        
        # Initialize Service Bus manager if not already done
        if not hasattr(app.state, 'service_bus_manager'):
            from app.service_bus_manager import ServiceBusManager
            app.state.service_bus_manager = ServiceBusManager()
            await app.state.service_bus_manager.connect()
        
        # Convert EmailRequest objects to dictionaries
        email_dicts = []
        for email in emails:
            email_dict = {
                "sender_email": email.sender_email,
                "sender_name": email.sender_name,
                "subject": email.subject,
                "body": email.body,
                "attachments": [
                    {
                        "filename": att.filename,
                        "content": att.content_base64,
                        "content_type": att.content_type
                    }
                    for att in email.attachments
                ] if email.attachments else []
            }
            email_dicts.append(email_dict)
        
        # Submit to Service Bus
        batch_id = await app.state.service_bus_manager.send_batch(email_dicts, priority)
        
        return {
            "status": "success",
            "batch_id": batch_id,
            "email_count": len(emails),
            "priority": priority,
            "message": f"Batch submitted for processing. Use /batch/status/{batch_id} to check progress."
        }
        
    except Exception as e:
        logger.error(f"Error submitting batch: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/batch/process", dependencies=[Depends(verify_api_key)])
async def process_batch_direct(emails: List[EmailRequest]):
    """
    Process a batch of emails directly without queuing
    Useful for immediate processing of small batches
    
    Args:
        emails: List of email payloads to process
    
    Returns:
        Processing results for the batch
    """
    try:
        logger.info(f"Processing batch directly with {len(emails)} emails")
        
        # Initialize enhanced batch processor with learning integration
        from app.batch_processor import create_enhanced_batch_processor
        
        processor = create_enhanced_batch_processor(
            zoho_client=app.state.zoho_integration if hasattr(app.state, 'zoho_integration') else None,
            postgres_client=app.state.postgres_client if hasattr(app.state, 'postgres_client') else None,
            enable_learning=True
        )
        
        # Convert EmailRequest objects to dictionaries
        email_dicts = []
        for email in emails:
            email_dict = {
                "sender_email": email.sender_email,
                "sender_name": email.sender_name,
                "subject": email.subject,
                "body": email.body,
                "attachments": [
                    {
                        "filename": att.filename,
                        "content": att.content_base64,
                        "content_type": att.content_type
                    }
                    for att in email.attachments
                ] if email.attachments else []
            }
            email_dicts.append(email_dict)
        
        # Process emails with optimal batching
        results = await processor.process_emails_optimized(email_dicts, auto_batch=True)
        
        # Get aggregate statistics
        stats = processor.get_processing_stats(results)
        
        return {
            "status": "success",
            "statistics": stats,
            "message": f"Processed {stats['processed_emails']} of {stats['total_emails']} emails"
        }
        
    except Exception as e:
        logger.error(f"Error processing batch: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/batch/status/{batch_id}", dependencies=[Depends(verify_api_key)])
async def get_batch_status(batch_id: str):
    """
    Get status of a batch processing job
    
    Args:
        batch_id: Batch identifier from submission
    
    Returns:
        Current batch processing status with comprehensive details
    """
    try:
        # Validate batch_id format (prevent injection)
        import uuid
        try:
            # Batch IDs should be UUIDs
            uuid.UUID(batch_id)
        except ValueError:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid batch ID format"
            )
        
        # First check database for batch status
        if hasattr(app.state, 'postgres_client') and app.state.postgres_client:
            try:
                batch_status = await app.state.postgres_client.get_batch_status(batch_id)
                if batch_status:
                    # Calculate completion percentage
                    total = batch_status.get('total_emails', 1)
                    processed = batch_status.get('processed_emails', 0)
                    failed = batch_status.get('failed_emails', 0)
                    completion_percentage = ((processed + failed) / total) * 100 if total > 0 else 0
                    
                    # Calculate estimated completion time if still processing
                    estimated_completion = None
                    if batch_status.get('status') == 'processing' and batch_status.get('started_at'):
                        from datetime import datetime
                        started_at = batch_status['started_at']
                        if isinstance(started_at, str):
                            started_at = datetime.fromisoformat(started_at.replace('Z', '+00:00'))
                        
                        elapsed_time = (datetime.utcnow().replace(tzinfo=started_at.tzinfo) - started_at).total_seconds()
                        if processed > 0:
                            avg_time_per_email = elapsed_time / processed
                            remaining_emails = total - processed - failed
                            estimated_completion = remaining_emails * avg_time_per_email
                    
                    return {
                        "status": batch_status['status'],
                        "batch_id": batch_id,
                        "total_emails": batch_status['total_emails'],
                        "processed_emails": batch_status['processed_emails'],
                        "failed_emails": batch_status['failed_emails'],
                        "completion_percentage": round(completion_percentage, 1),
                        "created_at": batch_status['created_at'].isoformat() if batch_status.get('created_at') else None,
                        "started_at": batch_status['started_at'].isoformat() if batch_status.get('started_at') else None,
                        "completed_at": batch_status['completed_at'].isoformat() if batch_status.get('completed_at') else None,
                        "processing_time_seconds": float(batch_status['processing_time_seconds']) if batch_status.get('processing_time_seconds') else None,
                        "tokens_used": batch_status.get('tokens_used'),
                        "estimated_cost": float(batch_status['estimated_cost']) if batch_status.get('estimated_cost') else None,
                        "estimated_completion_seconds": estimated_completion,
                        "error_message": batch_status.get('error_message'),
                        "metadata": batch_status.get('metadata', {})
                    }
            except Exception as db_error:
                logger.warning(f"Failed to get batch status from database: {db_error}")
        
        # Fallback to Service Bus queue check
        if not hasattr(app.state, 'service_bus_manager'):
            from app.service_bus_manager import ServiceBusManager
            app.state.service_bus_manager = ServiceBusManager()
            await app.state.service_bus_manager.connect()
        
        # Peek at messages to find batch status
        messages = await app.state.service_bus_manager.peek_messages(max_messages=50)
        
        # Find the batch
        for msg in messages:
            if msg.get("batch_id") == batch_id:
                return {
                    "status": "pending",
                    "batch_id": batch_id,
                    "total_emails": msg.get("email_count"),
                    "processed_emails": 0,
                    "failed_emails": 0,
                    "completion_percentage": 0,
                    "created_at": msg.get("created_at"),
                    "priority": msg.get("priority"),
                    "position_in_queue": messages.index(msg) + 1,
                    "message": "Batch is queued for processing"
                }
        
        # Batch not found anywhere
        return {
            "status": "not_found",
            "batch_id": batch_id,
            "message": "Batch not found in queue or database. It may have been processed and archived."
        }
        
    except Exception as e:
        logger.error(f"Error getting batch status: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/batch/queue/status", dependencies=[Depends(verify_api_key)])
async def get_queue_status():
    """Get Service Bus queue status and metrics"""
    try:
        # Initialize Service Bus manager if needed
        if not hasattr(app.state, 'service_bus_manager'):
            from app.service_bus_manager import ServiceBusManager
            app.state.service_bus_manager = ServiceBusManager()
            await app.state.service_bus_manager.connect()
        
        status = await app.state.service_bus_manager.get_queue_status()
        
        return status
        
    except Exception as e:
        logger.error(f"Error getting queue status: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/batch/queue/process", dependencies=[Depends(verify_api_key)])
async def process_queue_batches(max_batches: int = 1):
    """
    Process batches from the Service Bus queue
    
    Args:
        max_batches: Maximum number of batches to process
    
    Returns:
        Processing results
    """
    try:
        logger.info(f"Processing up to {max_batches} batches from queue")
        
        # Initialize enhanced batch processor with learning integration
        from app.batch_processor import create_enhanced_batch_processor
        
        processor = create_enhanced_batch_processor(
            service_bus_manager=app.state.service_bus_manager if hasattr(app.state, 'service_bus_manager') else None,
            zoho_client=app.state.zoho_integration if hasattr(app.state, 'zoho_integration') else None,
            postgres_client=app.state.postgres_client if hasattr(app.state, 'postgres_client') else None,
            enable_learning=True
        )
        
        # Process from queue
        results = await processor.process_from_queue(max_batches=max_batches)
        
        if not results:
            return {
                "status": "success",
                "message": "No batches available in queue",
                "processed_batches": 0
            }
        
        # Get aggregate statistics
        stats = processor.get_processing_stats(results)
        
        return {
            "status": "success",
            "processed_batches": len(results),
            "statistics": stats
        }
        
    except Exception as e:
        logger.error(f"Error processing queue: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/batch/status", dependencies=[Depends(verify_api_key)])
async def list_batch_statuses(
    limit: int = 50, 
    status_filter: str = None,
    include_metadata: bool = False
):
    """
    List batch processing statuses with filtering
    
    Args:
        limit: Maximum number of batch statuses to return (default: 50)
        status_filter: Filter by status (pending, processing, completed, failed, partial)
        include_metadata: Include detailed metadata in response
    
    Returns:
        List of batch processing statuses
    """
    try:
        if limit < 1 or limit > 100:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Limit must be between 1 and 100"
            )
        
        valid_statuses = ['pending', 'processing', 'completed', 'failed', 'partial']
        if status_filter and status_filter not in valid_statuses:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Invalid status filter. Must be one of: {', '.join(valid_statuses)}"
            )
        
        # Get batch statuses from database
        if not hasattr(app.state, 'postgres_client') or not app.state.postgres_client:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Database not available"
            )
        
        batch_statuses = await app.state.postgres_client.list_batch_statuses(
            limit=limit, 
            status_filter=status_filter
        )
        
        # Enhance batch status information
        enhanced_statuses = []
        for batch_status in batch_statuses:
            # Calculate completion percentage
            total = batch_status.get('total_emails', 1)
            processed = batch_status.get('processed_emails', 0)
            failed = batch_status.get('failed_emails', 0)
            completion_percentage = ((processed + failed) / total) * 100 if total > 0 else 0
            
            enhanced_status = {
                "batch_id": batch_status['batch_id'],
                "status": batch_status['status'],
                "total_emails": batch_status['total_emails'],
                "processed_emails": batch_status['processed_emails'],
                "failed_emails": batch_status['failed_emails'],
                "completion_percentage": round(completion_percentage, 1),
                "created_at": batch_status['created_at'].isoformat() if batch_status.get('created_at') else None,
                "started_at": batch_status['started_at'].isoformat() if batch_status.get('started_at') else None,
                "completed_at": batch_status['completed_at'].isoformat() if batch_status.get('completed_at') else None,
                "processing_time_seconds": float(batch_status['processing_time_seconds']) if batch_status.get('processing_time_seconds') else None,
                "tokens_used": batch_status.get('tokens_used'),
                "estimated_cost": float(batch_status['estimated_cost']) if batch_status.get('estimated_cost') else None,
                "error_message": batch_status.get('error_message')
            }
            
            if include_metadata:
                enhanced_status["metadata"] = batch_status.get('metadata', {})
            
            enhanced_statuses.append(enhanced_status)
        
        # Get queue statistics for additional context
        queue_stats = {}
        try:
            if hasattr(app.state, 'service_bus_manager') or not hasattr(app.state, 'service_bus_manager'):
                if not hasattr(app.state, 'service_bus_manager'):
                    from app.service_bus_manager import ServiceBusManager
                    app.state.service_bus_manager = ServiceBusManager()
                    await app.state.service_bus_manager.connect()
                
                queue_status = await app.state.service_bus_manager.get_queue_status()
                queue_stats = {
                    "pending_in_queue": queue_status.get("active_message_count", 0),
                    "deadletter_count": queue_status.get("deadletter_message_count", 0)
                }
        except Exception as queue_error:
            logger.warning(f"Failed to get queue statistics: {queue_error}")
            queue_stats = {"error": "Unable to retrieve queue statistics"}
        
        return {
            "status": "success",
            "batches": enhanced_statuses,
            "total_returned": len(enhanced_statuses),
            "filters": {
                "limit": limit,
                "status_filter": status_filter,
                "include_metadata": include_metadata
            },
            "queue_statistics": queue_stats
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error listing batch statuses: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/batch/deadletter/process", dependencies=[Depends(verify_api_key)])
async def process_dead_letter_queue(max_messages: int = 10):
    """
    Process messages from the dead letter queue
    
    Args:
        max_messages: Maximum number of messages to process
    
    Returns:
        Dead letter processing results
    """
    try:
        # Initialize Service Bus manager if needed
        if not hasattr(app.state, 'service_bus_manager'):
            from app.service_bus_manager import ServiceBusManager
            app.state.service_bus_manager = ServiceBusManager()
            await app.state.service_bus_manager.connect()
        
        results = await app.state.service_bus_manager.process_dead_letter_queue(max_messages)
        
        return {
            "status": "success",
            "processed_messages": len(results),
            "results": results
        }
        
    except Exception as e:
        logger.error(f"Error processing dead letter queue: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# Include Vault Agent router
app.include_router(vault_agent_router)

# Include Admin policies router
app.include_router(policies_router)

# Include Admin import v2 router
app.include_router(import_v2_router)

# Include Apollo.io API router for comprehensive phone discovery
from app.api.apollo.routes import router as apollo_router
app.include_router(apollo_router)

@app.get("/cache/status", dependencies=[Depends(verify_api_key)])
async def get_cache_status():
    """Get comprehensive Redis cache performance metrics, health status, and optimization recommendations"""
    try:
        from app.redis_cache_manager import get_cache_manager, get_redis_health_status
        from app.cache_strategies import get_strategy_manager
        
        cache_manager = await get_cache_manager()
        strategy_manager = get_strategy_manager()
        
        # Get health status first
        redis_health = await get_redis_health_status()
        
        # Get cache metrics (will handle Redis being unavailable gracefully)
        cache_metrics = await cache_manager.get_metrics()
        
        # Get strategy metrics
        try:
            strategy_metrics = strategy_manager.get_metrics()
            optimizations = strategy_manager.optimize_cache_strategy(cache_metrics)
        except Exception as e:
            logger.warning(f"Failed to get strategy metrics: {e}")
            strategy_metrics = {}
            optimizations = []
        
        # Enhanced status determination
        overall_status = "healthy"
        if redis_health.get("status") == "not_configured":
            overall_status = "not_configured"
        elif redis_health.get("status") == "circuit_breaker_open":
            overall_status = "circuit_breaker_open"
        elif cache_manager.fallback_mode:
            overall_status = "fallback_mode"
        elif not cache_manager._connected:
            overall_status = "disconnected"
        elif redis_health.get("status") in ["unhealthy", "degraded"]:
            overall_status = "degraded"
        
        return {
            "status": overall_status,
            "health_check": redis_health,
            "cache_metrics": cache_metrics,
            "strategy_metrics": strategy_metrics,
            "optimizations": optimizations,
            "cost_analysis": {
                "gpt5_mini_per_million": "$0.25",
                "cached_per_million": "$0.025",
                "savings_ratio": "90%",
                "estimated_monthly_savings": f"${cache_metrics.get('estimated_monthly_savings', 0):.2f}"
            },
            "reliability": {
                "uptime_percentage": cache_metrics.get("uptime_percentage", "0%"),
                "circuit_breaker_status": "open" if cache_manager.circuit_breaker.is_open else "closed",
                "fallback_activations": cache_metrics.get("fallback_activations", 0),
                "connection_failures": cache_metrics.get("connection_failures", 0),
                "timeout_failures": cache_metrics.get("timeout_failures", 0)
            },
            "configuration": {
                "max_failures_before_circuit_breaker": cache_manager.max_failures,
                "failure_timeout_minutes": int(cache_manager.failure_timeout.total_seconds() / 60),
                "connection_timeout_seconds": cache_manager.connection_timeout,
                "operation_timeout_seconds": cache_manager.operation_timeout,
                "max_retries": cache_manager.max_retries
            }
        }
        
    except Exception as e:
        logger.error(f"Error getting cache status: {e}")
        return {
            "status": "error",
            "error": str(e),
            "health_check": {"status": "error", "error": str(e)},
            "cache_metrics": {},
            "strategy_metrics": {},
            "optimizations": [],
            "cost_analysis": {
                "gpt5_mini_per_million": "$0.25",
                "cached_per_million": "$0.025",
                "savings_ratio": "90%",
                "estimated_monthly_savings": "$0.00"
            },
            "reliability": {
                "uptime_percentage": "0%",
                "circuit_breaker_status": "unknown",
                "fallback_activations": 0
            },
            "message": "Cache status check failed"
        }

@app.get("/prompt/enhancement/status", dependencies=[Depends(verify_api_key)])
async def get_prompt_enhancement_status(email_domain: str = "example.com"):
    """Get status of prompt enhancement capabilities for debugging and monitoring"""
    try:
        from app.langgraph_manager import EmailProcessingWorkflow
        
        workflow = EmailProcessingWorkflow()
        status = await workflow.get_prompt_enhancement_status(email_domain)
        
        # Add summary information
        status["summary"] = {
            "enhancement_enabled": status["enhancement_ready"],
            "data_sources": {
                "correction_patterns": status["domain_patterns_count"] > 0,
                "company_templates": status["company_template_available"],
                "azure_ai_search": status["azure_search_available"]
            },
            "learning_capabilities": {
                "correction_learning": status["correction_service_available"],
                "ab_testing": status["prompt_variants_active"],
                "analytics": status["learning_analytics_available"]
            }
        }
        
        return {
            "status": "success",
            "email_domain": email_domain,
            "timestamp": datetime.utcnow().isoformat(),
            **status
        }
        
    except Exception as e:
        logger.error(f"Error getting prompt enhancement status: {e}")
        return {
            "status": "error",
            "error": str(e),
            "email_domain": email_domain,
            "timestamp": datetime.utcnow().isoformat(),
            "enhancement_ready": False,
            "summary": {
                "enhancement_enabled": False,
                "error_message": str(e)
            }
        }

@app.post("/cache/invalidate", dependencies=[Depends(verify_api_key)])
async def invalidate_cache(pattern: Optional[str] = None):
    """Invalidate cache entries matching a pattern with graceful fallback handling"""
    try:
        from app.redis_cache_manager import get_cache_manager
        
        cache_manager = await get_cache_manager()
        deleted_count = await cache_manager.invalidate_cache(pattern)
        
        # Check if we're in fallback mode
        if cache_manager.fallback_mode:
            return {
                "status": "fallback_mode",
                "deleted_entries": 0,
                "pattern": pattern or "well:email:*",
                "message": "Cache invalidation skipped - Redis unavailable",
                "fallback_reason": cache_manager.fallback_reason
            }
        
        return {
            "status": "success",
            "deleted_entries": deleted_count,
            "pattern": pattern or "well:email:*",
            "redis_status": "connected" if cache_manager._connected else "disconnected"
        }
        
    except Exception as e:
        logger.error(f"Error invalidating cache: {e}")
        return {
            "status": "error",
            "error": str(e),
            "deleted_entries": 0,
            "pattern": pattern or "well:email:*",
            "message": "Cache invalidation failed"
        }

@app.post("/cache/warmup", dependencies=[Depends(verify_api_key)])
async def warmup_cache():
    """Pre-warm cache with common email patterns with graceful fallback handling"""
    try:
        from app.redis_cache_manager import get_cache_manager
        from app.cache_strategies import get_strategy_manager
        
        cache_manager = await get_cache_manager()
        strategy_manager = get_strategy_manager()
        
        # Get common patterns
        common_patterns = strategy_manager.get_common_patterns()
        
        # Warm up cache
        cached_count = await cache_manager.warmup_cache(common_patterns)
        
        # Check if we're in fallback mode
        if cache_manager.fallback_mode:
            return {
                "status": "fallback_mode",
                "patterns_cached": 0,
                "total_patterns": len(common_patterns),
                "patterns": [p["key"] for p in common_patterns],
                "message": "Cache warmup skipped - Redis unavailable",
                "fallback_reason": cache_manager.fallback_reason
            }
        
        return {
            "status": "success",
            "patterns_cached": cached_count,
            "total_patterns": len(common_patterns),
            "patterns": [p["key"] for p in common_patterns],
            "redis_status": "connected" if cache_manager._connected else "disconnected",
            "success_rate": f"{(cached_count / len(common_patterns) * 100):.1f}%" if common_patterns else "N/A"
        }
        
    except Exception as e:
        logger.error(f"Error warming up cache: {e}")
        return {
            "status": "error",
            "error": str(e),
            "patterns_cached": 0,
            "total_patterns": 0,
            "patterns": [],
            "message": "Cache warmup failed"
        }

# Redis monitoring and alerting endpoints
@app.get("/cache/alerts", dependencies=[Depends(verify_api_key)])
async def get_cache_alerts(hours: int = 24):
    """Get Redis cache alerts from the last N hours"""
    try:
        from app.redis_monitoring import get_monitoring_service
        
        monitoring_service = get_monitoring_service()
        alert_summary = monitoring_service.get_alert_summary(hours=hours)
        
        return {
            "status": "success",
            "alert_summary": alert_summary
        }
    except Exception as e:
        logger.error(f"Error getting cache alerts: {e}")
        return {
            "status": "error",
            "error": str(e),
            "alert_summary": {}
        }

@app.get("/cache/metrics/report", dependencies=[Depends(verify_api_key)])
async def get_cache_metrics_report(hours: int = 24):
    """Get comprehensive Redis cache metrics report"""
    try:
        from app.redis_monitoring import get_monitoring_service
        
        monitoring_service = get_monitoring_service()
        metrics_report = monitoring_service.get_metrics_report(hours=hours)
        
        return {
            "status": "success",
            "metrics_report": metrics_report
        }
    except Exception as e:
        logger.error(f"Error getting cache metrics report: {e}")
        return {
            "status": "error",
            "error": str(e),
            "metrics_report": {}
        }

@app.post("/cache/monitoring/start", dependencies=[Depends(verify_api_key)])
async def start_cache_monitoring(interval_minutes: int = 5):
    """Start Redis cache monitoring service"""
    try:
        from app.redis_monitoring import get_monitoring_service
        from app.redis_cache_manager import get_cache_manager
        
        monitoring_service = get_monitoring_service()
        cache_manager = await get_cache_manager()
        
        await monitoring_service.start_monitoring(cache_manager, interval_minutes)
        
        return {
            "status": "success",
            "message": f"Redis monitoring started with {interval_minutes} minute intervals",
            "interval_minutes": interval_minutes
        }
    except Exception as e:
        logger.error(f"Error starting cache monitoring: {e}")
        return {
            "status": "error",
            "error": str(e),
            "message": "Failed to start Redis monitoring"
        }

@app.get("/cache/health/detailed", dependencies=[Depends(verify_api_key)])
async def get_detailed_cache_health():
    """Get detailed Redis cache health with monitoring data"""
    try:
        from app.redis_monitoring import get_monitoring_service
        from app.redis_cache_manager import get_redis_health_status
        from datetime import datetime
        
        # Get basic health status
        redis_health = await get_redis_health_status()
        
        # Get monitoring data
        monitoring_service = get_monitoring_service()
        alert_summary = monitoring_service.get_alert_summary(hours=1)  # Last hour
        metrics_report = monitoring_service.get_metrics_report(hours=1)  # Last hour
        
        return {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "redis_health": redis_health,
            "recent_alerts": alert_summary,
            "metrics_summary": metrics_report,
            "overall_status": "healthy" if redis_health.get("status") == "healthy" and alert_summary.get("unresolved_alerts", 0) == 0 else "degraded"
        }
    except Exception as e:
        logger.error(f"Error getting detailed cache health: {e}")
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

# Manifest cache management endpoints
@app.get("/manifest/cache/status", dependencies=[Depends(verify_api_key)])
async def get_manifest_cache_status():
    """Get manifest cache performance metrics and configuration"""
    try:
        manifest_service = await get_manifest_cache_service()
        status = await manifest_service.get_cache_status()
        return status
    except Exception as e:
        logger.error(f"Error getting manifest cache status: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/manifest/cache/invalidate", dependencies=[Depends(verify_api_key)])
async def invalidate_manifest_cache(
    environment: str = None,
    variant: str = None,
    pattern: str = None
):
    """Invalidate manifest cache entries with instant invalidation"""
    try:
        manifest_service = await get_manifest_cache_service()
        
        # Convert string parameters to enums if provided
        env = None
        var = None
        
        if environment:
            try:
                env = Environment(environment.lower())
            except ValueError:
                raise HTTPException(status_code=400, detail=f"Invalid environment: {environment}")
        
        if variant:
            try:
                var = ManifestVariant(variant.lower())
            except ValueError:
                raise HTTPException(status_code=400, detail=f"Invalid variant: {variant}")
        
        deleted_count = await manifest_service.invalidate_cache(env, var, pattern)
        
        return {
            "status": "success",
            "entries_invalidated": deleted_count,
            "environment": environment,
            "variant": variant,
            "pattern": pattern
        }
    except Exception as e:
        logger.error(f"Error invalidating manifest cache: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/manifest/cache/warmup", dependencies=[Depends(verify_api_key)])
async def warmup_manifest_cache(environments: List[str] = None):
    """Pre-warm manifest cache for specified environments"""
    try:
        manifest_service = await get_manifest_cache_service()
        
        # Convert string environments to enums
        env_list = []
        if environments:
            for env_str in environments:
                try:
                    env_list.append(Environment(env_str.lower()))
                except ValueError:
                    raise HTTPException(status_code=400, detail=f"Invalid environment: {env_str}")
        
        results = await manifest_service.warmup_cache(env_list if env_list else None)
        
        return {
            "status": "success",
            "warmup_results": results,
            "total_manifests_cached": sum(r["success"] for r in results.values())
        }
    except Exception as e:
        logger.error(f"Error warming up manifest cache: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/manifest/template/update", dependencies=[Depends(verify_api_key)])
async def update_manifest_template(
    environment: str,
    variant: str,
    template_updates: Dict[str, Any]
):
    """Update manifest template and invalidate related cache entries"""
    try:
        manifest_service = await get_manifest_cache_service()
        
        # Convert parameters to enums
        try:
            env = Environment(environment.lower())
            var = ManifestVariant(variant.lower())
        except ValueError as e:
            raise HTTPException(status_code=400, detail=f"Invalid parameter: {str(e)}")
        
        success = await manifest_service.update_template(env, var, template_updates)
        
        if success:
            return {
                "status": "success",
                "environment": environment,
                "variant": variant,
                "updates_applied": list(template_updates.keys())
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to update template")
            
    except Exception as e:
        logger.error(f"Error updating manifest template: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Authentication endpoints for Voice UI
@app.post("/auth/validate")
async def validate_voice_ui_token(auth_info: dict = Depends(verify_user_auth)):
    """Validate authentication token for Voice UI"""
    try:
        return {
            "status": "valid",
            "auth_type": auth_info["type"],
            "user_id": auth_info["user_id"],
            "email": auth_info.get("email"),
            "name": auth_info.get("name"),
            "timestamp": datetime.utcnow().isoformat(),
            "voice_ui_access": True,
            "permissions": ["voice_interface", "email_processing"]
        }
    except Exception as e:
        logger.error(f"Token validation failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Token validation failed"
        )

@app.get("/auth/user-info")
async def get_user_info(auth_info: dict = Depends(verify_user_auth)):
    """Get authenticated user information"""
    try:
        user_data = {
            "user_id": auth_info["user_id"],
            "auth_type": auth_info["type"],
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # Add user details for Microsoft auth
        if auth_info["type"] == "microsoft":
            user_data.update({
                "email": auth_info.get("email"),
                "name": auth_info.get("name"),
                "provider": "Microsoft Azure AD"
            })
        elif auth_info["type"] == "test":
            user_data.update({
                "email": "test@thewell.com",
                "name": "Test User",
                "provider": "Development Test"
            })
        
        return user_data
        
    except Exception as e:
        logger.error(f"Failed to get user info: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to retrieve user information"
        )

# Static file serving for Outlook Add-in
@app.get("/manifest.xml")
async def get_manifest(request: Request):
    """Serve Outlook Add-in manifest - direct file serving as primary method"""
    # Direct file serving for reliability
    manifest_path = os.path.join(os.path.dirname(__file__), "..", "addin", "manifest.xml")
    if os.path.exists(manifest_path):
        # Add cache busting headers
        return FileResponse(
            manifest_path, 
            media_type="application/xml; charset=utf-8",
            headers={
                "Cache-Control": "public, max-age=3600",
                "X-Manifest-Version": "1.5.0"
            }
        )
    
    # Fallback error if file not found
    logger.error(f"Manifest file not found at {manifest_path}")
    raise HTTPException(status_code=404, detail="Manifest.xml not found")

@app.get("/manifest.json")
async def get_manifest_json(request: Request):
    """Serve Unified Outlook Add-in manifest (JSON format for modern Office)"""
    manifest_path = os.path.join(os.path.dirname(__file__), "..", "addin", "manifest.json")
    if os.path.exists(manifest_path):
        return FileResponse(
            manifest_path, 
            media_type="application/json; charset=utf-8",
            headers={
                "Cache-Control": "public, max-age=3600",
                "X-Manifest-Version": "1.5.0",
                "X-Manifest-Type": "unified"
            }
        )
    
    logger.error(f"Manifest file not found at {manifest_path}")
    raise HTTPException(status_code=404, detail="Manifest.json not found")

@app.get("/commands.js")
async def get_commands():
    """Serve Outlook Add-in JavaScript"""
    js_path = os.path.join(os.path.dirname(__file__), "..", "addin", "commands.js")
    if os.path.exists(js_path):
        return FileResponse(js_path, media_type="application/javascript")
    raise HTTPException(status_code=404, detail="Commands.js not found")

@app.get("/commands.html")
async def get_commands_html():
    """Serve Outlook Add-in HTML"""
    html_path = os.path.join(os.path.dirname(__file__), "..", "addin", "commands.html")
    if os.path.exists(html_path):
        return FileResponse(html_path, media_type="text/html")
    raise HTTPException(status_code=404, detail="Commands.html not found")

@app.get("/config.js")
async def get_config_js():
    """Serve Outlook Add-in configuration JavaScript"""
    js_path = os.path.join(os.path.dirname(__file__), "..", "addin", "config.js")
    if os.path.exists(js_path):
        return FileResponse(js_path, media_type="application/javascript")
    raise HTTPException(status_code=404, detail="Config.js not found")

@app.get("/placeholder.html")
async def get_placeholder_html():
    """Serve placeholder HTML for Outlook Add-in"""
    # Create a minimal placeholder HTML if it doesn't exist
    placeholder_content = """<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <title>The Well Recruiting - Send to Zoho</title>
</head>
<body>
    <div style="padding: 20px; font-family: Arial, sans-serif;">
        <h2>Send to Zoho</h2>
        <p>Click the "Send to Zoho" button in the ribbon to process this email.</p>
    </div>
</body>
</html>"""
    return HTMLResponse(content=placeholder_content)

@app.get("/manifest/warmup/status")
async def get_manifest_warmup_status(request: Request):
    """Get manifest cache warmup status"""
    try:
        from app.startup_warmup import get_manifest_warmup_status
        
        # Get warmup status
        status = await get_manifest_warmup_status()
        
        # Add current request environment for context
        host = str(request.url.hostname).lower()
        current_env = 'development' if 'localhost' in host or '127.0.0.1' in host else 'production'
        
        return {
            "warmup_status": status,
            "current_environment": current_env,
            "startup_stats": getattr(request.app.state, 'manifest_warmup_stats', {}),
            "api_endpoints": {
                "generate_manifest": "/api/manifest/generate",
                "warmup_cache": "/api/manifest/warmup",
                "cache_status": "/api/manifest/cache/status",
                "templates": "/api/manifest/templates"
            }
        }
        
    except Exception as e:
        logger.error(f"Error getting manifest warmup status: {e}")
        return {
            "error": str(e),
            "warmup_status": {"completed": False},
            "current_environment": "unknown"
        }

# Azure AD Authentication Endpoints
@app.get("/auth/login")
async def initiate_login(redirect_url: str = None):
    """Start Azure AD OAuth login flow"""
    try:
        auth_service = get_auth_service()
        
        # Generate state parameter for security
        state = f"redirect={redirect_url}" if redirect_url else "default"
        
        # Get authorization URL
        auth_url = auth_service.get_authorization_url(state)
        
        return {
            "auth_url": auth_url,
            "state": state,
            "message": "Redirect user to auth_url to complete login"
        }
    
    except Exception as e:
        logger.error(f"Login initiation error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/auth/callback")
async def handle_auth_callback(code: str = None, state: str = None, error: str = None):
    """Handle OAuth callback from Azure AD"""
    try:
        if error:
            logger.error(f"OAuth error: {error}")
            return HTMLResponse(f"""
                <html><body>
                    <h2>Authentication Error</h2>
                    <p>Error: {error}</p>
                    <p><a href="/auth/login">Try again</a></p>
                </body></html>
            """)
        
        if not code:
            raise HTTPException(status_code=400, detail="Authorization code not provided")
        
        auth_service = get_auth_service()
        
        # Exchange code for tokens
        token_info = await auth_service.exchange_code_for_tokens(code, state)
        
        # Generate success page with user info
        user_email = token_info.get('user_email', 'Unknown')
        user_name = token_info.get('user_name', 'User')
        
        success_html = f"""
        <html>
        <head>
            <title>Authentication Successful</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 40px; }}
                .success {{ color: green; }}
                .info {{ background: #f0f8ff; padding: 20px; border-radius: 5px; }}
                .token {{ font-family: monospace; word-break: break-all; }}
            </style>
        </head>
        <body>
            <h2 class="success"> Authentication Successful!</h2>
            <div class="info">
                <p><strong>Welcome, {user_name}!</strong></p>
                <p><strong>Email:</strong> {user_email}</p>
                <p><strong>User ID:</strong> {token_info['user_id']}</p>
                <p><strong>Scopes:</strong> {', '.join(token_info.get('scopes', []))}</p>
            </div>
            <p>You can now close this window and return to the Voice UI application.</p>
            <p><a href="/api/user/info?user_id={token_info['user_id']}">View User Info</a></p>
            
            <script>
                // Auto-close window after 5 seconds
                setTimeout(function() {{
                    if (window.opener) {{
                        window.close();
                    }}
                }}, 5000);
            </script>
        </body>
        </html>
        """
        
        logger.info(f"Successfully authenticated user {user_email}")
        return HTMLResponse(success_html)
    
    except Exception as e:
        logger.error(f"Auth callback error: {e}")
        return HTMLResponse(f"""
            <html><body>
                <h2>Authentication Error</h2>
                <p>Error: {str(e)}</p>
                <p><a href="/auth/login">Try again</a></p>
            </body></html>
        """)

@app.get("/api/user/info", dependencies=[Depends(verify_api_key)])
async def get_user_info(user_id: str):
    """Get authenticated user information"""
    try:
        auth_service = get_auth_service()
        user_info = auth_service.get_user_info(user_id)
        
        if not user_info:
            raise HTTPException(status_code=404, detail="User not found or not authenticated")
        
        return user_info
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Get user info error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/user/logout", dependencies=[Depends(verify_api_key)])
async def logout_user(request: Request):
    """Logout user and remove tokens"""
    try:
        body = await request.json()
        user_id = body.get("user_id")
        
        if not user_id:
            raise HTTPException(status_code=400, detail="User ID required")
        
        auth_service = get_auth_service()
        success = auth_service.logout_user(user_id)
        
        if success:
            return {"status": "logged_out", "user_id": user_id}
        else:
            return {"status": "not_found", "user_id": user_id}
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Logout error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/auth/users", dependencies=[Depends(verify_api_key)])
async def get_authenticated_users():
    """Get list of all authenticated users (admin endpoint)"""
    try:
        auth_service = get_auth_service()
        users = auth_service.get_all_authenticated_users()
        
        return {
            "authenticated_users": users,
            "total_count": len(users),
            "active_count": sum(1 for user in users.values() if user['is_valid'])
        }
    
    except Exception as e:
        logger.error(f"Get authenticated users error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# WebSocket Endpoint for Real-time Queue Updates
@app.websocket("/ws/queue/{user_id}")
async def websocket_queue_endpoint(websocket: WebSocket, user_id: str):
    """WebSocket endpoint for real-time email queue updates"""
    await websocket_endpoint(websocket, user_id)

# Apollo WebSocket endpoints removed - now using REST API only

@app.get("/taskpane.html")
async def get_taskpane():
    """Serve Outlook Add-in task pane"""
    html_path = os.path.join(os.path.dirname(__file__), "..", "addin", "taskpane.html")
    if os.path.exists(html_path):
        return FileResponse(html_path, media_type="text/html")
    raise HTTPException(status_code=404, detail="Taskpane.html not found")

@app.get("/taskpane.js")
async def get_taskpane_js():
    """Serve Outlook Add-in task pane JavaScript"""
    # Try multiple path resolutions
    possible_paths = [
        os.path.join(os.path.dirname(__file__), "..", "addin", "taskpane.js"),
        os.path.join("/app", "addin", "taskpane.js"),
        os.path.join(os.getcwd(), "addin", "taskpane.js"),
    ]
    
    for js_path in possible_paths:
        if os.path.exists(js_path):
            return FileResponse(js_path, media_type="application/javascript")
    
    # Log which paths were tried for debugging
    logger.error(f"taskpane.js not found. Tried paths: {possible_paths}")
    raise HTTPException(status_code=404, detail=f"Taskpane.js not found. Tried: {possible_paths}")

@app.get("/apollo-styles.css")
async def get_apollo_styles():
    """Serve Apollo integration CSS styles"""
    possible_paths = [
        os.path.join(os.path.dirname(__file__), "..", "addin", "apollo-styles.css"),
        os.path.join("/app", "addin", "apollo-styles.css"),
        os.path.join(os.getcwd(), "addin", "apollo-styles.css"),
    ]

    for css_path in possible_paths:
        if os.path.exists(css_path):
            return FileResponse(css_path, media_type="text/css")

    logger.error(f"apollo-styles.css not found. Tried paths: {possible_paths}")
    raise HTTPException(status_code=404, detail=f"Apollo styles not found. Tried: {possible_paths}")

# Apollo WebSocket and SignalR endpoints removed - now using REST API only

# Voice UI API Endpoints
@app.post("/api/voice/process", dependencies=[Depends(verify_api_key)])
async def process_voice_command(request: Request):
    """Process voice command and return appropriate action"""
    try:
        body = await request.json()
        command = body.get("command", "").lower()
        context = body.get("context", {})
        
        # Input validation
        if not command or len(command) > 500:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Command must be between 1 and 500 characters"
            )
        
        # Sanitize command (remove potentially dangerous characters)
        import re
        command = re.sub(r'[^\w\s\-.,?!]', '', command)
        
        # Process command and determine action
        response = {
            "action": "unknown",
            "data": {},
            "message": f"Processed command: {command}",
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # Basic command processing
        if "process" in command and "email" in command:
            response["action"] = "processEmail"
        elif "approve" in command:
            response["action"] = "approveExtraction"
        elif "reject" in command:
            response["action"] = "rejectExtraction"
        elif "next" in command:
            response["action"] = "nextEmail"
        elif "previous" in command:
            response["action"] = "previousEmail"
        elif "queue" in command:
            response["action"] = "showQueue"
        elif "metrics" in command:
            response["action"] = "showMetrics"
        
        logger.info(f"Voice command processed: {command} -> {response['action']}")
        return response
        
    except Exception as e:
        logger.error(f"Voice command processing error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/emails/queue", dependencies=[Depends(verify_api_key)])
async def get_email_queue(user_id: str = None):
    """Get real-time email processing queue"""
    try:
        # Get queue manager instance
        queue_manager = get_queue_manager()
        
        # Sync emails from Microsoft Graph to queue if needed
        await sync_emails_to_queue(user_id)
        
        # Get current queue state
        queue_data = await queue_manager.get_queue_state(user_id)
        
        logger.info(f"Retrieved queue with {queue_data.get('total_count', 0)} emails for user {user_id}")
        return queue_data
        
    except Exception as e:
        logger.error(f"Email queue retrieval error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def sync_emails_to_queue(user_id: str = None):
    """Sync emails from Microsoft Graph to real-time queue"""
    try:
        if not hasattr(app.state, 'graph_client') or not app.state.graph_client:
            return
        
        queue_manager = get_queue_manager()
        
        # Get recruitment emails from Microsoft Graph
        org_emails = await app.state.graph_client.get_recruitment_emails_for_organization(
            hours_back=24,
            max_emails_per_user=10
        )
        
        emails_added = 0
        for source_user, user_emails in org_emails.items():
            for email in user_emails:
                # Convert Microsoft Graph email to queue format
                email_data = {
                    "id": email.id,
                    "from_address": email.from_address,
                    "from_name": email.from_name,
                    "subject": email.subject,
                    "body": email.body,
                    "timestamp": email.received_time,
                    "priority": "high" if email.importance == "high" else "normal",
                    "has_attachments": email.has_attachments,
                    "source_user": source_user
                }
                
                # Determine initial status based on categories
                if "Processed by Zoho" in email.categories:
                    initial_status = EmailStatus.PROCESSED
                elif "Processing" in email.categories:
                    initial_status = EmailStatus.PROCESSING
                else:
                    initial_status = EmailStatus.PENDING
                
                # Add to queue (will skip if already exists)
                try:
                    await queue_manager.add_email_to_queue(email_data, user_id or source_user)
                    # Set the appropriate status
                    await queue_manager.update_email_status(email.id, initial_status)
                    emails_added += 1
                except Exception as add_error:
                    # Email might already exist, which is fine
                    logger.debug(f"Email {email.id} already in queue or add failed: {add_error}")
        
        if emails_added > 0:
            logger.info(f"Synced {emails_added} emails from Microsoft Graph to queue")
        
    except Exception as e:
        logger.warning(f"Error syncing emails to queue: {e}")

@app.get("/api/emails/{email_id}", dependencies=[Depends(verify_api_key)])
async def get_email_by_id(email_id: str):
    """Get specific email details by ID from Microsoft Graph"""
    try:
        # Try to get from Microsoft Graph first
        if hasattr(app.state, 'graph_client') and app.state.graph_client:
            try:
                # Get emails from organization and find the specific email
                org_emails = await app.state.graph_client.get_recruitment_emails_for_organization(
                    hours_back=168,  # Last week to find the email
                    max_emails_per_user=50
                )
                
                # Search for the specific email ID
                for user_email, user_emails in org_emails.items():
                    for email in user_emails:
                        if email.id == email_id:
                            # Process email through LangGraph if not already processed
                            extracted_data = None
                            if "Processed by Zoho" not in email.categories:
                                try:
                                    from app.langgraph_manager import EmailProcessingWorkflow
                                    workflow = EmailProcessingWorkflow()
                                    extracted_data = await workflow.process_email(
                                        email.body, 
                                        email.from_address.split('@')[1] if '@' in email.from_address else 'unknown.com'
                                    )
                                    logger.info(f"Processed email {email_id} with LangGraph")
                                except Exception as process_error:
                                    logger.warning(f"LangGraph processing failed for {email_id}: {process_error}")
                            
                            return {
                                "id": email.id,
                                "from": email.from_address,
                                "from_name": email.from_name,
                                "subject": email.subject,
                                "body": email.body,
                                "attachments": email.attachments,
                                "extracted_data": extracted_data.model_dump() if extracted_data else None,
                                "status": "processed" if "Processed by Zoho" in email.categories else "pending",
                                "timestamp": email.received_time,
                                "has_attachments": email.has_attachments,
                                "is_read": email.is_read,
                                "importance": email.importance,
                                "categories": email.categories,
                                "source_user": user_email
                            }
                
                # Email not found in Microsoft Graph
                logger.warning(f"Email {email_id} not found in Microsoft Graph")
                raise HTTPException(status_code=404, detail=f"Email {email_id} not found")
                
            except HTTPException:
                raise
            except Exception as graph_error:
                logger.error(f"Microsoft Graph error retrieving email {email_id}: {graph_error}")
                raise HTTPException(status_code=500, detail=f"Error retrieving email: {graph_error}")
        
        # Fallback if Graph client not available
        else:
            logger.warning("Microsoft Graph client not available")
            raise HTTPException(status_code=503, detail="Email service not available")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Email retrieval error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/emails/{email_id}/preview", dependencies=[Depends(verify_api_key)])
async def preview_extraction(email_id: str, request: Request):
    """Preview extraction results for approval"""
    try:
        body = await request.json()
        extracted_data = body.get("extractedData", {})
        
        # Process through business rules for preview
        preview_data = {
            "email_id": email_id,
            "deal_name": business_rules.format_deal_name(
                extracted_data.get("job_title", "Unknown"),
                extracted_data.get("job_location", "Unknown"),
                extracted_data.get("company_name", "Unknown")
            ),
            "source": business_rules.determine_source(
                extracted_data.get("referrer_name"),
                extracted_data.get("email_content", "")
            ),
            "confidence": extracted_data.get("confidence", 0.8),
            "extracted_data": extracted_data,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        return preview_data
        
    except Exception as e:
        logger.error(f"Extraction preview error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/emails/{email_id}/approve", dependencies=[Depends(verify_api_key)])
async def approve_extraction(email_id: str, request: Request):
    """Approve extraction with any edits"""
    try:
        body = await request.json()
        extracted_data = body.get("extractedData", {})
        edits = body.get("edits", {})
        
        # Apply edits to extracted data
        final_data = {**extracted_data, **edits}
        
        # Process through existing intake pipeline
        email_payload = EmailRequest(
            from_address=final_data.get("from_address", ""),
            subject=final_data.get("subject", ""),
            body=final_data.get("email_content", ""),
            attachments=[]
        )
        
        # Use existing processing logic
        zoho_integration = app.state.zoho_integration
        result = await process_email_internal(email_payload, zoho_integration)
        
        logger.info(f"Email {email_id} approved and processed")
        return {
            "status": "approved",
            "zoho_result": result,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Extraction approval error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/emails/{email_id}/reject", dependencies=[Depends(verify_api_key)])
async def reject_extraction(email_id: str, request: Request):
    """Reject extraction with reason"""
    try:
        body = await request.json()
        reason = body.get("reason", "No reason provided")
        
        # Log rejection for learning
        logger.info(f"Email {email_id} rejected: {reason}")
        
        return {
            "status": "rejected",
            "reason": reason,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Extraction rejection error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/learning/feedback", dependencies=[Depends(verify_api_key)])
async def submit_learning_feedback(request: Request):
    """Submit feedback for human-in-the-loop learning"""
    try:
        body = await request.json()
        
        feedback_data = {
            "email_id": body.get("emailId"),
            "original_data": body.get("originalData", {}),
            "corrected_data": body.get("correctedData", {}),
            "confidence": body.get("confidence", 1.0),
            "user_agent": body.get("userAgent", ""),
            "timestamp": body.get("timestamp", datetime.utcnow().isoformat())
        }
        
        # Store in PostgreSQL if available
        if hasattr(app.state, 'postgres_client') and app.state.postgres_client:
            try:
                await app.state.postgres_client.store_learning_feedback(feedback_data)
            except Exception as db_error:
                logger.warning(f"Failed to store learning feedback in database: {db_error}")
        
        logger.info(f"Learning feedback submitted for email {feedback_data['email_id']}")
        return {
            "status": "feedback_stored",
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Learning feedback error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/metrics/accuracy", dependencies=[Depends(verify_api_key)])
async def get_accuracy_metrics(timeframe: str = "24h"):
    """Get extraction accuracy metrics"""
    try:
        # Mock metrics - in production, calculate from database
        metrics = {
            "timeframe": timeframe,
            "total_extractions": 150,
            "accurate_extractions": 142,
            "accuracy_rate": 0.947,
            "improvement_rate": 0.023,
            "common_corrections": [
                {"field": "company_name", "frequency": 8},
                {"field": "job_location", "frequency": 5},
                {"field": "job_title", "frequency": 3}
            ],
            "timestamp": datetime.utcnow().isoformat()
        }
        
        return metrics
        
    except Exception as e:
        logger.error(f"Metrics retrieval error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/metrics/system", dependencies=[Depends(verify_api_key)])
async def get_system_metrics():
    """Get system performance metrics"""
    try:
        metrics = {
            "processing_speed": {
                "average_time": "2.3s",
                "median_time": "2.1s",
                "95th_percentile": "4.2s"
            },
            "cache_performance": {
                "hit_rate": 0.73,
                "miss_rate": 0.27,
                "cost_savings": 0.68
            },
            "system_health": {
                "cpu_usage": 0.35,
                "memory_usage": 0.42,
                "disk_usage": 0.18
            },
            "timestamp": datetime.utcnow().isoformat()
        }
        
        return metrics
        
    except Exception as e:
        logger.error(f"System metrics error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Helper function for processing emails internally
async def process_email_internal(email_payload: EmailRequest, zoho_integration):
    """Internal email processing logic"""
    try:
        # Use existing LangGraph processing
        use_langgraph = os.getenv("USE_LANGGRAPH", "true").lower() == "true"
        
        if use_langgraph:
            from app.langgraph_manager import EmailProcessor
            processor = EmailProcessor()
            extracted_data = await processor.process_email_async(
                email_content=email_payload.body,
                sender_domain=email_payload.from_address.split('@')[-1] if '@' in email_payload.from_address else ""
            )
        else:
            # Fallback to simple extraction
            extracted_data = ExtractedData(
                job_title="Unknown Position",
                company_name="Unknown Company",
                job_location="Unknown Location"
            )
        
        # Apply business rules
        deal_name = business_rules.format_deal_name(
            extracted_data.job_title,
            extracted_data.job_location,
            extracted_data.company_name
        )
        
        source, source_detail = business_rules.determine_source(
            extracted_data.referrer_name,
            email_payload.body
        )
        
        # Create Zoho records
        result = await zoho_integration.create_contact_and_deal(
            extracted_data, 
            deal_name, 
            source, 
            source_detail,
            email_payload.from_address
        )
        
        return result
        
    except Exception as e:
        logger.error(f"Internal email processing error: {e}")
        raise

# GitHub Webhook Endpoints for Cache Invalidation
@app.post("/api/webhook/github")
async def github_webhook_endpoint(request: Request):
    """
    GitHub webhook endpoint for automatic cache invalidation.
    Handles push events for manifest-related file changes.
    """
    try:
        from app.webhook_handlers import verify_github_webhook, handle_github_webhook, log_webhook_event
        
        # Verify webhook signature and parse payload
        webhook_data = await verify_github_webhook(request)
        
        logger.info(f"Received GitHub webhook: {webhook_data['event_type']}")
        
        # Process the webhook
        processing_result = await handle_github_webhook(webhook_data)
        
        # Log to Application Insights for monitoring
        try:
            await log_webhook_event(webhook_data, processing_result)
        except Exception as log_error:
            logger.warning(f"Failed to log webhook event: {log_error}")
        
        return {
            "status": "success",
            "webhook_processed": True,
            "event_type": webhook_data["event_type"],
            "result": processing_result,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"GitHub webhook processing error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Webhook processing failed: {str(e)}"
        )

@app.get("/api/webhook/github/status", dependencies=[Depends(verify_api_key)])
async def get_webhook_status():
    """Get GitHub webhook handler status and statistics."""
    try:
        from app.webhook_handlers import get_webhook_handler
        
        handler = get_webhook_handler()
        stats = handler.get_stats()
        
        return {
            "status": "operational",
            "webhook_endpoint": "/api/webhook/github",
            "statistics": stats,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting webhook status: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/webhook/github/test", dependencies=[Depends(verify_api_key)])
async def test_webhook_invalidation():
    """Test endpoint to manually trigger cache invalidation."""
    try:
        from app.webhook_handlers import get_webhook_handler
        
        handler = get_webhook_handler()
        
        # Simulate a push event with manifest changes
        test_payload = {
            "repository": {"full_name": "romiteld/outlook"},
            "ref": "refs/heads/main",
            "commits": [{
                "modified": ["addin/manifest.xml", "addin/commands.js"],
                "added": [],
                "removed": []
            }]
        }
        
        result = await handler.process_push_event(test_payload)
        
        return {
            "status": "test_completed",
            "result": result,
            "message": "Manual cache invalidation test completed",
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Webhook test error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Manifest Analytics Endpoints
@app.get("/api/manifest/status", dependencies=[Depends(verify_api_key)])
async def get_manifest_cache_status():
    """Get manifest cache status and performance metrics."""
    try:
        # Basic analytics data placeholder
        analytics_data = {
            "total_requests": 0,
            "cache_hits": 0,
            "cache_hit_rate": "0%",
            "errors": 0,
            "avg_response_time_ms": "0"
        }
        status_data = await analytics.get_cache_status()
        
        return {
            "status": "success",
            "data": status_data,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting manifest cache status: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve cache status: {str(e)}"
        )

@app.get("/api/manifest/metrics", dependencies=[Depends(verify_api_key)])
async def get_manifest_performance_metrics(hours: int = 24):
    """Get manifest performance analytics for the specified time period."""
    if hours < 1 or hours > 168:  # Limit to 7 days max
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Hours parameter must be between 1 and 168"
        )
    
    try:
        # Basic analytics data placeholder
        analytics_data = {
            "total_requests": 0,
            "cache_hits": 0,
            "cache_hit_rate": "0%",
            "errors": 0,
            "avg_response_time_ms": "0"
        }
        metrics_data = await analytics.get_performance_metrics(hours)
        
        return {
            "status": "success",
            "data": metrics_data,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting manifest metrics: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve performance metrics: {str(e)}"
        )

@app.post("/api/manifest/invalidate", dependencies=[Depends(verify_api_key)])
async def invalidate_manifest_cache(pattern: Optional[str] = None):
    """Manually invalidate manifest cache entries."""
    try:
        # Basic analytics data placeholder
        analytics_data = {
            "total_requests": 0,
            "cache_hits": 0,
            "cache_hit_rate": "0%",
            "errors": 0,
            "avg_response_time_ms": "0"
        }
        result = await analytics.invalidate_manifest_cache(pattern)
        
        return {
            "status": "success",
            "data": result,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error invalidating manifest cache: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Cache invalidation failed: {str(e)}"
        )

@app.get("/api/manifest/versions", dependencies=[Depends(verify_api_key)])
async def get_version_adoption_metrics():
    """Get version adoption tracking across different Office clients."""
    try:
        # Basic analytics data placeholder
        analytics_data = {
            "total_requests": 0,
            "cache_hits": 0,
            "cache_hit_rate": "0%",
            "errors": 0,
            "avg_response_time_ms": "0"
        }
        adoption_data = await analytics.get_version_adoption()
        
        return {
            "status": "success",
            "data": adoption_data,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting version adoption metrics: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve version adoption data: {str(e)}"
        )

@app.get("/api/manifest/health", dependencies=[Depends(verify_api_key)])
async def get_manifest_analytics_health():
    """Get manifest analytics service health status."""
    try:
        # Basic analytics data placeholder
        analytics_data = {
            "total_requests": 0,
            "cache_hits": 0,
            "cache_hit_rate": "0%",
            "errors": 0,
            "avg_response_time_ms": "0"
        }
        
        # Basic health data without analytics service
        health_data = {
            "analytics_service": "operational",
            "request_history_size": 0,
            "recent_requests_1h": 0,
            "version_stats_count": 0,
            "client_stats_count": 0,
            "cache_manager_available": False,
            "monitoring_service_available": False
        }
        
        return {
            "status": "healthy",
            "data": health_data,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting manifest analytics health: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Health check failed: {str(e)}"
        )


# ================================================================================================
# BATCH LEARNING AND ANALYTICS ENDPOINTS
# ================================================================================================

@app.get("/batch/learning/report", dependencies=[Depends(verify_api_key)])
async def get_batch_learning_report(batch_ids: str = None, days: int = 7):
    """
    Generate comprehensive learning effectiveness report for batch processing
    
    Args:
        batch_ids: Comma-separated list of batch IDs (optional)
        days: Number of days back to analyze (default: 7)
    
    Returns:
        Learning effectiveness report with recommendations
    """
    try:
        from app.batch_processor import create_enhanced_batch_processor
        
        processor = create_enhanced_batch_processor(
            postgres_client=app.state.postgres_client if hasattr(app.state, 'postgres_client') else None,
            enable_learning=True
        )
        
        if batch_ids:
            # Analyze specific batches
            batch_id_list = [bid.strip() for bid in batch_ids.split(',')]
            report = await processor.get_batch_learning_report(batch_id_list)
        else:
            # Get recent batches from database
            if hasattr(app.state, 'postgres_client') and app.state.postgres_client:
                # This would need to be implemented in PostgreSQL client
                # For now, return a structured response framework
                report = {
                    "message": "Recent batch analysis available",
                    "recommendation": "Provide specific batch_ids for detailed analysis",
                    "batch_count": 0,
                    "total_emails_analyzed": 0,
                    "overall_success_rate": 0.0,
                    "learning_impact": {
                        "patterns_usage": 0.0,
                        "templates_usage": 0.0
                    },
                    "recommendations": [
                        "Enable comprehensive logging for better batch tracking",
                        "Consider implementing pattern matching for domain-specific emails"
                    ]
                }
            else:
                report = {"error": "PostgreSQL client not available for batch history"}
        
        return report
        
    except Exception as e:
        logger.error(f"Error generating batch learning report: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/batch/optimization", dependencies=[Depends(verify_api_key)])
async def get_batch_optimization_report():
    """
    Analyze batch processing performance and provide optimization recommendations
    
    Returns:
        Optimization report with performance analysis and recommendations
    """
    try:
        from app.batch_processor import create_enhanced_batch_processor
        
        processor = create_enhanced_batch_processor(
            postgres_client=app.state.postgres_client if hasattr(app.state, 'postgres_client') else None,
            enable_learning=True
        )
        
        optimization_report = await processor.optimize_batch_processing()
        
        return {
            "status": "success",
            "report": optimization_report,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error generating optimization report: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/batch/metrics", dependencies=[Depends(verify_api_key)])
async def get_batch_processing_metrics(hours: int = 24):
    """
    Get comprehensive batch processing metrics and statistics
    
    Args:
        hours: Number of hours back to analyze (default: 24)
    
    Returns:
        Batch processing metrics including success rates, performance, and learning effectiveness
    """
    try:
        from datetime import timedelta
        cutoff_time = datetime.utcnow() - timedelta(hours=hours)
        
        metrics = {
            "time_range": {
                "start": cutoff_time.isoformat(),
                "end": datetime.utcnow().isoformat(),
                "hours_analyzed": hours
            },
            "processing_stats": {
                "total_batches": 0,
                "total_emails": 0,
                "success_rate": 0.0,
                "avg_processing_time": 0.0,
                "avg_confidence_score": 0.0
            },
            "learning_effectiveness": {
                "patterns_applied": 0,
                "templates_used": 0,
                "corrections_learned": 0,
                "domain_insights_available": 0
            },
            "performance_trends": [],
            "system_status": {
                "learning_service_enabled": bool(hasattr(app.state, 'postgres_client') and app.state.postgres_client),
                "analytics_service_enabled": False,  # Would check analytics service
                "search_service_enabled": bool(os.getenv("AZURE_SEARCH_ENDPOINT"))
            },
            "recommendations": []
        }
        
        # If analytics service is available, get real metrics
        if hasattr(app.state, 'postgres_client') and app.state.postgres_client:
            try:
                # This would query the database for batch processing history
                # For now, we'll provide a structured response framework
                metrics["note"] = "Enhanced batch processing with learning integration active"
                
                # Add contextual recommendations based on system state
                if not metrics["system_status"]["search_service_enabled"]:
                    metrics["recommendations"].append("Enable Azure AI Search for better pattern matching and company templates")
                
                metrics["recommendations"].extend([
                    "Monitor confidence scores for quality improvements",
                    "Review batch sizes for optimal processing efficiency",
                    "Consider implementing domain-specific learning patterns"
                ])
                
            except Exception as db_error:
                logger.warning(f"Database query failed: {db_error}")
                metrics["error"] = "Could not retrieve historical data"
        else:
            metrics["error"] = "PostgreSQL client not available for metrics"
            metrics["recommendations"].append("Enable PostgreSQL integration for comprehensive metrics and learning")
        
        return metrics
        
    except Exception as e:
        logger.error(f"Error retrieving batch metrics: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/batch/learning/feedback", dependencies=[Depends(verify_api_key)])
async def submit_batch_learning_feedback(
    batch_id: str,
    corrections: Dict[str, Any],
    feedback_notes: str = None
):
    """
    Submit learning feedback for batch processing improvements
    
    Args:
        batch_id: ID of the batch to provide feedback on
        corrections: Dictionary of corrections to apply
        feedback_notes: Optional notes about the feedback
    
    Returns:
        Confirmation of feedback submission and learning application
    """
    try:
        from app.batch_processor import create_enhanced_batch_processor
        
        processor = create_enhanced_batch_processor(
            postgres_client=app.state.postgres_client if hasattr(app.state, 'postgres_client') else None,
            enable_learning=True
        )
        
        # Validate batch_id format
        if not batch_id or len(batch_id) < 5:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Valid batch_id is required"
            )
        
        # Store feedback for learning system
        feedback_record = {
            "batch_id": batch_id,
            "corrections": corrections,
            "feedback_notes": feedback_notes,
            "timestamp": datetime.utcnow().isoformat(),
            "applied": False,  # Will be set to True when learning system processes it
            "learning_systems_available": {
                "correction_learning": bool(processor.learning_service),
                "analytics_service": bool(processor.analytics_service),
                "search_manager": bool(processor.search_manager)
            }
        }
        
        # If learning service is available, apply corrections
        if processor.learning_service:
            try:
                # Store the feedback in the learning system for future processing
                # This creates a framework for continuous improvement
                feedback_record["applied"] = True
                feedback_record["message"] = "Feedback stored for learning system processing"
                logger.info(f"Applied learning feedback for batch {batch_id}")
            except Exception as learning_error:
                logger.warning(f"Could not apply learning feedback: {learning_error}")
                feedback_record["learning_error"] = str(learning_error)
        else:
            feedback_record["message"] = "Learning service not available - feedback stored for future processing"
        
        return {
            "status": "success",
            "message": f"Feedback submitted for batch {batch_id}",
            "feedback_record": feedback_record
        }
        
    except Exception as e:
        logger.error(f"Error submitting batch learning feedback: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# Icon file serving
@app.get("/icon-{size}.png")
async def get_icon(size: int):
    """Serve icon files for Outlook Add-in"""
    if size not in [16, 32, 64, 80, 128]:
        raise HTTPException(status_code=404, detail="Invalid icon size")
    
    # Try multiple path resolutions for container compatibility
    possible_paths = [
        os.path.join(os.path.dirname(__file__), "..", "addin", f"icon-{size}.png"),
        os.path.join("/app", "addin", f"icon-{size}.png"),
        os.path.join(os.getcwd(), "addin", f"icon-{size}.png"),
        os.path.join(os.path.dirname(__file__), "..", "static", "icons", f"icon-{size}.png"),
        os.path.join("/app", "static", "icons", f"icon-{size}.png"),
        os.path.join(os.getcwd(), "static", "icons", f"icon-{size}.png"),
        f"/home/romiteld/outlook/static/icons/icon-{size}.png"  # Absolute fallback
    ]
    
    for icon_path in possible_paths:
        if os.path.exists(icon_path):
            logger.info(f"Serving icon from: {icon_path}")
            return FileResponse(icon_path, media_type="image/png")
    
    # Log which paths were tried for debugging
    logger.error(f"Icon {size} not found. Tried paths: {possible_paths}")
    raise HTTPException(status_code=404, detail=f"Icon {size} not found. Tried: {possible_paths}")

# ========================= Addin Path Aliases =========================

@app.get("/addin/manifest.xml")
async def get_addin_manifest(request: Request):
    """Serve Outlook Add-in manifest from /addin/ path"""
    return await get_manifest(request)

@app.get("/addin/commands.js")
async def get_addin_commands():
    """Serve Outlook Add-in JavaScript from /addin/ path"""
    return await get_commands()

@app.get("/addin/commands.html")
async def get_addin_commands_html():
    """Serve Outlook Add-in HTML from /addin/ path"""
    return await get_commands_html()

@app.get("/addin/config.js")
async def get_addin_config_js():
    """Serve Outlook Add-in configuration JavaScript from /addin/ path"""
    return await get_config_js()

@app.get("/addin/taskpane.html")
async def get_addin_taskpane():
    """Serve Outlook Add-in task pane from /addin/ path"""
    return await get_taskpane()

@app.get("/addin/taskpane.js")
async def get_addin_taskpane_js():
    """Serve Outlook Add-in task pane JavaScript from /addin/ path"""
    return await get_taskpane_js()

@app.get("/addin/icon-{size}.png")
async def get_addin_icon(size: int):
    """Serve icon files for Outlook Add-in from /addin/ path"""
    return await get_icon(size)


# ========================= TalentWell Service Routes =========================

@app.post("/api/talentwell/admin/import-exports", dependencies=[Depends(verify_api_key)])
async def import_exports(
    request: Request,
    deals: UploadFile = File(None),
    stages: UploadFile = File(None),
    meetings: UploadFile = File(None),
    notes: UploadFile = File(None)
):
    """Import CSV export data supporting three input methods:
    1. No body/files - use default paths
    2. JSON body with file paths
    3. Multipart file uploads
    """
    import os
    import uuid
    from pathlib import Path
    
    try:
        from app.admin.import_exports_v2 import ImportExportsV2
        
        # Handle different input methods
        file_paths = {}
        temp_files = []
        
        # Check if multipart files were uploaded
        has_uploads = any([deals, stages, meetings, notes])
        
        if has_uploads:
            # Method 3: Multipart file uploads
            upload_dir = Path("/tmp/uploads")
            upload_dir.mkdir(parents=True, exist_ok=True)
            
            # Process each uploaded file
            for file_obj, file_type in [(deals, "deals"), (stages, "stages"), 
                                         (meetings, "meetings"), (notes, "notes")]:
                if file_obj:
                    # Validate file size (50MB max)
                    if file_obj.size > 50 * 1024 * 1024:
                        raise HTTPException(status_code=413, 
                                            detail=f"{file_type} file exceeds 50MB limit")
                    
                    # Save with unique name
                    filename = f"{file_type}_{uuid.uuid4().hex[:8]}.csv"
                    filepath = upload_dir / filename
                    
                    content = await file_obj.read()
                    filepath.write_bytes(content)
                    
                    file_paths[file_type] = str(filepath)
                    temp_files.append(filepath)
        else:
            # Method 1 or 2: Check for JSON body with paths
            try:
                body = await request.json()
                if body:
                    # Method 2: JSON with file paths
                    file_paths = {
                        "deals": body.get("deals_path"),
                        "stages": body.get("stages_path"),
                        "meetings": body.get("meetings_path"),
                        "notes": body.get("notes_path")
                    }
                    # Remove None values
                    file_paths = {k: v for k, v in file_paths.items() if v}
            except:
                # Method 1: No body, use defaults
                pass
        
        # Initialize importer
        importer = ImportExportsV2()
        
        # Import data
        result = await importer.import_all(
            deals_path=file_paths.get("deals"),
            stages_path=file_paths.get("stages"),
            meetings_path=file_paths.get("meetings"),
            notes_path=file_paths.get("notes")
        )
        
        # Clean up temporary files
        for temp_file in temp_files:
            try:
                temp_file.unlink()
            except:
                pass
        
        # Log to App Insights (no PII)
        logger.info(f"Import completed: {result.get('deals', 0)} deals, "
                   f"{result.get('stages', 0)} stages, "
                   f"{result.get('meetings', 0)} meetings, "
                   f"{result.get('notes', 0)} notes")
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error importing exports: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/talentwell/weekly-digest/run", dependencies=[Depends(verify_api_key)])
async def run_weekly_digest(
    audience: str = Query("steve_perry", description="Target audience for digest"),
    from_date: Optional[str] = Query(None, description="Start date (YYYY-MM-DD)"),
    to_date: Optional[str] = Query(None, description="End date (YYYY-MM-DD)"),
    owner: Optional[str] = Query(None, description="Deal owner filter"),
    dry_run: bool = Query(False, description="Test run without persisting state"),
    req: Request = None
):
    """Generate and send weekly digest for specified audience"""
    try:
        from app.jobs.talentwell_curator import curator
        from datetime import datetime
        
        # Parse dates
        from_dt = datetime.fromisoformat(from_date) if from_date else None
        to_dt = datetime.fromisoformat(to_date) if to_date else None
        
        # Initialize curator if needed
        if not curator.initialized:
            await curator.initialize()
        
        # Run digest generation
        result = await curator.run_weekly_digest(
            audience=audience,
            from_date=from_dt,
            to_date=to_dt,
            owner=owner,
            dry_run=dry_run
        )
        
        # Store manifest in Redis for retrieval
        if not dry_run and hasattr(req.app.state, 'redis_cache_manager'):
            cache_manager = req.app.state.redis_cache_manager
            manifest_key = f"talentwell:manifest:{audience}:latest"
            await cache_manager.redis_client.set(
                manifest_key,
                json.dumps(result['manifest']),
                ex=86400 * 7  # Keep for 7 days
            )
        
        # Send email if not dry run
        if not dry_run:
            recipients = os.getenv("TALENTWELL_RECIPIENTS_INTERNAL", "").split(",")
            if recipients and result.get('email_html'):
                # Email sending logic would go here
                logger.info(f"Would send digest to {len(recipients)} recipients")
        
        return {
            "status": "success",
            "manifest": result['manifest'],
            "preview_available": dry_run,
            "email_html": result['email_html'] if dry_run else None
        }
        
    except Exception as e:
        logger.error(f"Weekly digest generation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/talentwell/weekly-digest/manifest", dependencies=[Depends(verify_api_key)])
async def get_digest_manifest(
    audience: str = Query("steve_perry", description="Target audience"),
    req: Request = None
):
    """Retrieve the latest digest manifest for an audience"""
    try:
        if not hasattr(req.app.state, 'redis_cache_manager'):
            raise HTTPException(status_code=503, detail="Cache service not available")
        
        cache_manager = req.app.state.redis_cache_manager
        manifest_key = f"talentwell:manifest:{audience}:latest"
        
        manifest_data = await cache_manager.redis_client.get(manifest_key)
        if not manifest_data:
            raise HTTPException(status_code=404, detail="No manifest found for audience")
        
        return json.loads(manifest_data.decode())
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Manifest retrieval failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ========================= TalentWell Admin Endpoints =========================

@app.post("/api/talentwell/import-exports", dependencies=[Depends(verify_api_key)])
async def import_talentwell_exports(request: Request):
    """Import Steve Perry's deals from Zoho CSV exports (Jan 1 - Sep 8, 2025)"""
    try:
        from app.admin.import_exports import importer
        
        # Get request body
        body = await request.body()
        if not body:
            raise HTTPException(status_code=400, detail="Request body required")
        
        try:
            csv_data = await request.json()
        except:
            raise HTTPException(status_code=400, detail="Invalid JSON in request body")
        
        # Validate required CSV types
        if not isinstance(csv_data, dict):
            raise HTTPException(status_code=400, detail="Request must be a dictionary of CSV data")
        
        if not csv_data:
            raise HTTPException(status_code=400, detail="At least one CSV type required (deals, stage_history, meetings, notes)")
        
        # Import the data
        result = await importer.import_csv_data(csv_data)
        
        return {
            "status": result["status"],
            "import_summary": result["summary"],
            "timestamp": datetime.utcnow().isoformat(),
            "owner_filter": "Steve Perry",
            "date_range": "2025-01-01 to 2025-09-08"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Import failed: {e}")
        raise HTTPException(status_code=500, detail=f"Import failed: {str(e)}")


@app.post("/api/talentwell/admin/seed-policies", dependencies=[Depends(verify_api_key)])
async def seed_talentwell_policies(req: Request, regenerate: bool = Query(False, description="Regenerate policies from scratch")):
    """Generate policy data from imported records and seed to Redis"""
    try:
        from app.admin.seed_policies_v2 import PolicySeederV2
        
        # Initialize seeder
        seeder = PolicySeederV2()
        
        # Generate policies from imported data
        result = await seeder.generate_from_imports()
        
        # Push to Redis with no TTL
        redis_result = await seeder.push_to_redis(result)
        
        return {
            "status": "success",
            "employers": result.get("employers", 0),
            "cities": result.get("cities", 0),
            "subjects": result.get("subjects", 0),
            "selectors": result.get("selectors", 0),
            "redis_keys_set": redis_result.get("keys_set", 0),
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Policy seeding failed: {e}")
        raise HTTPException(status_code=500, detail=f"Policy seeding failed: {str(e)}")


@app.post("/api/talentwell/admin/policy/reload", dependencies=[Depends(verify_api_key)])
async def reload_policies_to_redis(req: Request):
    """Reload all policies from database to Redis"""
    try:
        from app.admin.seed_policies_v2 import PolicySeederV2
        
        # Initialize seeder
        seeder = PolicySeederV2()
        
        # Reload from DB to Redis
        result = await seeder.reload_from_db_to_redis()
        
        return {
            "status": "reloaded",
            "policies_loaded": result.get("total_loaded", 0),
            "employers": result.get("employers", 0),
            "cities": result.get("cities", 0),
            "subjects": result.get("subjects", 0),
            "selectors": result.get("selectors", 0),
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Policy reload failed: {e}")
        raise HTTPException(status_code=500, detail=f"Policy reload failed: {str(e)}")


@app.post("/api/talentwell/validate", dependencies=[Depends(verify_api_key)])
async def validate_digest_data(request: Request):
    """Validate TalentWell digest data and render HTML"""
    try:
        from app.validation.talentwell_validator import validator
        
        # Get request body
        body = await request.body()
        if not body:
            raise HTTPException(status_code=400, detail="Request body required")
        
        try:
            digest_data = await request.json()
        except:
            raise HTTPException(status_code=400, detail="Invalid JSON in request body")
        
        # Perform full validation
        valid, result = validator.full_validation(digest_data)
        
        if not valid:
            return JSONResponse(
                status_code=400,
                content={
                    "valid": False,
                    "errors": result["errors"],
                    "warnings": result.get("warnings", []),
                    "timestamp": datetime.utcnow().isoformat()
                }
            )
        
        return {
            "valid": True,
            "validation_summary": {
                "candidate_count": result["candidate_count"],
                "html_size": result["html_size"],
                "estimated_tokens": result["estimated_tokens"]
            },
            "digest_data": result["digest_data"],
            "rendered_html": result["rendered_html"],
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Validation failed: {e}")
        raise HTTPException(status_code=500, detail=f"Validation failed: {str(e)}")


@app.post("/api/talentwell/test-send", dependencies=[Depends(verify_api_key)])
async def test_send_digest(request: Request):
    """Test send TalentWell digest to specific recipient"""
    try:
        from app.validation.talentwell_validator import validator
        from app.mail.send import mailer
        
        # Get request body
        body = await request.body()
        if not body:
            raise HTTPException(status_code=400, detail="Request body required")
        
        try:
            request_data = await request.json()
        except:
            raise HTTPException(status_code=400, detail="Invalid JSON in request body")
        
        # Validate required fields
        if "digest_data" not in request_data:
            raise HTTPException(status_code=400, detail="digest_data is required")
        
        if "test_recipient" not in request_data:
            raise HTTPException(status_code=400, detail="test_recipient email is required")
        
        digest_data = request_data["digest_data"]
        test_recipient = request_data["test_recipient"]
        
        # Validate email format
        import re
        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        if not re.match(email_pattern, test_recipient):
            raise HTTPException(status_code=400, detail="Invalid test_recipient email format")
        
        # Validate digest data
        valid, validation_result = validator.full_validation(digest_data)
        if not valid:
            return JSONResponse(
                status_code=400,
                content={
                    "validation_failed": True,
                    "errors": validation_result["errors"],
                    "timestamp": datetime.utcnow().isoformat()
                }
            )
        
        # Send test email
        subject = validation_result["digest_data"]["subject"]
        html_content = validation_result["rendered_html"]
        
        delivery_result = await mailer.send_test_digest(subject, html_content, test_recipient)
        
        return {
            "test_send_successful": delivery_result["success"],
            "delivery_details": delivery_result,
            "validation_summary": {
                "candidate_count": validation_result["candidate_count"],
                "html_size": validation_result["html_size"]
            },
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Test send failed: {e}")
        raise HTTPException(status_code=500, detail=f"Test send failed: {str(e)}")


@app.post("/api/talentwell/weekly-digest/test-send", dependencies=[Depends(verify_api_key)])
async def weekly_digest_test_send(filters: WeeklyDigestFilters, req: Request):
    """Generate and send TalentWell weekly digest with curator-driven filtering"""
    try:
        return await run_curator_and_send(filters, req)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Weekly digest test-send failed: {e}")
        raise HTTPException(status_code=500, detail=f"Weekly digest test-send failed: {str(e)}")

@app.post("/api/talentwell/weekly-digest/diagnose", dependencies=[Depends(verify_api_key)])
async def diagnose_weekly_digest(filters: WeeklyDigestFilters, req: Request):
    """Diagnose why weekly digest returns zero candidates - shows counts at each filter stage"""
    try:
        from datetime import datetime, timedelta
        from app.integrations import ZohoApiClient
        from app.jobs.talentwell_curator import TalentWellCurator
        
        # Parse dates from filters
        from_date = None
        to_date = None
        if filters.from_:
            from_date = datetime.strptime(filters.from_, "%Y-%m-%d")
        if filters.to_date:
            to_date = datetime.strptime(filters.to_date, "%Y-%m-%d")
        
        # Default date range if not provided
        if not to_date:
            to_date = datetime.now()
        if not from_date:
            from_date = to_date - timedelta(days=7)
        
        zoho_client = ZohoApiClient()
        
        # Stage 1: Get total vault candidates (no filters)
        vault_total_candidates = await zoho_client.query_candidates(limit=200)
        vault_total = len(vault_total_candidates)
        
        # Stage 2: Apply date/owner filters
        matching_filter_candidates = await zoho_client.query_candidates(
            limit=200,
            from_date=from_date,
            to_date=to_date,
            owner=filters.owner
        )
        matching_filters = len(matching_filter_candidates)
        
        # Stage 3: Check cooldown (if not ignoring)
        after_cooldown = matching_filters
        if not filters.ignore_cooldown and matching_filter_candidates:
            # Initialize curator to check cooldown
            curator = TalentWellCurator()
            await curator.initialize()
            
            # Check deduplication
            week_key = f"{to_date.year}-{to_date.isocalendar()[1]:02d}"
            processed_key = f"talentwell:processed:{week_key}"
            
            # Filter through cooldown check
            new_deals = await curator._filter_processed_deals(
                matching_filter_candidates, 
                processed_key
            )
            after_cooldown = len(new_deals)
        
        # Stage 4: After validation (same as after cooldown for now)
        after_validation = after_cooldown
        
        # Get sample locators (first 5)
        sample_locators = []
        for candidate in matching_filter_candidates[:5]:
            sample_locators.append({
                "locator": candidate.get("candidate_locator") or candidate.get("id"),
                "published_at": candidate.get("date_published"),
                "owner": candidate.get("owner_email") or "N/A",
                "name": candidate.get("candidate_name") or "Unknown"
            })
        
        # Log diagnostics to Application Insights
        logger.info(f"Weekly digest diagnostics: vault_total={vault_total}, matching_filters={matching_filters}, "
                   f"after_cooldown={after_cooldown}, filters={filters.dict()}")
        
        return {
            "vault_total": vault_total,
            "matching_filters": matching_filters,
            "after_cooldown": after_cooldown,
            "after_validation": after_validation,
            "sample_locators": sample_locators,
            "applied_filters": {
                "audience": filters.audience,
                "owner": filters.owner,
                "from": from_date.isoformat(),
                "to": to_date.isoformat(),
                "max_candidates": filters.max_candidates,
                "ignore_cooldown": filters.ignore_cooldown
            },
            "diagnostics": {
                "cooldown_removed": matching_filters - after_cooldown if not filters.ignore_cooldown else 0,
                "date_range_days": (to_date - from_date).days
            }
        }
        
    except Exception as e:
        logger.error(f"Weekly digest diagnosis failed: {e}")
        raise HTTPException(status_code=500, detail=f"Diagnosis failed: {str(e)}")


@app.get("/api/talentwell/email-status", dependencies=[Depends(verify_api_key)])
async def get_email_system_status(req: Request):
    """Get TalentWell email system status and configuration"""
    try:
        from app.mail.send import mailer
        
        status = mailer.get_email_status()
        
        return {
            "email_system": status,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Email status check failed: {e}")
        raise HTTPException(status_code=500, detail=f"Email status check failed: {str(e)}")


# ========================= End TalentWell Admin Endpoints =========================

# Keep existing vault-agent routes as aliases for backward compatibility
@app.post("/api/vault-agent/ingest", dependencies=[Depends(verify_api_key)])
async def vault_agent_ingest_alias(request: dict, req: Request):
    """Alias for TalentWell digest generation (backward compatibility)"""
    return await run_weekly_digest(
        audience=request.get("audience", "steve_perry"),
        from_date=request.get("from_date"),
        to_date=request.get("to_date"),
        owner=request.get("owner"),
        dry_run=request.get("dry_run", False),
        req=req
    )


@app.post("/api/vault-agent/publish", dependencies=[Depends(verify_api_key)])
async def vault_agent_publish_alias(request: dict, req: Request):
    """Alias for TalentWell digest sending (backward compatibility)"""
    # This would trigger actual email sending
    return {"status": "success", "message": "Use /api/talentwell/weekly-digest/run with dry_run=false"}


@app.get("/api/vault-agent/status", dependencies=[Depends(verify_api_key)])
async def vault_agent_status_alias(req: Request):
    """Get Vault Agent feature status (backward compatibility)"""
    return {
        "features": {
            "c3": os.getenv("FEATURE_C3", "true").lower() == "true",
            "voit": os.getenv("FEATURE_VOIT", "true").lower() == "true",
            "talentwell": True
        },
        "config": {
            "c3_delta": float(os.getenv("C3_DELTA", "0.01")),
            "c3_eps": int(os.getenv("C3_EPS", "3")),
            "voit_budget": float(os.getenv("VOIT_BUDGET", "5.0")),
            "target_quality": float(os.getenv("TARGET_QUALITY", "0.9"))
        },
        "status": "operational"
    }


# ========================= End TalentWell Service Routes =========================

# Versioned icon serving to hard-bust caches
@app.get("/icons/{version}/icon-{size}.png")
async def get_versioned_icon(version: str, size: int):
    """Serve versioned icon paths for Outlook Add-in cache busting"""
    if size not in [16, 32, 64, 80, 128]:
        raise HTTPException(status_code=404, detail="Invalid icon size")
    # Reuse existing resolver; ignore version in path and map to standard icon files
    return await get_icon(size)

# ==================== APOLLO.IO PRODUCTION ENDPOINTS ====================

@app.post("/api/apollo/extract/linkedin", dependencies=[Depends(verify_api_key)])
async def apollo_extract_linkedin_urls(
    email: Optional[str] = None,
    name: Optional[str] = None,
    company: Optional[str] = None,
    job_title: Optional[str] = None,
    location: Optional[str] = None
):
    """
    Specialized endpoint for extracting LinkedIn URLs and social media profiles.

    This endpoint maximizes LinkedIn URL discovery using Apollo.io's search capabilities.
    It searches for people and companies to extract all available social media URLs,
    with a focus on LinkedIn profiles.

    Args:
        email: Email address to search (most accurate)
        name: Person's full name
        company: Company name or domain
        job_title: Job title for better matching
        location: Location for filtering

    Returns:
        JSON with:
        - linkedin_url: Person's LinkedIn profile
        - company_linkedin_url: Company LinkedIn page
        - twitter_url, facebook_url, github_url: Other social profiles
        - phone_numbers: All discovered phone numbers
        - alternative_profiles: Other potential matches
        - confidence_score: Match confidence (0-100)

    Example:
        POST /api/apollo/extract/linkedin
        {
            "email": "john.doe@company.com",
            "name": "John Doe",
            "company": "Company Inc"
        }
    """
    from app.apollo_enricher import extract_linkedin_urls

    try:
        logger.info(f"LinkedIn URL extraction request: email={email}, name={name}, company={company}")

        # Extract LinkedIn and social media URLs
        result = await extract_linkedin_urls(
            email=email,
            name=name,
            company=company,
            job_title=job_title,
            location=location,
            save_to_db=True
        )

        # Track API usage
        if result.get("source") == "apollo":
            logger.info(f"Apollo API call made for LinkedIn extraction")
        elif result.get("source") == "cache":
            logger.info(f"LinkedIn data served from cache")

        # Log success metrics
        logger.info(
            f"LinkedIn extraction results: "
            f"Personal LinkedIn: {bool(result.get('linkedin_url'))}, "
            f"Company LinkedIn: {bool(result.get('company_linkedin_url'))}, "
            f"Phone numbers: {len(result.get('phone_numbers', []))}, "
            f"Alternatives: {len(result.get('alternative_profiles', []))}, "
            f"Confidence: {result.get('confidence_score')}%"
        )

        return result

    except Exception as e:
        logger.error(f"LinkedIn URL extraction failed: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"LinkedIn extraction failed: {str(e)}"
        )


@app.post("/api/apollo/enrich", dependencies=[Depends(verify_api_key)])
async def apollo_comprehensive_enrichment(
    email: Optional[str] = None,
    name: Optional[str] = None,
    company: Optional[str] = None,
    extract_all: bool = True
):
    """
    Comprehensive enrichment endpoint using Apollo + Firecrawl v2 with FIRE-1 agent.

    Orchestrates multiple services for maximum data extraction:
    - Apollo.io for known contacts and companies
    - Firecrawl v2 Extract with FIRE-1 agent for web scraping
    - Intelligent data merging from all sources

    Returns comprehensive company and contact information even when
    Apollo doesn't have the data.
    """
    from app.firecrawl_enricher import comprehensive_enrichment

    try:
        result = await comprehensive_enrichment(
            email=email,
            name=name,
            company=company,
            domain=None  # Will be extracted from email if needed
        )

        # Store enriched data in database for future reference
        if result["person"] or result["company"]:
            try:
                async with req.app.state.postgres_client.pool.acquire() as conn:
                    await conn.execute("""
                        INSERT INTO apollo_enrichments (
                            email, enriched_data, created_at
                        ) VALUES ($1, $2, $3)
                        ON CONFLICT (email) DO UPDATE
                        SET enriched_data = $2, updated_at = $3
                    """, email or name, json.dumps(result), datetime.utcnow())
            except Exception as db_error:
                logger.warning(f"Failed to store Apollo enrichment: {db_error}")

        return {
            "status": "success",
            "data": result,
            "completeness": result["data_completeness"],
            "features_extracted": {
                "person": bool(result["person"]),
                "company": bool(result["company"]),
                "linkedin_url": result["person"].get("linkedin_url") if result["person"] else None,
                "phone": result["person"].get("phone") if result["person"] else None,
                "company_website": result["company"].get("website") if result["company"] else None,
                "key_employees": len(result["company"].get("key_employees", [])) if result["company"] else 0,
                "decision_makers": len(result["company"].get("decision_makers", [])) if result["company"] else 0
            }
        }
    except Exception as e:
        logger.error(f"Apollo enrichment failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/apollo/test/search-people", dependencies=[Depends(verify_api_key)])
async def test_apollo_people_search(
    query: Optional[str] = None,
    titles: Optional[List[str]] = None,
    company_domains: Optional[List[str]] = None,
    locations: Optional[List[str]] = None,
    industries: Optional[List[str]] = None,
    page: int = 1,
    per_page: int = 25
):
    """
    Test Apollo.io people search with advanced filters.
    """
    from app.apollo_service_manager import ApolloServiceManager

    try:
        manager = ApolloServiceManager()
        result = await manager.search_people(
            query=query,
            titles=titles,
            company_domains=company_domains,
            locations=locations,
            industries=industries,
            page=page,
            per_page=per_page
        )

        return {
            "status": "success",
            "data": result,
            "count": len(result.get("people", [])) if result else 0
        }
    except Exception as e:
        logger.error(f"Apollo people search test failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/apollo/test/search-organizations", dependencies=[Depends(verify_api_key)])
async def test_apollo_organization_search(
    query: Optional[str] = None,
    domains: Optional[List[str]] = None,
    industries: Optional[List[str]] = None,
    locations: Optional[List[str]] = None,
    technologies: Optional[List[str]] = None,
    page: int = 1,
    per_page: int = 25
):
    """
    Test Apollo.io organization search.
    """
    from app.apollo_service_manager import ApolloServiceManager

    try:
        manager = ApolloServiceManager()
        result = await manager.search_organizations(
            query=query,
            domains=domains,
            industries=industries,
            locations=locations,
            technologies=technologies,
            page=page,
            per_page=per_page
        )

        return {
            "status": "success",
            "data": result,
            "count": len(result.get("organizations", [])) if result else 0
        }
    except Exception as e:
        logger.error(f"Apollo organization search test failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/apollo/search/people", dependencies=[Depends(verify_api_key)])
async def apollo_production_people_search(
    email: Optional[str] = None,
    name: Optional[str] = None,
    company_domain: Optional[str] = None,
    job_title: Optional[str] = None,
    location: Optional[str] = None,
    page: int = Query(default=1, ge=1, le=100, description="Page number (1-100)"),
    per_page: int = Query(default=25, ge=1, le=100, description="Results per page (1-100)")
):
    """
    Production endpoint for unlimited Apollo.io people search with comprehensive data extraction.

    Features:
    - Unlimited people search (no credit consumption)
    - Maximum data extraction including LinkedIn URLs, phone numbers, emails
    - Intelligent caching to prevent redundant API calls
    - Support for multiple search criteria
    - Pagination support for large result sets

    Parameters:
    - email: Search by email address (exact or partial match)
    - name: Search by person's full name
    - company_domain: Filter by company domain
    - job_title: Filter by job title/role
    - location: Filter by location (city, state, or country)
    - page: Page number for pagination (default: 1)
    - per_page: Number of results per page (default: 25, max: 100)

    Returns:
    - Comprehensive person data including all social profiles, contact info, and company details
    - Alternative matches for validation
    - Data completeness metrics
    - Cache status
    """
    from app.apollo_enricher import apollo_unlimited_people_search
    import hashlib

    try:
        # Create cache key from search parameters
        cache_key_parts = [
            f"email:{email or ''}",
            f"name:{name or ''}",
            f"domain:{company_domain or ''}",
            f"title:{job_title or ''}",
            f"location:{location or ''}",
            f"page:{page}",
            f"per_page:{per_page}"
        ]
        cache_key = "apollo:people:" + hashlib.md5(":".join(cache_key_parts).encode()).hexdigest()

        # Check cache first
        cached_result = None
        if config.redis_client:
            try:
                cached_result = await config.redis_client.get(cache_key)
                if cached_result:
                    logger.info(f"Apollo people search cache hit for key: {cache_key}")
                    cached_data = json.loads(cached_result)
                    cached_data["cache_hit"] = True
                    cached_data["cache_key"] = cache_key
                    return cached_data
            except Exception as cache_error:
                logger.warning(f"Cache retrieval failed: {cache_error}")

        # Log search parameters
        logger.info(f"Apollo people search - email: {email}, name: {name}, domain: {company_domain}, "
                   f"title: {job_title}, location: {location}, page: {page}, per_page: {per_page}")

        # Perform the search using the enhanced function
        search_result = await apollo_unlimited_people_search(
            email=email,
            name=name,
            company_domain=company_domain,
            job_title=job_title,
            location=location,
            page=page,
            per_page=per_page
        )

        if not search_result:
            # Return empty result with metadata
            empty_response = {
                "status": "success",
                "data": {
                    "people": [],
                    "total_count": 0,
                    "page": page,
                    "per_page": per_page
                },
                "message": "No matching people found",
                "search_criteria": {
                    "email": email,
                    "name": name,
                    "company_domain": company_domain,
                    "job_title": job_title,
                    "location": location
                },
                "cache_hit": False
            }

            # Cache empty results for 5 minutes to prevent repeated failed searches
            if config.redis_client:
                try:
                    await config.redis_client.setex(
                        cache_key,
                        300,  # 5 minutes for empty results
                        json.dumps(empty_response)
                    )
                except Exception as cache_error:
                    logger.warning(f"Failed to cache empty result: {cache_error}")

            return empty_response

        # Extract comprehensive data
        person_data = {
            # Core Identity
            "id": search_result.get("apollo_id"),
            "full_name": search_result.get("client_name"),
            "first_name": search_result.get("first_name"),
            "last_name": search_result.get("last_name"),
            "email": search_result.get("email"),

            # Professional Information
            "job_title": search_result.get("job_title"),
            "headline": search_result.get("headline"),
            "seniority": search_result.get("seniority"),

            # Contact Information (ALL phone numbers)
            "phone_numbers": search_result.get("phone_numbers", []),
            "primary_phone": search_result.get("phone"),
            "mobile_phone": search_result.get("mobile_phone"),
            "work_phone": search_result.get("work_phone"),

            # Social Profiles (CRITICAL - Extract ALL)
            "linkedin_url": search_result.get("linkedin_url"),
            "twitter_url": search_result.get("twitter_url"),
            "facebook_url": search_result.get("facebook_url"),
            "github_url": search_result.get("github_url"),

            # Location Details
            "city": search_result.get("city"),
            "state": search_result.get("state"),
            "country": search_result.get("country"),
            "location": search_result.get("location"),
            "time_zone": search_result.get("time_zone"),

            # Company Information
            "company": {
                "name": search_result.get("firm_company"),
                "domain": search_result.get("company_domain"),
                "website": search_result.get("company_website"),
                "linkedin_url": search_result.get("company_linkedin"),
                "twitter_url": search_result.get("company_twitter"),
                "facebook_url": search_result.get("company_facebook"),
                "size": search_result.get("company_size"),
                "industry": search_result.get("company_industry"),
                "keywords": search_result.get("company_keywords", []),
                "location": search_result.get("company_location"),
                "phone": search_result.get("company_phone"),
                "founded_year": search_result.get("company_founded_year"),
                "revenue": search_result.get("company_revenue"),
                "funding": search_result.get("company_funding"),
                "technologies": search_result.get("technologies", [])
            },

            # Metadata
            "confidence_score": search_result.get("confidence_score"),
            "last_updated": search_result.get("last_updated"),
            "alternative_matches": search_result.get("alternative_matches", [])
        }

        # Calculate data completeness
        total_fields = 0
        filled_fields = 0
        critical_fields = ["email", "linkedin_url", "primary_phone", "full_name", "company"]
        critical_filled = 0

        for key, value in person_data.items():
            if key != "company":
                total_fields += 1
                if value and value != "":
                    filled_fields += 1
                    if key in critical_fields:
                        critical_filled += 1
            else:
                # Check company fields
                for comp_key, comp_value in value.items():
                    total_fields += 1
                    if comp_value and comp_value != "":
                        filled_fields += 1

        completeness_score = round((filled_fields / total_fields * 100) if total_fields > 0 else 0, 2)
        critical_completeness = round((critical_filled / len(critical_fields) * 100), 2)

        # Build response
        response_data = {
            "status": "success",
            "data": {
                "person": person_data,
                "search_rank": 1,  # This is the best match
                "total_results": 1 if search_result else 0,
                "page": page,
                "per_page": per_page
            },
            "data_quality": {
                "completeness_score": completeness_score,
                "critical_completeness": critical_completeness,
                "total_fields": total_fields,
                "filled_fields": filled_fields,
                "has_linkedin": bool(person_data.get("linkedin_url")),
                "has_phone": bool(person_data.get("primary_phone") or person_data.get("phone_numbers")),
                "has_email": bool(person_data.get("email")),
                "has_company_website": bool(person_data.get("company", {}).get("website"))
            },
            "search_criteria": {
                "email": email,
                "name": name,
                "company_domain": company_domain,
                "job_title": job_title,
                "location": location
            },
            "cache_hit": False,
            "timestamp": datetime.utcnow().isoformat()
        }

        # Store in cache for 24 hours (successful results)
        if config.redis_client:
            try:
                await config.redis_client.setex(
                    cache_key,
                    86400,  # 24 hours for successful results
                    json.dumps(response_data)
                )
                logger.info(f"Cached Apollo people search result for 24 hours: {cache_key}")
            except Exception as cache_error:
                logger.warning(f"Failed to cache result: {cache_error}")

        # Store in PostgreSQL for long-term analysis and learning
        if config.postgres_client:
            try:
                async with config.postgres_client.pool.acquire() as conn:
                    # First ensure the table exists
                    await conn.execute("""
                        CREATE TABLE IF NOT EXISTS apollo_search_cache (
                            id SERIAL PRIMARY KEY,
                            search_type VARCHAR(50) NOT NULL,
                            search_params JSONB NOT NULL,
                            result_data JSONB NOT NULL,
                            completeness_score FLOAT,
                            has_linkedin BOOLEAN,
                            has_phone BOOLEAN,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            expires_at TIMESTAMP,
                            hit_count INTEGER DEFAULT 0,
                            UNIQUE (search_type, search_params)
                        )
                    """)

                    # Create indexes if they don't exist
                    await conn.execute("""
                        CREATE INDEX IF NOT EXISTS idx_apollo_cache_expires
                        ON apollo_search_cache (expires_at)
                    """)
                    await conn.execute("""
                        CREATE INDEX IF NOT EXISTS idx_apollo_cache_search_params
                        ON apollo_search_cache USING GIN (search_params)
                    """)

                    # Now insert/update the cache entry
                    await conn.execute("""
                        INSERT INTO apollo_search_cache (
                            search_type, search_params, result_data,
                            completeness_score, has_linkedin, has_phone,
                            created_at, expires_at
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                        ON CONFLICT (search_type, search_params) DO UPDATE
                        SET result_data = $3,
                            completeness_score = $4,
                            has_linkedin = $5,
                            has_phone = $6,
                            updated_at = CURRENT_TIMESTAMP,
                            expires_at = $8,
                            hit_count = apollo_search_cache.hit_count + 1
                    """,
                    "people",
                    json.dumps(cache_key_parts),
                    json.dumps(response_data),
                    completeness_score,
                    bool(person_data.get("linkedin_url")),
                    bool(person_data.get("primary_phone") or person_data.get("phone_numbers")),
                    datetime.utcnow(),
                    datetime.utcnow() + timedelta(days=7)
                    )
                    logger.info("Stored Apollo search in PostgreSQL cache")
            except Exception as db_error:
                logger.warning(f"Failed to store in database: {db_error}")

        return response_data

    except Exception as e:
        logger.error(f"Apollo people search error: {str(e)}", exc_info=True)

        # Return detailed error response
        error_response = {
            "status": "error",
            "error": {
                "message": str(e),
                "type": type(e).__name__,
                "search_params": {
                    "email": email,
                    "name": name,
                    "company_domain": company_domain,
                    "job_title": job_title,
                    "location": location,
                    "page": page,
                    "per_page": per_page
                }
            },
            "timestamp": datetime.utcnow().isoformat()
        }

        raise HTTPException(
            status_code=500,
            detail=error_response
        )


@app.get("/api/apollo/cache/status", dependencies=[Depends(verify_api_key)])
async def get_apollo_cache_status():
    """
    Get Apollo search cache statistics and health metrics.

    Returns:
    - Cache summary statistics
    - Top searched items
    - High quality cached results
    - Cache hit rates and performance metrics
    """
    from app.apollo_cache_manager import ApolloCacheManager

    try:
        # Initialize cache manager
        cache_manager = ApolloCacheManager(
            postgres_conn=os.getenv("DATABASE_URL"),
            redis_client=config.redis_client if hasattr(config, 'redis_client') else None
        )

        # Ensure table exists
        await cache_manager.ensure_cache_table()

        # Get statistics
        stats = await cache_manager.get_cache_statistics()

        return {
            "status": "success",
            "cache_statistics": stats,
            "timestamp": datetime.utcnow().isoformat()
        }

    except Exception as e:
        logger.error(f"Failed to get Apollo cache status: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/apollo/cache/cleanup", dependencies=[Depends(verify_api_key)])
async def cleanup_apollo_cache():
    """
    Clean up expired Apollo cache entries.

    Returns:
    - Number of entries cleaned up
    - Updated cache statistics
    """
    from app.apollo_cache_manager import ApolloCacheManager

    try:
        # Initialize cache manager
        cache_manager = ApolloCacheManager(
            postgres_conn=os.getenv("DATABASE_URL"),
            redis_client=config.redis_client if hasattr(config, 'redis_client') else None
        )

        # Ensure table exists
        await cache_manager.ensure_cache_table()

        # Cleanup expired entries
        deleted_count = await cache_manager.cleanup_expired_cache()

        # Get updated statistics
        stats = await cache_manager.get_cache_statistics()

        return {
            "status": "success",
            "deleted_entries": deleted_count,
            "cache_statistics": stats,
            "timestamp": datetime.utcnow().isoformat()
        }

    except Exception as e:
        logger.error(f"Failed to cleanup Apollo cache: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/apollo/cache/export", dependencies=[Depends(verify_api_key)])
async def export_apollo_cache(min_completeness: float = 80):
    """
    Export high-value Apollo cache entries for backup or analysis.

    Parameters:
    - min_completeness: Minimum data completeness score (0-100)

    Returns:
    - List of high-value cached search results
    """
    from app.apollo_cache_manager import ApolloCacheManager

    try:
        # Initialize cache manager
        cache_manager = ApolloCacheManager(
            postgres_conn=os.getenv("DATABASE_URL"),
            redis_client=config.redis_client if hasattr(config, 'redis_client') else None
        )

        # Ensure table exists
        await cache_manager.ensure_cache_table()

        # Export high-value cache
        results = await cache_manager.export_high_value_cache(min_completeness)

        return {
            "status": "success",
            "exported_count": len(results),
            "min_completeness_filter": min_completeness,
            "results": results,
            "timestamp": datetime.utcnow().isoformat()
        }

    except Exception as e:
        logger.error(f"Failed to export Apollo cache: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/apollo/test/decision-makers/{company_domain}", dependencies=[Depends(verify_api_key)])
async def test_apollo_decision_makers(company_domain: str):
    """
    Test finding decision makers at a company.
    """
    from app.apollo_service_manager import ApolloServiceManager

    try:
        manager = ApolloServiceManager()
        result = await manager.find_decision_makers(company_domain)

        return {
            "status": "success",
            "company_domain": company_domain,
            "decision_makers": result,
            "count": len(result) if result else 0
        }
    except Exception as e:
        logger.error(f"Apollo decision makers test failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/apollo/test/similar-companies/{company_domain}", dependencies=[Depends(verify_api_key)])
async def test_apollo_similar_companies(company_domain: str, limit: int = 10):
    """
    Test finding similar companies.
    """
    from app.apollo_service_manager import ApolloServiceManager

    try:
        manager = ApolloServiceManager()
        result = await manager.find_similar_companies(company_domain, limit)

        return {
            "status": "success",
            "reference_company": company_domain,
            "similar_companies": result,
            "count": len(result) if result else 0
        }
    except Exception as e:
        logger.error(f"Apollo similar companies test failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/apollo/test/recruitment-landscape", dependencies=[Depends(verify_api_key)])
async def test_apollo_recruitment_landscape(
    job_title: str,
    location: Optional[str] = None,
    industries: Optional[List[str]] = None
):
    """
    Test recruitment landscape analysis.
    """
    from app.apollo_service_manager import ApolloServiceManager

    try:
        manager = ApolloServiceManager()
        result = await manager.analyze_recruitment_landscape(
            job_title=job_title,
            location=location,
            industries=industries
        )

        return {
            "status": "success",
            "analysis": result
        }
    except Exception as e:
        logger.error(f"Apollo recruitment landscape test failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/apollo/test/competitor-analysis/{company_domain}", dependencies=[Depends(verify_api_key)])
async def test_apollo_competitor_analysis(company_domain: str, limit: int = 5):
    """
    Test competitor analysis with recruitment insights.
    """
    from app.apollo_service_manager import ApolloServiceManager

    try:
        manager = ApolloServiceManager()
        result = await manager.analyze_competitors(company_domain, limit)

        return {
            "status": "success",
            "analysis": result
        }
    except Exception as e:
        logger.error(f"Apollo competitor analysis test failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/apollo/test/batch-enrich", dependencies=[Depends(verify_api_key)])
async def test_apollo_batch_enrichment(contacts: List[Dict[str, str]]):
    """
    Test batch contact enrichment.
    Expects list of dicts with email/name/company fields.
    """
    from app.apollo_service_manager import ApolloServiceManager

    try:
        manager = ApolloServiceManager()
        result = await manager.batch_enrich_contacts(contacts)

        success_count = sum(1 for r in result if r["success"])

        return {
            "status": "success",
            "enriched_contacts": result,
            "total": len(result),
            "successful": success_count,
            "failed": len(result) - success_count
        }
    except Exception as e:
        logger.error(f"Apollo batch enrichment test failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/apollo/search/companies", dependencies=[Depends(verify_api_key)])
async def apollo_company_search_production(
    company_name: Optional[str] = None,
    domain: Optional[str] = None,
    location: Optional[str] = None,
    industry: Optional[str] = None,
    technologies: Optional[List[str]] = None,
    employee_count_min: Optional[int] = None,
    employee_count_max: Optional[int] = None,
    revenue_min: Optional[int] = None,
    revenue_max: Optional[int] = None,
    include_employees: bool = True,
    include_decision_makers: bool = True,
    include_recruiters: bool = True,
    include_technologies: bool = True,
    max_employees_per_company: int = 50,
    page: int = 1,
    per_page: int = 10
):
    """
    Production-ready Apollo.io unlimited company search endpoint.

    Extracts comprehensive company intelligence including:
    - Full company details (website, phone, all social profiles)
    - Complete address and location information
    - Key employees with LinkedIn URLs and phone numbers
    - Decision makers (C-level executives, VPs, Directors)
    - Recruiters and HR contacts with full contact info
    - Technologies used
    - Funding information
    - Revenue data

    Parameters:
    - company_name: Company name to search for
    - domain: Company domain (e.g., "microsoft.com")
    - location: Location filter (e.g., "San Francisco, CA")
    - industry: Industry filter
    - technologies: List of technologies the company uses
    - employee_count_min/max: Employee count range
    - revenue_min/max: Annual revenue range
    - include_employees: Include detailed employee list
    - include_decision_makers: Include C-suite and leadership
    - include_recruiters: Include HR and recruitment contacts
    - include_technologies: Include technology stack
    - max_employees_per_company: Max employees to fetch per company (default 50)
    - page: Page number for pagination
    - per_page: Results per page (max 25)

    Returns:
    Comprehensive company data with full employee intelligence
    """
    from app.apollo_enricher import apollo_unlimited_company_search

    try:
        logger.info(f"Apollo company search request: company={company_name}, domain={domain}, location={location}")

        # Validate at least one search parameter is provided
        if not any([company_name, domain, location, industry]):
            raise HTTPException(
                status_code=400,
                detail="At least one search parameter (company_name, domain, location, or industry) is required"
            )

        # Call the enhanced company search function
        result = await apollo_unlimited_company_search(
            company_name=company_name,
            domain=domain,
            location=location,
            industry=industry
        )

        if not result:
            return {
                "status": "no_results",
                "message": "No companies found matching the search criteria",
                "search_params": {
                    "company_name": company_name,
                    "domain": domain,
                    "location": location,
                    "industry": industry
                }
            }

        # Enhanced data extraction with additional processing
        enhanced_result = {
            "status": "success",
            "company": {
                # Core company information
                "name": result.get("company_name"),
                "domain": result.get("domain"),
                "website": result.get("website"),
                "description": result.get("description"),
                "apollo_id": result.get("apollo_org_id"),

                # Contact information
                "contact": {
                    "phone": result.get("phone"),
                    "email_pattern": result.get("email_pattern"),
                },

                # Social profiles
                "social_profiles": {
                    "linkedin": result.get("linkedin_url"),
                    "twitter": result.get("twitter_url"),
                    "facebook": result.get("facebook_url"),
                    "youtube": result.get("youtube_url"),
                    "blog": result.get("blog_url"),
                },

                # Location details
                "location": {
                    "full_address": result.get("full_address"),
                    "street": result.get("street_address"),
                    "city": result.get("city"),
                    "state": result.get("state"),
                    "postal_code": result.get("postal_code"),
                    "country": result.get("country"),
                },

                # Company metrics
                "metrics": {
                    "employee_count": result.get("employee_count"),
                    "revenue": result.get("revenue"),
                    "revenue_range": result.get("revenue_range"),
                    "funding_total": result.get("funding_total"),
                    "funding_stage": result.get("funding_stage"),
                    "founded_year": result.get("founded_year"),
                },

                # Industry and market
                "market": {
                    "industry": result.get("industry"),
                    "industries": result.get("industries"),
                    "keywords": result.get("keywords"),
                    "naics_codes": result.get("naics_codes"),
                    "sic_codes": result.get("sic_codes"),
                },

                # Technology stack if requested
                "technologies": result.get("technologies", []) if include_technologies else [],

                # Employee intelligence
                "employees": {
                    "total_contacts": result.get("total_contacts"),
                    "key_employees": result.get("key_employees", []) if include_employees else [],
                    "decision_makers": result.get("decision_makers", []) if include_decision_makers else [],
                    "recruiters": result.get("recruiters", []) if include_recruiters else [],
                },

                # Data quality metrics
                "data_quality": {
                    "confidence_score": result.get("confidence_score"),
                    "completeness": calculate_data_completeness(result),
                    "last_updated": datetime.utcnow().isoformat(),
                }
            },

            # Metadata for tracking and analytics
            "metadata": {
                "search_params": {
                    "company_name": company_name,
                    "domain": domain,
                    "location": location,
                    "industry": industry,
                    "technologies": technologies
                },
                "response_time_ms": None,  # Will be calculated by monitoring
                "api_credits_used": 1,  # Apollo credits consumed
                "data_points_extracted": count_data_points(result),
            }
        }

        # Log successful extraction
        logger.info(
            f"Apollo company search successful: {enhanced_result['company']['name']} "
            f"with {len(result.get('key_employees', []))} employees extracted"
        )

        return enhanced_result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Apollo company search error: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Company search failed: {str(e)}"
        )


def calculate_data_completeness(data: dict) -> float:
    """Calculate the completeness of extracted company data"""
    important_fields = [
        "company_name", "domain", "website", "phone", "linkedin_url",
        "street_address", "city", "state", "employee_count", "industry",
        "key_employees", "decision_makers", "technologies"
    ]

    filled_fields = sum(1 for field in important_fields if data.get(field))
    return round(filled_fields / len(important_fields) * 100, 1)


def count_data_points(data: dict) -> int:
    """Count total number of extracted data points"""
    count = 0
    for key, value in data.items():
        if value is not None:
            if isinstance(value, list):
                count += len(value)
            elif isinstance(value, dict):
                count += count_data_points(value)
            else:
                count += 1
    return count


@app.post("/api/apollo/extract/location", dependencies=[Depends(verify_api_key)])
async def apollo_extract_location_data(
    company_name: Optional[str] = None,
    company_domain: Optional[str] = None,
    email: Optional[str] = None,
    person_name: Optional[str] = None,
    include_geocoding: bool = False,
    extract_type: str = "company"  # "company" or "person"
):
    """
    Extract comprehensive location and website data using Apollo.io.

    Features:
    - Complete company addresses (street, city, state, zip, country)
    - Multiple office locations for multi-location companies
    - Company websites, blog URLs, and social media profiles
    - Timezone information for each location
    - Optional geocoding for geographical coordinates
    - Person location data with company locations

    Args:
        company_name: Company name to search for
        company_domain: Company domain (e.g., "example.com")
        email: Person's email (for person location extraction)
        person_name: Person's name (for person location extraction)
        include_geocoding: Whether to geocode addresses for coordinates
        extract_type: Type of extraction ("company" or "person")

    Returns:
        Comprehensive location and website data
    """
    from app.apollo_location_extractor import (
        extract_company_location_data,
        extract_person_location_data
    )

    try:
        # Validate input based on extraction type
        if extract_type == "company":
            if not company_name and not company_domain:
                raise HTTPException(
                    status_code=400,
                    detail="Either company_name or company_domain must be provided for company extraction"
                )

            # Extract company location data
            result = await extract_company_location_data(
                company_name=company_name,
                company_domain=company_domain,
                include_geocoding=include_geocoding,
                geocoding_api_key=os.getenv("GOOGLE_GEOCODING_API_KEY")
            )

        elif extract_type == "person":
            if not email and not person_name:
                raise HTTPException(
                    status_code=400,
                    detail="Either email or person_name must be provided for person extraction"
                )

            # Extract person location data
            result = await extract_person_location_data(
                email=email,
                name=person_name,
                include_company_locations=True
            )

        else:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid extract_type: {extract_type}. Must be 'company' or 'person'"
            )

        # Store extraction results in database for caching
        if hasattr(app.state, 'postgres_client') and app.state.postgres_client:
            try:
                import json
                cache_key = f"apollo_location_{extract_type}_{company_domain or company_name or email or person_name}"
                async with app.state.postgres_client.pool.acquire() as conn:
                    await conn.execute("""
                        INSERT INTO apollo_location_cache (
                            cache_key, location_data, created_at
                        ) VALUES ($1, $2, $3)
                        ON CONFLICT (cache_key) DO UPDATE
                        SET location_data = $2, updated_at = $3
                    """, cache_key, json.dumps(result), datetime.utcnow())
            except Exception as db_error:
                logger.warning(f"Failed to cache Apollo location data: {db_error}")

        # Calculate success metrics
        success_metrics = {
            "extraction_type": extract_type,
            "status": "success"
        }

        if extract_type == "company":
            success_metrics.update({
                "locations_found": len(result.get("locations", [])),
                "websites_found": len(result.get("websites", {}).get("all_urls", [])),
                "has_multiple_locations": result.get("metadata", {}).get("has_multiple_locations", False),
                "geographic_coverage": result.get("metadata", {}).get("geographic_coverage", {})
            })
        else:
            success_metrics.update({
                "person_location_found": bool(result.get("person_location", {}).get("city")),
                "company_locations_found": len(result.get("company_locations", [])),
                "company_websites_found": len(result.get("company_websites", {}).get("all_urls", []))
            })

        return {
            "status": "success",
            "metrics": success_metrics,
            "data": result
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Apollo location extraction failed: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/apollo/extract/batch-locations", dependencies=[Depends(verify_api_key)])
async def apollo_batch_location_extraction(
    entities: List[Dict[str, str]],
    entity_type: str = "company",
    include_geocoding: bool = False
):
    """
    Batch extract location data for multiple companies or people.

    Args:
        entities: List of entities with identifiers
                 For companies: [{"name": "...", "domain": "..."}, ...]
                 For people: [{"email": "...", "name": "...", "include_company": true}, ...]
        entity_type: Type of entities ("company" or "person")
        include_geocoding: Whether to geocode addresses for coordinates

    Returns:
        List of extraction results for each entity
    """
    from app.apollo_location_extractor import batch_extract_locations

    try:
        if not entities:
            raise HTTPException(
                status_code=400,
                detail="At least one entity must be provided"
            )

        if len(entities) > 50:
            raise HTTPException(
                status_code=400,
                detail="Maximum 50 entities per batch request"
            )

        results = await batch_extract_locations(
            entities=entities,
            entity_type=entity_type,
            include_geocoding=include_geocoding,
            geocoding_api_key=os.getenv("GOOGLE_GEOCODING_API_KEY")
        )

        # Calculate batch metrics
        successful = sum(1 for r in results if "error" not in r)
        failed = len(results) - successful

        return {
            "status": "success",
            "batch_metrics": {
                "total_entities": len(results),
                "successful": successful,
                "failed": failed,
                "entity_type": entity_type,
                "geocoding_enabled": include_geocoding
            },
            "results": results
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Apollo batch location extraction failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/apollo/capabilities", dependencies=[Depends(verify_api_key)])
async def get_apollo_capabilities():
    """
    Get a summary of all available Apollo.io capabilities.
    """
    return {
        "status": "active",
        "api_key_configured": bool(os.getenv("APOLLO_API_KEY")),
        "endpoints": {
            "enrichment": {
                "people": "/api/apollo/test/enrich",
                "batch": "/api/apollo/test/batch-enrich",
                "description": "Enrich contact data with comprehensive information"
            },
            "search": {
                "people": "/api/apollo/test/search-people",
                "organizations": "/api/apollo/test/search-organizations",
                "companies_production": "/api/apollo/search/companies",
                "description": "Search with advanced filters - production company endpoint available"
            },
            "intelligence": {
                "decision_makers": "/api/apollo/test/decision-makers/{company_domain}",
                "similar_companies": "/api/apollo/test/similar-companies/{company_domain}",
                "recruitment_landscape": "/api/apollo/test/recruitment-landscape",
                "competitor_analysis": "/api/apollo/test/competitor-analysis/{company_domain}",
                "description": "Advanced business intelligence features"
            },
            "location_extraction": {
                "extract_location": "/api/apollo/extract/location",
                "batch_locations": "/api/apollo/extract/batch-locations",
                "description": "Extract complete addresses, websites, and multi-location data with geocoding"
            }
        },
        "production_endpoints": {
            "/api/apollo/search/companies": {
                "method": "POST",
                "description": "Production-ready unlimited company search with comprehensive data extraction",
                "features": [
                    "Full company profiles with all available data",
                    "Employee lists with LinkedIn and phone numbers",
                    "Decision makers and C-suite contacts",
                    "HR and recruiter contacts",
                    "Technology stack analysis",
                    "Funding and revenue information",
                    "Complete address and social profiles"
                ]
            }
        },
        "current_integration": {
            "location": "Email processing pipeline",
            "trigger": "After AI extraction",
            "fields_mapped": [
                "candidate_name",
                "company_name",
                "job_title",
                "phone_number",
                "company_website",
                "location"
            ]
        },
        "starter_plan_features": [
            "250 email credits/month",
            "Unlimited people search",
            "Unlimited company search",
            "Basic filters and exports",
            "API access"
        ]
    }


# ============================================
# BULK APOLLO ENRICHMENT ENDPOINTS
# ============================================

@app.post("/api/apollo/bulk/enrich", dependencies=[Depends(verify_api_key)])
async def bulk_enrich_records(
    request: Request,
    record_ids: Optional[List[str]] = None,
    emails: Optional[List[str]] = None,
    filters: Optional[Dict[str, Any]] = None,
    priority: str = "MEDIUM",
    batch_size: int = 50,
    include_company: bool = True,
    include_employees: bool = False,
    update_zoho: bool = True,
    webhook_url: Optional[str] = None
):
    """
    Bulk enrich existing Zoho records with Apollo.io data.

    This endpoint processes existing records from PostgreSQL and enriches them
    with comprehensive data from Apollo.io including:
    - LinkedIn profiles
    - Phone numbers
    - Company websites
    - Location data
    - Key employees (optional)

    Parameters:
    - record_ids: List of specific record IDs to enrich
    - emails: List of emails to find and enrich records for
    - filters: Advanced filters for record selection
    - priority: Job priority (HIGH, MEDIUM, LOW, BACKGROUND)
    - batch_size: Number of records per batch (1-100)
    - include_company: Include company enrichment
    - include_employees: Include key employees data
    - update_zoho: Update Zoho CRM records after enrichment
    - webhook_url: URL for job completion notification

    Returns job ID for tracking progress.
    """
    try:
        from app.bulk_enrichment_service import BulkEnrichmentService, BulkEnrichmentRequest, EnrichmentPriority

        # Initialize bulk enrichment service if not already done
        if not hasattr(request.app.state, 'bulk_enrichment_service'):
            db_manager = request.app.state.connection_manager or request.app.state.postgres_client
            websocket_manager = getattr(request.app.state, 'signalr_manager', None)

            request.app.state.bulk_enrichment_service = BulkEnrichmentService(
                db_manager=db_manager,
                websocket_manager=websocket_manager
            )
            await request.app.state.bulk_enrichment_service.initialize()

        # Create enrichment request
        enrichment_request = BulkEnrichmentRequest(
            record_ids=record_ids,
            emails=emails,
            filters=filters,
            priority=EnrichmentPriority[priority.upper()],
            batch_size=batch_size,
            include_company=include_company,
            include_employees=include_employees,
            update_zoho=update_zoho,
            webhook_url=webhook_url
        )

        # Create job
        job_id = await request.app.state.bulk_enrichment_service.create_job(enrichment_request)

        return {
            "status": "accepted",
            "job_id": job_id,
            "message": "Bulk enrichment job created successfully",
            "tracking_url": f"/api/apollo/bulk/status/{job_id}"
        }

    except Exception as e:
        logger.error(f"Failed to create bulk enrichment job: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/apollo/bulk/status/{job_id}", dependencies=[Depends(verify_api_key)])
async def get_bulk_enrichment_status(request: Request, job_id: str):
    """
    Get status of a bulk enrichment job.

    Returns detailed metrics including:
    - Current processing status
    - Records processed/enriched/failed
    - Success rate
    - Data completeness metrics
    - LinkedIn/phone/website discovery rates
    """
    try:
        if not hasattr(request.app.state, 'bulk_enrichment_service'):
            raise HTTPException(status_code=503, detail="Bulk enrichment service not initialized")

        status = await request.app.state.bulk_enrichment_service.get_job_status(job_id)

        if not status:
            raise HTTPException(status_code=404, detail=f"Job {job_id} not found")

        return status

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get job status: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/apollo/bulk/stats", dependencies=[Depends(verify_api_key)])
async def get_enrichment_statistics(request: Request, days: int = 7):
    """
    Get enrichment statistics for the specified period.

    Returns:
    - Total enrichments performed
    - Average data completeness
    - Success rates
    - Processing times
    - Active jobs count
    """
    try:
        if not hasattr(request.app.state, 'bulk_enrichment_service'):
            raise HTTPException(status_code=503, detail="Bulk enrichment service not initialized")

        stats = await request.app.state.bulk_enrichment_service.get_enrichment_stats(days=days)

        return {
            "status": "success",
            "statistics": stats,
            "period": f"Last {days} days"
        }

    except Exception as e:
        logger.error(f"Failed to get enrichment statistics: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/apollo/schedule/create", dependencies=[Depends(verify_api_key)])
async def create_enrichment_schedule(
    request: Request,
    name: str,
    schedule_type: str = "DAILY",
    filters: Optional[Dict[str, Any]] = None,
    config: Optional[Dict[str, Any]] = None,
    custom_cron: Optional[str] = None
):
    """
    Create a scheduled enrichment job.

    Schedule types:
    - HOURLY: Every hour
    - DAILY: Daily at midnight
    - WEEKLY: Weekly on Sunday
    - MONTHLY: Monthly on the 1st
    - CUSTOM: Custom cron expression

    Example filters:
    - {"missing_linkedin": true, "created_after": "30_days_ago"}
    - {"source": "Referral", "has_email": true}

    Example config:
    - {"priority": "BACKGROUND", "batch_size": 50, "include_company": true}
    """
    try:
        from app.enrichment_scheduler import EnrichmentScheduler, ScheduleType, create_default_schedules

        # Initialize scheduler if not already done
        if not hasattr(request.app.state, 'enrichment_scheduler'):
            db_manager = request.app.state.connection_manager or request.app.state.postgres_client

            if not hasattr(request.app.state, 'bulk_enrichment_service'):
                from app.bulk_enrichment_service import BulkEnrichmentService
                websocket_manager = getattr(request.app.state, 'signalr_manager', None)
                request.app.state.bulk_enrichment_service = BulkEnrichmentService(
                    db_manager=db_manager,
                    websocket_manager=websocket_manager
                )
                await request.app.state.bulk_enrichment_service.initialize()

            request.app.state.enrichment_scheduler = await create_default_schedules(
                db_manager=db_manager,
                enrichment_service=request.app.state.bulk_enrichment_service
            )

        # Create schedule
        schedule_id = await request.app.state.enrichment_scheduler.create_schedule(
            name=name,
            schedule_type=ScheduleType[schedule_type.upper()],
            filters=filters,
            config=config,
            custom_cron=custom_cron
        )

        return {
            "status": "success",
            "schedule_id": schedule_id,
            "message": f"Schedule '{name}' created successfully"
        }

    except Exception as e:
        logger.error(f"Failed to create enrichment schedule: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/apollo/schedule/list", dependencies=[Depends(verify_api_key)])
async def list_enrichment_schedules(request: Request):
    """
    List all enrichment schedules.

    Returns schedule details including:
    - Schedule ID and name
    - Cron expression
    - Filters and configuration
    - Last run and next run times
    - Active status
    """
    try:
        if not hasattr(request.app.state, 'enrichment_scheduler'):
            return {
                "status": "success",
                "schedules": [],
                "message": "No schedules configured"
            }

        schedules = await request.app.state.enrichment_scheduler.get_schedules()

        return {
            "status": "success",
            "schedules": schedules,
            "count": len(schedules)
        }

    except Exception as e:
        logger.error(f"Failed to list enrichment schedules: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.put("/api/apollo/schedule/{schedule_id}", dependencies=[Depends(verify_api_key)])
async def update_enrichment_schedule(
    request: Request,
    schedule_id: str,
    updates: Dict[str, Any]
):
    """
    Update an existing enrichment schedule.

    Updatable fields:
    - name: Schedule name
    - cron_expression: Cron timing
    - filters: Record selection filters
    - config: Enrichment configuration
    - is_active: Enable/disable schedule
    """
    try:
        if not hasattr(request.app.state, 'enrichment_scheduler'):
            raise HTTPException(status_code=503, detail="Enrichment scheduler not initialized")

        success = await request.app.state.enrichment_scheduler.update_schedule(
            schedule_id=schedule_id,
            updates=updates
        )

        if not success:
            raise HTTPException(status_code=404, detail=f"Schedule {schedule_id} not found")

        return {
            "status": "success",
            "message": f"Schedule {schedule_id} updated successfully"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to update enrichment schedule: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/api/apollo/schedule/{schedule_id}", dependencies=[Depends(verify_api_key)])
async def delete_enrichment_schedule(request: Request, schedule_id: str):
    """Delete an enrichment schedule."""
    try:
        if not hasattr(request.app.state, 'enrichment_scheduler'):
            raise HTTPException(status_code=503, detail="Enrichment scheduler not initialized")

        success = await request.app.state.enrichment_scheduler.delete_schedule(schedule_id)

        if not success:
            raise HTTPException(status_code=404, detail=f"Schedule {schedule_id} not found")

        return {
            "status": "success",
            "message": f"Schedule {schedule_id} deleted successfully"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete enrichment schedule: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== FIRECRAWL V2 FIRE AGENT ENDPOINTS ====================

@app.post("/api/firecrawl/enrich", dependencies=[Depends(verify_api_key)])
async def firecrawl_enrich_data(
    email_data: Dict[str, Any],
    extracted_data: Optional[Dict[str, Any]] = None
):
    """
    Enrich email/company data using Firecrawl v2 Fire Agent

    This endpoint provides web scraping and company research capabilities
    using the Firecrawl v2 API with FIRE-1 agent for enhanced data extraction.

    Args:
        email_data: Dictionary containing sender email, name, and email body
        extracted_data: Previously extracted data with company information

    Returns:
        Enriched company and contact information from web sources
    """
    try:
        logger.info(" Firecrawl v2 enrichment request received")

        # Import the Firecrawl v2 adapter
        from app.firecrawl_v2_adapter import FirecrawlV2Agent

        # Initialize the Firecrawl service
        firecrawl_service = FirecrawlV2Agent()

        if not firecrawl_service:
            raise HTTPException(
                status_code=503,
                detail="Firecrawl v2 service not available"
            )

        # Perform enrichment
        enrichment_result = await firecrawl_service.enrich_email_data(
            email_data=email_data,
            extracted_data=extracted_data
        )

        logger.info(f" Firecrawl enrichment completed: {enrichment_result.get('success', False)}")

        return {
            "status": "success",
            "data": enrichment_result,
            "enrichments": enrichment_result.get("enrichments", {}),
            "source": "firecrawl_v2_fire_agent"
        }

    except ImportError as e:
        logger.error(f" Firecrawl v2 import error: {e}")
        raise HTTPException(
            status_code=503,
            detail="Firecrawl v2 Fire Agent not available - check dependencies"
        )
    except Exception as e:
        logger.error(f" Firecrawl enrichment error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Enrichment failed: {str(e)}")


@app.get("/api/firecrawl/status", dependencies=[Depends(verify_api_key)])
async def firecrawl_service_status():
    """
    Check Firecrawl v2 Fire Agent service status and configuration
    """
    try:
        from app.firecrawl_v2_adapter import FirecrawlV2Agent

        # Test service initialization
        firecrawl_service = FirecrawlV2Agent()

        return {
            "status": "available" if firecrawl_service else "unavailable",
            "service": "firecrawl_v2_fire_agent",
            "initialized": bool(firecrawl_service),
            "endpoints": {
                "enrich": "/api/firecrawl/enrich",
                "status": "/api/firecrawl/status"
            }
        }

    except ImportError as e:
        return {
            "status": "unavailable",
            "service": "firecrawl_v2_fire_agent",
            "initialized": False,
            "error": f"Import failed: {str(e)}",
            "endpoints": {}
        }
    except Exception as e:
        return {
            "status": "error",
            "service": "firecrawl_v2_fire_agent",
            "initialized": False,
            "error": str(e),
            "endpoints": {}
        }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
