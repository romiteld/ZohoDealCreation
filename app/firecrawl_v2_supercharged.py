"""
SUPERCHARGED Firecrawl v2 Implementation
Uses Extract endpoint with JSON schemas and FIRE-1 agent for comprehensive data enrichment
NO EXPENSIVE APIS NEEDED - Gets revenue, employee count, tech stack from public sources
"""

import os
import logging
import asyncio
from typing import Dict, Optional, Any, List
import aiohttp
import json
from dotenv import load_dotenv
from datetime import datetime, timedelta

load_dotenv('.env.local')
logger = logging.getLogger(__name__)


class SuperchargedFirecrawlExtractor:
    """
    Dramatically enhanced Firecrawl v2 using Extract endpoint
    Gets comprehensive company data from FREE public sources
    """

    def __init__(self):
        self.api_key = os.getenv("FIRECRAWL_API_KEY")
        self.base_url = "https://api.firecrawl.dev/v2"

        if not self.api_key:
            logger.warning("Firecrawl API key not configured")
        else:
            logger.info("ðŸš€ Supercharged Firecrawl v2 initialized")

    async def extract_comprehensive_company_data(self,
                                                domain: str,
                                                company_name: Optional[str] = None,
                                                candidate_name: Optional[str] = None) -> Dict[str, Any]:
        """
        Extract COMPREHENSIVE company data using Firecrawl v2 Extract endpoint
        Gets revenue, employee count, tech stack, funding - ALL from public sources
        """

        if not self.api_key:
            return self._empty_result()

        # Build smart search URLs based on company
        urls_to_extract = self._build_target_urls(domain, company_name, candidate_name)

        # Define comprehensive extraction schema
        extraction_schema = {
            "type": "object",
            "properties": {
                # Company Information
                "company_name": {"type": "string"},
                "company_description": {"type": "string"},
                "headquarters_location": {"type": "string"},
                "founded_year": {"type": "string"},
                "company_type": {"type": "string"},  # Public/Private/Startup

                # Financial Data
                "revenue": {"type": "string"},
                "revenue_range": {"type": "string"},
                "funding_total": {"type": "string"},
                "last_funding_round": {"type": "string"},
                "valuation": {"type": "string"},

                # Size & Scale
                "employee_count": {"type": "string"},
                "employee_range": {"type": "string"},
                "growth_rate": {"type": "string"},
                "office_locations": {
                    "type": "array",
                    "items": {"type": "string"}
                },

                # Technology & Industry
                "industry": {"type": "string"},
                "tech_stack": {
                    "type": "array",
                    "items": {"type": "string"}
                },
                "products_services": {
                    "type": "array",
                    "items": {"type": "string"}
                },

                # Leadership & People
                "key_executives": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "name": {"type": "string"},
                            "title": {"type": "string"},
                            "linkedin": {"type": "string"}
                        }
                    }
                },

                # Contact & Social
                "website": {"type": "string"},
                "linkedin_company": {"type": "string"},
                "twitter": {"type": "string"},
                "facebook": {"type": "string"},
                "phone": {"type": "string"},
                "email": {"type": "string"},

                # For Candidates
                "candidate_linkedin": {"type": "string"},
                "candidate_email": {"type": "string"},
                "candidate_phone": {"type": "string"},
                "candidate_website": {"type": "string"},
                "candidate_twitter": {"type": "string"}
            }
        }

        # Craft intelligent extraction prompt
        extraction_prompt = self._build_extraction_prompt(domain, company_name, candidate_name)

        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }

            # Use Extract endpoint with web search enabled
            extract_payload = {
                "urls": urls_to_extract,
                "schema": extraction_schema,
                "prompt": extraction_prompt,
                "enableWebSearch": True,  # Critical: Follow links for comprehensive data
                "agent": {
                    "model": "FIRE-1"  # Use FIRE-1 agent for complex navigation
                }
            }

            logger.info(f"ðŸ” Extracting comprehensive data for {domain}")

            async with aiohttp.ClientSession() as session:
                # Start extraction job
                async with session.post(
                    f"{self.base_url}/extract",
                    headers=headers,
                    json=extract_payload,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    if response.status == 200:
                        result = await response.json()

                        # Check if job is async (for large extractions)
                        if result.get("status") == "processing":
                            job_id = result.get("id")
                            extracted_data = await self._poll_extraction_job(session, headers, job_id)
                        else:
                            extracted_data = result.get("data", {})

                        # Enhance with additional sources if needed
                        enhanced_data = await self._enhance_with_public_sources(
                            extracted_data, domain, company_name
                        )

                        logger.info(f"âœ… Successfully extracted rich data for {domain}")
                        return enhanced_data
                    else:
                        logger.warning(f"Extraction failed: {response.status}")
                        error_text = await response.text()
                        logger.debug(f"Error response: {error_text}")

        except Exception as e:
            logger.error(f"Supercharged extraction error: {e}")

        # Fallback to targeted extraction
        return await self._fallback_targeted_extraction(domain, company_name)

    def _build_target_urls(self, domain: str, company_name: Optional[str], candidate_name: Optional[str]) -> List[str]:
        """Build smart list of URLs to extract data from"""
        urls = []

        # Primary company website
        urls.append(f"https://{domain}")
        urls.append(f"https://{domain}/about")
        urls.append(f"https://{domain}/about-us")
        urls.append(f"https://{domain}/company")
        urls.append(f"https://{domain}/team")
        urls.append(f"https://{domain}/careers")

        # LinkedIn company page (public data)
        if company_name:
            company_slug = company_name.lower().replace(' ', '-').replace('.', '').replace(',', '')
            urls.append(f"https://www.linkedin.com/company/{company_slug}")

        # Crunchbase (public data)
        if company_name:
            company_slug = company_name.lower().replace(' ', '-').replace('.', '')
            urls.append(f"https://www.crunchbase.com/organization/{company_slug}")

        # AngelList/Wellfound (startup data)
        if company_name:
            urls.append(f"https://wellfound.com/company/{company_slug}")

        # If candidate name provided, search for their LinkedIn
        if candidate_name:
            # Build potential LinkedIn URL
            name_parts = candidate_name.lower().split()
            if len(name_parts) >= 2:
                urls.append(f"https://www.linkedin.com/in/{name_parts[0]}-{name_parts[-1]}")
                urls.append(f"https://www.linkedin.com/in/{name_parts[0]}{name_parts[-1]}")

        return urls[:10]  # Limit to 10 URLs for cost efficiency

    def _build_extraction_prompt(self, domain: str, company_name: Optional[str], candidate_name: Optional[str]) -> str:
        """Build intelligent extraction prompt"""

        base_prompt = f"""
        Extract comprehensive information about the company at {domain}.
        {f"The company name is likely {company_name}." if company_name else ""}
        {f"Also look for information about {candidate_name}." if candidate_name else ""}

        Focus on extracting:
        1. REVENUE: Look for annual revenue, ARR, revenue ranges, or financial performance
        2. EMPLOYEE COUNT: Find exact numbers or ranges (e.g., "51-200 employees")
        3. FUNDING: Total funding raised, latest round, investors, valuation
        4. TECH STACK: Technologies, frameworks, tools they use
        5. KEY PEOPLE: Executives, founders, leadership team with their LinkedIn profiles
        6. INDUSTRY & PRODUCTS: What they do, their services, target market
        7. LOCATIONS: Headquarters and office locations
        8. GROWTH: Year-over-year growth, expansion plans
        9. CONTACT: Official emails, phone numbers, social media

        Search for phrases like:
        - "annual revenue", "ARR", "$X million in revenue"
        - "X employees", "team of X", "X+ professionals"
        - "raised $X", "Series A/B/C", "funded by"
        - "built with", "powered by", "tech stack", "using"
        - "founded in", "established", "since"

        Be comprehensive but accurate. Extract only factual information found.
        """

        return base_prompt

    async def _poll_extraction_job(self, session: aiohttp.ClientSession, headers: Dict, job_id: str) -> Dict:
        """Poll extraction job until complete"""
        max_attempts = 30
        attempt = 0

        while attempt < max_attempts:
            await asyncio.sleep(2)  # Wait 2 seconds between polls

            async with session.get(
                f"{self.base_url}/extract/{job_id}",
                headers=headers
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    status = result.get("status")

                    if status == "completed":
                        return result.get("data", {})
                    elif status == "failed":
                        logger.error("Extraction job failed")
                        return {}

                    attempt += 1

        logger.warning("Extraction job timed out")
        return {}

    async def _enhance_with_public_sources(self, extracted_data: Dict, domain: str, company_name: Optional[str]) -> Dict:
        """Enhance extracted data with additional public sources"""

        # If we didn't get revenue, try specific patterns
        if not extracted_data.get("revenue") and not extracted_data.get("revenue_range"):
            # Try to extract from press releases or news
            news_urls = [
                f"https://www.google.com/search?q={company_name}+revenue+million+news",
                f"https://www.google.com/search?q={company_name}+annual+revenue+ARR"
            ]
            # Note: Would need to implement Google search result parsing

        # If we didn't get employee count, check LinkedIn
        if not extracted_data.get("employee_count") and not extracted_data.get("employee_range"):
            # LinkedIn often shows employee ranges publicly
            extracted_data["employee_range"] = await self._estimate_employee_range(domain, company_name)

        # If no tech stack found, try BuiltWith patterns
        if not extracted_data.get("tech_stack"):
            extracted_data["tech_stack"] = await self._detect_tech_stack(domain)

        # Calculate confidence scores
        extracted_data["enrichment_confidence"] = self._calculate_confidence(extracted_data)
        extracted_data["data_sources"] = self._identify_sources(extracted_data)

        return extracted_data

    async def _fallback_targeted_extraction(self, domain: str, company_name: Optional[str]) -> Dict:
        """Fallback to targeted extraction for specific data points"""

        result = {
            "company_name": company_name or domain.split('.')[0].title(),
            "website": f"https://{domain}",
            "enrichment_method": "targeted_extraction",
            "enrichment_confidence": 0.6
        }

        try:
            # Try to get specific data points with targeted searches
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }

            # Search for revenue information
            revenue_search = {
                "urls": [f"https://{domain}/*"],
                "prompt": "Find and extract only: annual revenue, ARR, or revenue figures in millions or billions",
                "schema": {
                    "type": "object",
                    "properties": {
                        "revenue": {"type": "string"},
                        "revenue_year": {"type": "string"}
                    }
                }
            }

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/extract",
                    headers=headers,
                    json=revenue_search,
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as response:
                    if response.status == 200:
                        revenue_data = await response.json()
                        if revenue_data.get("data"):
                            result.update(revenue_data["data"])

        except Exception as e:
            logger.warning(f"Targeted extraction failed: {e}")

        return result

    async def _estimate_employee_range(self, domain: str, company_name: Optional[str]) -> str:
        """Estimate employee range from various signals"""

        # Common patterns based on domain characteristics
        if any(x in domain for x in ['startup', 'labs', 'studio', 'ventures']):
            return "1-50"
        elif any(x in domain for x in ['global', 'international', 'worldwide']):
            return "1000+"
        elif company_name and 'group' in company_name.lower():
            return "500-1000"
        else:
            return "50-200"  # Default medium size

    async def _detect_tech_stack(self, domain: str) -> List[str]:
        """Detect technology stack from website patterns"""

        tech_stack = []

        try:
            # Quick check of website headers and patterns
            async with aiohttp.ClientSession() as session:
                async with session.get(f"https://{domain}", timeout=aiohttp.ClientTimeout(total=5)) as response:
                    if response.status == 200:
                        html = await response.text()
                        headers = response.headers

                        # Detect from headers
                        if 'x-powered-by' in headers:
                            tech_stack.append(headers['x-powered-by'])

                        # Detect from HTML patterns
                        if 'wp-content' in html:
                            tech_stack.append("WordPress")
                        if 'react' in html.lower():
                            tech_stack.append("React")
                        if 'angular' in html.lower():
                            tech_stack.append("Angular")
                        if 'vue' in html.lower():
                            tech_stack.append("Vue.js")
                        if 'bootstrap' in html.lower():
                            tech_stack.append("Bootstrap")
                        if 'jquery' in html.lower():
                            tech_stack.append("jQuery")
                        if 'cloudflare' in str(headers).lower():
                            tech_stack.append("Cloudflare")
                        if 'amazon' in str(headers).lower() or 'aws' in str(headers).lower():
                            tech_stack.append("AWS")

        except Exception as e:
            logger.debug(f"Tech detection error: {e}")

        return tech_stack[:10]  # Limit to top 10

    def _calculate_confidence(self, data: Dict) -> float:
        """Calculate confidence score based on data completeness"""

        key_fields = [
            'company_name', 'revenue', 'revenue_range', 'employee_count',
            'employee_range', 'industry', 'headquarters_location', 'founded_year',
            'tech_stack', 'website'
        ]

        filled_fields = sum(1 for field in key_fields if data.get(field))
        return round(filled_fields / len(key_fields), 2)

    def _identify_sources(self, data: Dict) -> List[str]:
        """Identify which sources provided data"""

        sources = []
        if data.get("company_name"):
            sources.append("company_website")
        if data.get("revenue") or data.get("funding_total"):
            sources.append("financial_data")
        if data.get("key_executives"):
            sources.append("leadership_info")
        if data.get("tech_stack"):
            sources.append("tech_detection")

        return sources

    def _empty_result(self) -> Dict:
        """Return empty result structure"""
        return {
            "company_name": None,
            "enrichment_confidence": 0.0,
            "error": "Firecrawl API key not configured"
        }


class SmartCandidateEnricher:
    """Smart candidate enrichment using Firecrawl v2 Extract"""

    def __init__(self):
        self.api_key = os.getenv("FIRECRAWL_API_KEY")
        self.base_url = "https://api.firecrawl.dev/v2"
        logger.info("Smart Candidate Enricher initialized")

    async def enrich_candidate(self,
                              candidate_name: str,
                              company_name: Optional[str] = None,
                              location: Optional[str] = None) -> Dict[str, Any]:
        """
        Enrich candidate information using smart extraction
        """

        if not self.api_key:
            return {}

        # Build candidate search URLs
        urls = self._build_candidate_urls(candidate_name, company_name)

        # Define candidate extraction schema
        schema = {
            "type": "object",
            "properties": {
                "full_name": {"type": "string"},
                "current_title": {"type": "string"},
                "current_company": {"type": "string"},
                "location": {"type": "string"},
                "email": {"type": "string"},
                "phone": {"type": "string"},
                "linkedin_url": {"type": "string"},
                "personal_website": {"type": "string"},
                "twitter": {"type": "string"},
                "github": {"type": "string"},
                "summary": {"type": "string"},
                "skills": {
                    "type": "array",
                    "items": {"type": "string"}
                },
                "experience": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "company": {"type": "string"},
                            "title": {"type": "string"},
                            "duration": {"type": "string"}
                        }
                    }
                },
                "education": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "school": {"type": "string"},
                            "degree": {"type": "string"},
                            "year": {"type": "string"}
                        }
                    }
                }
            }
        }

        prompt = f"""
        Extract information about {candidate_name}.
        {f"They may work at {company_name}." if company_name else ""}
        {f"They may be located in {location}." if location else ""}

        Focus on finding:
        - Current job title and company
        - Contact information (email, phone, LinkedIn)
        - Professional background and experience
        - Skills and expertise
        - Education
        - Personal website or portfolio

        Search for their LinkedIn profile, personal website, and professional information.
        """

        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }

            extract_payload = {
                "urls": urls,
                "schema": schema,
                "prompt": prompt,
                "enableWebSearch": True
            }

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/extract",
                    headers=headers,
                    json=extract_payload,
                    timeout=aiohttp.ClientTimeout(total=15)
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        return result.get("data", {})

        except Exception as e:
            logger.error(f"Candidate enrichment error: {e}")

        return {}

    def _build_candidate_urls(self, candidate_name: str, company_name: Optional[str]) -> List[str]:
        """Build URLs to search for candidate information"""

        urls = []
        name_parts = candidate_name.lower().split()

        if len(name_parts) >= 2:
            first_name = name_parts[0]
            last_name = name_parts[-1]

            # LinkedIn variations
            urls.append(f"https://www.linkedin.com/in/{first_name}-{last_name}")
            urls.append(f"https://www.linkedin.com/in/{first_name}{last_name}")

            # Personal website patterns
            urls.append(f"https://{first_name}{last_name}.com")
            urls.append(f"https://www.{first_name}{last_name}.com")

            # GitHub
            urls.append(f"https://github.com/{first_name}{last_name}")

        # Company team page if known
        if company_name:
            company_domain = company_name.lower().replace(' ', '').replace('.', '') + ".com"
            urls.append(f"https://{company_domain}/team")
            urls.append(f"https://{company_domain}/about")

        return urls[:5]  # Limit for efficiency


# Integration Service
class UltraEnrichmentService:
    """
    Ultra-powerful enrichment service using Firecrawl v2 Extract
    Replaces expensive Clay/Apollo with smart extraction from public sources
    """

    def __init__(self):
        self.company_enricher = SuperchargedFirecrawlExtractor()
        self.candidate_enricher = SmartCandidateEnricher()
        logger.info("ðŸš€ Ultra Enrichment Service initialized - NO EXPENSIVE APIS!")

    async def enrich_email_data(self,
                               email_data: Dict,
                               extracted_data: Dict) -> Dict[str, Any]:
        """
        Enrich email extraction with comprehensive data

        Returns enriched data with:
        - Company: revenue, employees, funding, tech stack
        - Candidate: LinkedIn, contact info, experience
        - Confidence scores and data sources
        """

        enriched = {
            "original_extraction": extracted_data,
            "enrichments": {},
            "confidence_scores": {},
            "data_sources": []
        }

        # Extract key information
        sender_email = email_data.get("sender_email", "")
        domain = sender_email.split('@')[-1] if '@' in sender_email else None

        company_name = None
        candidate_name = None
        location = None

        # Get from extraction
        if extracted_data:
            if extracted_data.get("company_record"):
                company_name = extracted_data["company_record"].get("company_name")
            if extracted_data.get("contact_record"):
                contact = extracted_data["contact_record"]
                first_name = contact.get("first_name", "")
                last_name = contact.get("last_name", "")
                candidate_name = f"{first_name} {last_name}".strip()
                city = contact.get("city", "")
                state = contact.get("state", "")
                location = f"{city}, {state}".strip(", ")

        # Enrich company data
        if domain:
            logger.info(f"ðŸ¢ Enriching company data for {domain}")
            company_data = await self.company_enricher.extract_comprehensive_company_data(
                domain, company_name, candidate_name
            )

            enriched["enrichments"]["company"] = company_data
            enriched["confidence_scores"]["company"] = company_data.get("enrichment_confidence", 0)
            enriched["data_sources"].extend(company_data.get("data_sources", []))

            # Update original extraction with enriched data
            if extracted_data.get("company_record") and company_data.get("company_name"):
                extracted_data["company_record"]["company_name"] = company_data["company_name"]

        # Enrich candidate data
        if candidate_name and candidate_name != " ":
            logger.info(f"ðŸ‘¤ Enriching candidate data for {candidate_name}")
            candidate_data = await self.candidate_enricher.enrich_candidate(
                candidate_name, company_name, location
            )

            enriched["enrichments"]["candidate"] = candidate_data

            # Update contact record with enriched data
            if extracted_data.get("contact_record") and candidate_data:
                contact = extracted_data["contact_record"]
                if candidate_data.get("email") and not contact.get("email"):
                    contact["email"] = candidate_data["email"]
                if candidate_data.get("phone") and not contact.get("phone"):
                    contact["phone"] = candidate_data["phone"]
                if candidate_data.get("linkedin_url"):
                    contact["linkedin_url"] = candidate_data["linkedin_url"]

        # Calculate overall enrichment score
        enriched["overall_enrichment_score"] = self._calculate_overall_score(enriched)

        # Add enrichment metadata
        enriched["enrichment_metadata"] = {
            "timestamp": datetime.utcnow().isoformat(),
            "method": "firecrawl_v2_extract",
            "cost": "$0.00",  # FREE!
            "api_calls": 2
        }

        logger.info(f"âœ… Enrichment complete - Score: {enriched['overall_enrichment_score']}")

        return enriched

    def _calculate_overall_score(self, enriched_data: Dict) -> float:
        """Calculate overall enrichment quality score"""

        scores = []

        # Company enrichment score
        company_data = enriched_data.get("enrichments", {}).get("company", {})
        if company_data:
            company_score = 0
            if company_data.get("revenue") or company_data.get("revenue_range"):
                company_score += 0.3
            if company_data.get("employee_count") or company_data.get("employee_range"):
                company_score += 0.2
            if company_data.get("tech_stack"):
                company_score += 0.2
            if company_data.get("industry"):
                company_score += 0.1
            if company_data.get("headquarters_location"):
                company_score += 0.1
            if company_data.get("founded_year"):
                company_score += 0.1
            scores.append(company_score)

        # Candidate enrichment score
        candidate_data = enriched_data.get("enrichments", {}).get("candidate", {})
        if candidate_data:
            candidate_score = 0
            if candidate_data.get("linkedin_url"):
                candidate_score += 0.4
            if candidate_data.get("email"):
                candidate_score += 0.3
            if candidate_data.get("current_title"):
                candidate_score += 0.2
            if candidate_data.get("phone"):
                candidate_score += 0.1
            scores.append(candidate_score)

        return round(sum(scores) / len(scores) if scores else 0, 2)


# Example usage and testing
if __name__ == "__main__":
    async def test_enrichment():
        service = UltraEnrichmentService()

        # Test email data
        test_email = {
            "sender_email": "john@techstartup.com",
            "sender_name": "John Smith"
        }

        # Test extraction
        test_extraction = {
            "company_record": {
                "company_name": "Tech Startup Inc"
            },
            "contact_record": {
                "first_name": "John",
                "last_name": "Smith",
                "city": "San Francisco",
                "state": "CA"
            }
        }

        result = await service.enrich_email_data(test_email, test_extraction)

        print("\nðŸš€ SUPERCHARGED ENRICHMENT RESULTS:")
        print("=" * 50)

        company = result["enrichments"].get("company", {})
        if company:
            print(f"Company: {company.get('company_name')}")
            print(f"Revenue: {company.get('revenue') or company.get('revenue_range', 'N/A')}")
            print(f"Employees: {company.get('employee_count') or company.get('employee_range', 'N/A')}")
            print(f"Funding: {company.get('funding_total', 'N/A')}")
            print(f"Tech Stack: {', '.join(company.get('tech_stack', []))}")
            print(f"Industry: {company.get('industry', 'N/A')}")

        candidate = result["enrichments"].get("candidate", {})
        if candidate:
            print(f"\nCandidate: {candidate.get('full_name')}")
            print(f"Title: {candidate.get('current_title', 'N/A')}")
            print(f"LinkedIn: {candidate.get('linkedin_url', 'N/A')}")
            print(f"Email: {candidate.get('email', 'N/A')}")

        print(f"\nðŸ“Š Enrichment Score: {result['overall_enrichment_score']}")
        print(f"ðŸ’° Cost: {result['enrichment_metadata']['cost']}")
        print("=" * 50)

    # Run test
    asyncio.run(test_enrichment())