# Phase 2 Completion Summary: Worker Implementation & KEDA Configuration

**Date:** 2025-01-15
**Status:** âœ… COMPLETE
**Duration:** Continuous from Phase 1

---

## ğŸ¯ Phase 2 Objectives

Build on Phase 1 Service Bus infrastructure by implementing:
1. Service Bus queue consumers (digest & NLP workers)
2. KEDA autoscaling configuration for Container Apps
3. Docker images and deployment automation
4. Integration with ProactiveMessagingService

---

## âœ… Deliverables Completed

### 1. Digest Worker Implementation
**File:** [`teams_bot/app/workers/digest_worker.py`](teams_bot/app/workers/digest_worker.py)

**Features:**
- âœ… Subscribes to `teams-digest-requests` queue via async Service Bus client
- âœ… Processes up to 5 concurrent messages
- âœ… Integrates with `TalentWellCurator` for digest generation
- âœ… Uses `ProactiveMessagingService` to send results to Teams users
- âœ… Database tracking (teams_digest_requests table)
- âœ… Graceful shutdown on SIGTERM/SIGINT (Container Apps compatibility)
- âœ… Automatic retries via Service Bus (3 attempts, then dead letter)
- âœ… Error handling with fallback notifications

**Key Code Pattern:**
```python
async def process_message(self, message) -> bool:
    # 1. Parse Service Bus message â†’ DigestRequestMessage
    # 2. Create teams_digest_requests record (status='processing')
    # 3. Generate digest via TalentWellCurator.run_weekly_digest()
    # 4. Update database with results (cards_metadata, execution_time_ms)
    # 5. Send adaptive card via ProactiveMessagingService
    # 6. Mark database as delivered_at
    return True  # Complete message, remove from queue
```

**Concurrency:** 5 messages (5min lock duration = 25min total processing capacity per replica)

---

### 2. NLP Worker Implementation
**File:** [`teams_bot/app/workers/nlp_worker.py`](teams_bot/app/workers/nlp_worker.py)

**Features:**
- âœ… Subscribes to `teams-nlp-queries` queue
- âœ… Processes up to 10 concurrent messages (lighter workload than digest)
- âœ… Integrates with `process_natural_language_query()` from QueryEngine
- âœ… Proactive messaging for query results
- âœ… Database tracking (teams_conversations table)
- âœ… 2min message lock (faster timeout than digest)
- âœ… Supports both text and adaptive card responses

**Key Differences from Digest Worker:**
| Aspect | Digest Worker | NLP Worker |
|--------|---------------|------------|
| Queue | teams-digest-requests | teams-nlp-queries |
| Concurrency | 5 messages | 10 messages |
| Lock Duration | 5 minutes | 2 minutes |
| Processing Time | 10-30 seconds | 1-5 seconds |
| KEDA Scaling | 1 replica per 5 msgs | 1 replica per 10 msgs |

---

### 3. Docker Images
**Files:**
- [`teams_bot/Dockerfile.digest-worker`](teams_bot/Dockerfile.digest-worker)
- [`teams_bot/Dockerfile.nlp-worker`](teams_bot/Dockerfile.nlp-worker)

**Build Commands:**
```bash
# Digest worker
docker build -t wellintakeacr0903.azurecr.io/teams-digest-worker:latest \
  -f teams_bot/Dockerfile.digest-worker .

# NLP worker
docker build -t wellintakeacr0903.azurecr.io/teams-nlp-worker:latest \
  -f teams_bot/Dockerfile.nlp-worker .
```

**Image Specifications:**
- Base: `python:3.11-slim`
- Dependencies: `requirements.txt` + `well_shared` library
- Entrypoint: `python -m teams_bot.app.workers.<worker_name>`
- Size: ~450MB (optimized for Container Apps)

---

### 4. KEDA Autoscaling Configuration
**File:** [`KEDA_CONFIGURATION.md`](KEDA_CONFIGURATION.md)

**Comprehensive Documentation Includes:**
- âœ… Architecture diagrams (Teams â†’ Service Bus â†’ KEDA â†’ Workers â†’ ProactiveMessaging)
- âœ… Azure CLI commands for Container Apps creation with KEDA rules
- âœ… Scaling behavior explanation (scale-to-zero, scale-out thresholds)
- âœ… Environment variables reference
- âœ… Deployment workflow
- âœ… Monitoring commands (queue depth, replica count, logs)
- âœ… Cost analysis (before/after comparison)
- âœ… Troubleshooting guide
- âœ… Testing procedures

**KEDA Scaling Rules:**

**Digest Worker:**
```bash
az containerapp create \
  --name teams-digest-worker \
  --min-replicas 0 \
  --max-replicas 10 \
  --scale-rule-type azure-servicebus \
  --scale-rule-metadata "queueName=teams-digest-requests" \
                        "namespace=wellintakebus-standard" \
                        "messageCount=5"  # 1 replica per 5 messages
```

**Scaling Behavior:**
- 0 messages â†’ 0 replicas (scale-to-zero, $0 cost)
- 1-5 messages â†’ 1 replica
- 6-10 messages â†’ 2 replicas
- 46-50 messages â†’ 10 replicas (max)
- Cooldown: 300 seconds before scaling down

**NLP Worker:**
```bash
az containerapp create \
  --name teams-nlp-worker \
  --min-replicas 0 \
  --max-replicas 20 \
  --scale-rule-type azure-servicebus \
  --scale-rule-metadata "queueName=teams-nlp-queries" \
                        "namespace=wellintakebus-standard" \
                        "messageCount=10"  # 1 replica per 10 messages
```

**Scaling Behavior:**
- 0 messages â†’ 0 replicas
- 1-10 messages â†’ 1 replica
- 11-20 messages â†’ 2 replicas
- 191-200 messages â†’ 20 replicas (max)
- Cooldown: 120 seconds

---

### 5. Deployment Automation
**File:** [`scripts/deploy-workers.sh`](scripts/deploy-workers.sh)

**Features:**
- âœ… Builds both Docker images in parallel
- âœ… Tags with timestamp and `latest`
- âœ… Pushes to Azure Container Registry
- âœ… Updates Container Apps with new revisions
- âœ… Shows post-deployment replica status
- âœ… Provides monitoring commands

**Usage:**
```bash
# Deploy both workers
./scripts/deploy-workers.sh all

# Deploy only digest worker
./scripts/deploy-workers.sh digest

# Deploy only NLP worker
./scripts/deploy-workers.sh nlp
```

**Output:**
- âœ… Image URIs with timestamps
- âœ… Container App update status
- âœ… Replica count verification
- âœ… Monitoring command references

---

## ğŸ“Š Architecture Flow (Complete)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Teams Webhook (Port 8001)                     â”‚
â”‚              FastAPI Routes - HTTP Handler                    â”‚
â”‚                                                               â”‚
â”‚  /api/teams/webhook â†’ receives Teams activities              â”‚
â”‚                                                               â”‚
â”‚  Synchronous Response (<150ms):                              â”‚
â”‚  â€¢ Parse command                                             â”‚
â”‚  â€¢ Validate user                                             â”‚
â”‚  â€¢ MessageBusService.publish_digest_request()                â”‚
â”‚  â€¢ Return "â³ Generating digest..." acknowledgment            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼ publish_digest_request()
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Azure Service Bus (Standard Tier)                â”‚
â”‚                                                               â”‚
â”‚  Queue: teams-digest-requests                                â”‚
â”‚  â€¢ Active Messages: 0-50                                     â”‚
â”‚  â€¢ Lock Duration: 5 minutes                                  â”‚
â”‚  â€¢ Max Delivery: 3 attempts                                  â”‚
â”‚  â€¢ Dead Letter: Enabled                                      â”‚
â”‚                                                               â”‚
â”‚  Queue: teams-nlp-queries                                    â”‚
â”‚  â€¢ Active Messages: 0-200                                    â”‚
â”‚  â€¢ Lock Duration: 2 minutes                                  â”‚
â”‚  â€¢ Max Delivery: 2 attempts                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼ KEDA Scaler monitors queue depth
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  KEDA Autoscaling Engine                      â”‚
â”‚              (Built into Azure Container Apps)                â”‚
â”‚                                                               â”‚
â”‚  Digest Worker Scaler:                                       â”‚
â”‚  â€¢ Poll Interval: 30 seconds                                 â”‚
â”‚  â€¢ Scale Rule: queueLength / 5 = desired replicas            â”‚
â”‚  â€¢ Scale Up: Immediate (0s stabilization)                    â”‚
â”‚  â€¢ Scale Down: 300s cooldown                                 â”‚
â”‚                                                               â”‚
â”‚  NLP Worker Scaler:                                          â”‚
â”‚  â€¢ Poll Interval: 30 seconds                                 â”‚
â”‚  â€¢ Scale Rule: queueLength / 10 = desired replicas           â”‚
â”‚  â€¢ Scale Up: Immediate                                       â”‚
â”‚  â€¢ Scale Down: 120s cooldown                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼ Scales Container Apps
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Worker Container Apps (0-30 replicas)               â”‚
â”‚                                                               â”‚
â”‚  Digest Worker (0-10 replicas):                              â”‚
â”‚  â€¢ CPU: 1.0 core, Memory: 2Gi                                â”‚
â”‚  â€¢ Receives 5 concurrent messages                            â”‚
â”‚  â€¢ Processes via TalentWellCurator.run_weekly_digest()       â”‚
â”‚  â€¢ Avg processing time: 15-30 seconds per message            â”‚
â”‚                                                               â”‚
â”‚  NLP Worker (0-20 replicas):                                 â”‚
â”‚  â€¢ CPU: 0.5 core, Memory: 1Gi                                â”‚
â”‚  â€¢ Receives 10 concurrent messages                           â”‚
â”‚  â€¢ Processes via QueryEngine.process_natural_language_query()â”‚
â”‚  â€¢ Avg processing time: 1-5 seconds per message              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼ send_card_to_conversation()
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            ProactiveMessagingService                          â”‚
â”‚         (Bot Framework Adapter with MicrosoftAppCredentials) â”‚
â”‚                                                               â”‚
â”‚  Features:                                                   â”‚
â”‚  â€¢ Retrieves conversation_reference from PostgreSQL          â”‚
â”‚  â€¢ Constructs ConversationReference with service_url         â”‚
â”‚  â€¢ Uses adapter.continue_conversation() for proactive send   â”‚
â”‚  â€¢ Supports both text messages and adaptive cards            â”‚
â”‚  â€¢ Retry logic: 3 attempts with exponential backoff          â”‚
â”‚  â€¢ Correlation ID tracking for debugging                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼ Result delivered
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Teams User                               â”‚
â”‚                                                               â”‚
â”‚  Receives Adaptive Card:                                     â”‚
â”‚  â€¢ Digest preview (cards_metadata, audience, request_id)     â”‚
â”‚  â€¢ NLP query results (adaptive card or text)                 â”‚
â”‚  â€¢ Total time: 10-35 seconds (vs 6+ seconds blocking)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” Already Completed (Inherited from Phase 1)

âœ… **ProactiveMessagingService** ([teams_bot/app/services/proactive_messaging.py](teams_bot/app/services/proactive_messaging.py))
- Send adaptive cards to conversations
- Send text messages
- Store/retrieve conversation references
- Retry logic with tenacity
- AAD user email extraction

âœ… **MessageBusService** ([teams_bot/app/services/message_bus.py](teams_bot/app/services/message_bus.py))
- Singleton Service Bus client
- `publish_digest_request()` method
- `publish_nlp_query()` method
- Async context managers
- Message TTL and correlation IDs

âœ… **Circuit Breaker Implementation** ([teams_bot/app/services/circuit_breaker.py](teams_bot/app/services/circuit_breaker.py))
- pybreaker integration
- Redis, PostgreSQL, Zoho API breakers
- In-memory fallback for Redis rate limiting
- Telemetry tracking with Application Insights
- Fallback metrics (success rate, avg latency)

âœ… **Message Schemas** ([teams_bot/app/models/messages.py](teams_bot/app/models/messages.py))
- Pydantic models: `DigestRequestMessage`, `NLPQueryMessage`
- Enums: `DigestAudience`, `MessagePriority`
- Validation rules (min_length, ge, le)

âœ… **Service Bus Infrastructure**
- Namespace: `wellintakebus-standard` (Standard tier)
- Queue 1: `teams-digest-requests` (5min lock, 3 retries, 1024MB)
- Queue 2: `teams-nlp-queries` (2min lock, 2 retries, 1024MB)
- Connection string stored in Azure Key Vault

âœ… **Database Schema**
- Table: `conversation_references` (with indexes on conversation_id, user_email, created_at)
- Stores: conversation_id, service_url, tenant_id, user details, reference_json

---

## ğŸ§ª Testing Results (Phase 1)

**Message Publishing Test:**
```bash
# Published 2 test messages to teams-digest-requests queue
âœ… Message 1: correlation_id=test-123, message_id=uuid-456
âœ… Message 2: correlation_id=test-789, message_id=uuid-012

# Verified queue depth
az servicebus queue show --name teams-digest-requests \
  --namespace-name wellintakebus-standard \
  --query "countDetails.activeMessageCount"
# Output: 2
```

**Database Test:**
```sql
-- Created conversation_references table
SELECT * FROM conversation_references;
-- 0 rows (table ready for production)
```

**Azure Key Vault Test:**
```bash
# Stored Service Bus connection string
az keyvault secret show --name ServiceBusConnectionString --vault-name <vault-name>
# Secret ID: 49f8e7a80cdf404badfe0725b609ee78
```

---

## ğŸ“‹ Pending Tasks (Phase 3)

### 1. âš ï¸ **Deploy Worker Container Apps to Azure**
**Status:** Not yet deployed (awaiting production approval)

**Required Steps:**
```bash
# 1. Create digest worker Container App
az containerapp create \
  --name teams-digest-worker \
  --resource-group TheWell-Infra-East \
  --environment <CONTAINER_APP_ENVIRONMENT_NAME> \
  --image wellintakeacr0903.azurecr.io/teams-digest-worker:latest \
  --cpu 1.0 --memory 2Gi \
  --min-replicas 0 --max-replicas 10 \
  --scale-rule-name azure-servicebus-queue-rule \
  --scale-rule-type azure-servicebus \
  --scale-rule-metadata "queueName=teams-digest-requests" \
                        "namespace=wellintakebus-standard" \
                        "messageCount=5" \
  --scale-rule-auth "connection=service-bus-connection-string" \
  --secrets "service-bus-connection-string=$SERVICE_BUS_CONNECTION_STRING" \
  --env-vars \
    AZURE_SERVICE_BUS_CONNECTION_STRING=secretref:service-bus-connection-string \
    AZURE_SERVICE_BUS_DIGEST_QUEUE=teams-digest-requests \
    TEAMS_BOT_APP_ID=$TEAMS_BOT_APP_ID \
    TEAMS_BOT_APP_PASSWORD=$TEAMS_BOT_APP_PASSWORD \
    DATABASE_URL=$DATABASE_URL

# 2. Create NLP worker Container App (similar command)
```

**Blockers:**
- Need Container Apps environment name
- Need to verify TEAMS_BOT_APP_ID and TEAMS_BOT_APP_PASSWORD in Key Vault

---

### 2. âš ï¸ **Refactor routes.py to Use Message Bus**
**Status:** routes.py still uses synchronous digest generation

**Current Code (Blocking):**
```python
# app/api/teams/routes.py (line ~450)
@router.post("/webhook")
async def handle_message(turn_context: TurnContext):
    if message.startswith("digest"):
        # ğŸ”´ BLOCKING: Waits 6+ seconds for curator.run_weekly_digest()
        result = await curator.run_weekly_digest(audience=audience)
        card = create_digest_preview_card(result["cards"])
        await turn_context.send_activity(MessageFactory.attachment(card))
```

**Target Code (Async):**
```python
# app/api/teams/routes.py (refactored)
@router.post("/webhook")
async def handle_message(turn_context: TurnContext):
    if message.startswith("digest"):
        # âœ… NON-BLOCKING: Returns immediately with acknowledgment
        message_bus = MessageBusService.get_instance()
        message_id = await message_bus.publish_digest_request(
            conversation_id=turn_context.activity.conversation.id,
            service_url=turn_context.activity.service_url,
            audience=audience,
            user_email=extract_user_email(turn_context.activity)
        )

        # Store conversation reference for proactive messaging
        proactive_service = await create_proactive_messaging_service()
        await proactive_service.store_conversation_reference(turn_context.activity)

        # Send acknowledgment card
        ack_card = create_acknowledgment_card(
            message="â³ Generating your digest... I'll send it shortly!",
            request_id=message_id
        )
        await turn_context.send_activity(MessageFactory.attachment(ack_card))
```

**Files to Update:**
- [`app/api/teams/routes.py`](app/api/teams/routes.py) (line ~200-600)
- Add acknowledgment card helper to [`app/api/teams/adaptive_cards.py`](app/api/teams/adaptive_cards.py)

---

### 3. âš ï¸ **Create Integration Tests**
**Status:** No end-to-end tests yet

**Test Scenarios Needed:**

**Test 1: Message Publishing**
```python
# tests/integration/test_service_bus_flow.py
async def test_publish_digest_request():
    """Test publishing message to Service Bus queue"""
    message_bus = MessageBusService.get_instance()

    message_id = await message_bus.publish_digest_request(
        conversation_id="test-conv-123",
        service_url="https://smba.trafficmanager.net/amer/",
        audience="advisors",
        user_email="test@emailthewell.com"
    )

    assert message_id is not None

    # Verify message in queue
    queue_depth = await message_bus.get_queue_metrics("teams-digest-requests")
    assert queue_depth["active_messages"] > 0
```

**Test 2: Worker Processing (Mock)**
```python
async def test_digest_worker_processes_message():
    """Test digest worker can process a message"""
    worker = DigestWorker()
    await worker.initialize()

    # Create test message
    test_message = create_test_service_bus_message({
        "message_id": "test-123",
        "conversation_id": "conv-456",
        "audience": "advisors"
    })

    success = await worker.process_message(test_message)
    assert success is True

    # Verify database record
    async with get_connection_manager() as db:
        record = await db.fetchrow(
            "SELECT * FROM teams_digest_requests WHERE request_id = $1",
            "test-123"
        )
        assert record["status"] == "completed"
```

**Test 3: Proactive Messaging**
```python
async def test_proactive_message_delivery():
    """Test proactive messaging sends card to Teams user"""
    proactive_service = await create_proactive_messaging_service()

    # Store test conversation reference
    test_activity = create_test_teams_activity()
    await proactive_service.store_conversation_reference(test_activity)

    # Send proactive message
    test_card = {"type": "AdaptiveCard", "body": [...]}
    success = await proactive_service.send_card_to_conversation(
        conversation_id="test-conv-123",
        service_url="https://smba.trafficmanager.net/amer/",
        tenant_id=None,
        card_json=test_card
    )

    assert success is True
```

**Files to Create:**
- `tests/integration/test_service_bus_flow.py`
- `tests/integration/test_digest_worker.py`
- `tests/integration/test_nlp_worker.py`
- `tests/integration/test_proactive_messaging.py`

---

## ğŸ’° Cost Analysis

### Current State (Monolithic)
- **Teams Bot Container App:** 1 replica Ã— 24h Ã— $0.12/h = **$86.40/month**
- **Always-on cost:** Fixed regardless of usage
- **No scaling:** Can't handle burst traffic

### Future State (Event-Driven with KEDA)
- **Teams Bot (HTTP Handler):** 1 replica Ã— 24h Ã— $0.12/h = **$86.40/month** (always-on)
- **Digest Worker:** ~2h/day Ã— $0.12/h = **$7.20/month** (scale-to-zero when idle)
- **NLP Worker:** ~1h/day Ã— $0.12/h = **$3.60/month** (scale-to-zero when idle)
- **Service Bus Standard:** **$10/month**

**Total:** ~$107/month

**ROI Analysis:**
- **Additional Cost:** +$20/month (+23%)
- **Performance Gain:** 95% faster HTTP response (6s â†’ <200ms)
- **Reliability Gain:** Automatic retries, dead letter queues, circuit breakers
- **Scalability Gain:** 0 â†’ 30 replicas based on demand
- **Developer Experience:** Async patterns, observability, testability

**Verdict:** Worth the cost for production reliability and user experience.

---

## ğŸ“ˆ Performance Metrics (Projected)

### Before (Synchronous)
- **HTTP Response Time:** 6-10 seconds (blocking on curator.run_weekly_digest())
- **Concurrent Users:** Limited by single replica processing capacity (~5 requests/min)
- **Failure Mode:** HTTP 500 errors if curator times out or fails
- **Retry Strategy:** Manual user retry

### After (Event-Driven)
- **HTTP Response Time:** <150ms (immediate HTTP 202 acknowledgment)
- **Concurrent Users:** Unlimited (queued messages, auto-scaling workers)
- **Failure Mode:** Dead letter queue + retry (up to 3 attempts)
- **Proactive Delivery:** 10-35 seconds total time (user doesn't wait)

---

## ğŸ› ï¸ Monitoring & Observability

### Key Metrics to Track

**Service Bus Metrics:**
```bash
# Queue depth (active messages)
az servicebus queue show --name teams-digest-requests \
  --query "countDetails.activeMessageCount"

# Dead letter queue depth
az servicebus queue show --name teams-digest-requests \
  --query "countDetails.deadLetterMessageCount"
```

**Container Apps Metrics:**
```bash
# Replica count
az containerapp replica list --name teams-digest-worker \
  --query "length([])"

# CPU/Memory usage
az monitor metrics list --resource <CONTAINER_APP_ID> \
  --metric "UsageNanoCores" --metric "WorkingSetBytes"
```

**Application Insights Queries:**
```kusto
// Digest generation latency
customMetrics
| where name == "digest.generation.time"
| summarize avg(value), percentile(value, 95) by bin(timestamp, 5m)

// Worker processing errors
exceptions
| where cloud_RoleName in ("teams-digest-worker", "teams-nlp-worker")
| summarize count() by bin(timestamp, 1h), problemId
```

---

## ğŸ¯ Success Criteria

âœ… **Phase 2 Complete When:**
- [x] Digest worker code complete and tested locally
- [x] NLP worker code complete and tested locally
- [x] Docker images buildable and pushable to ACR
- [x] KEDA configuration documented with Azure CLI commands
- [x] Deployment script created and executable
- [x] Phase 2 completion document written

âš ï¸ **Phase 3 Required For Production:**
- [ ] Worker Container Apps deployed to Azure
- [ ] KEDA autoscaling verified (scale-to-zero â†’ scale-out â†’ scale-down)
- [ ] routes.py refactored to use MessageBusService
- [ ] Integration tests passing (message flow, worker processing, proactive messaging)
- [ ] Monitoring dashboards created in Application Insights
- [ ] Dead letter queue monitoring and alerting configured

---

## ğŸ“š References

- [Phase 1 Completion Summary](PHASE1_COMPLETION_SUMMARY.md)
- [KEDA Configuration Guide](KEDA_CONFIGURATION.md)
- [Service Bus Deliverables](SERVICE_BUS_DELIVERABLES.md)
- [Azure Service Bus Configuration](AZURE_SERVICE_BUS_CONFIG.md)

---

## ğŸš€ Next Steps

1. **Deploy Worker Container Apps** (requires production approval)
2. **Refactor routes.py** (replace sync digest generation with message bus)
3. **Create Integration Tests** (pytest with Service Bus emulator)
4. **Enable Feature Flag** (`TEAMS_USE_ASYNC_DIGEST=true` for 10% of users)
5. **Monitor & Iterate** (tune KEDA thresholds, observe latency metrics)

---

**Phase 2 Status:** âœ… **COMPLETE** (awaiting Phase 3 deployment approval)
